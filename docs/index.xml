<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Christophe Ambroise</title>
    <link>https://cambroise.github.io/</link>
      <atom:link href="https://cambroise.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Christophe Ambroise</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sat, 01 Jun 2030 13:00:00 +0000</lastBuildDate>
    <image>
      <url>https://cambroise.github.io/images/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_3.png</url>
      <title>Christophe Ambroise</title>
      <link>https://cambroise.github.io/</link>
    </image>
    
    <item>
      <title>PhD Student</title>
      <link>https://cambroise.github.io/co-workers/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/co-workers/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Example Talk</title>
      <link>https://cambroise.github.io/talk/example/</link>
      <pubDate>Sat, 01 Jun 2030 13:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/talk/example/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click on the &lt;strong&gt;Slides&lt;/strong&gt; button above to view the built-in slides feature.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Slides can be added in a few ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Create&lt;/strong&gt; slides using Wowchemy&amp;rsquo;s &lt;a href=&#34;https://wowchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Slides&lt;/em&gt;&lt;/a&gt; feature and link using &lt;code&gt;slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Upload&lt;/strong&gt; an existing slide deck to &lt;code&gt;static/&lt;/code&gt; and link using &lt;code&gt;url_slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embed&lt;/strong&gt; your slides (e.g. Google Slides) or presentation video on this page using &lt;a href=&#34;https://wowchemy.com/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;shortcodes&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Further event details, including &lt;a href=&#34;https://wowchemy.com/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;page elements&lt;/a&gt; such as image galleries, can be added to the body of this page.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title> Epigenetic Variation in Tree Evolution: a case study in black poplar (Populus nigra) </title>
      <link>https://cambroise.github.io/publication/sow2023epigenetic/</link>
      <pubDate>Mon, 01 Jan 2024 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/sow2023epigenetic/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Mixture of multilayer stochastic block models for multiview clustering </title>
      <link>https://cambroise.github.io/publication/desantiago2024mixture/</link>
      <pubDate>Mon, 01 Jan 2024 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/desantiago2024mixture/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Holistic view of the seascape dynamics and environment impact on macro-scale genetic connectivity of marine plankton populations </title>
      <link>https://cambroise.github.io/publication/laso2023holistic/</link>
      <pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/laso2023holistic/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Inference of Multiscale Gaussian Graphical Model </title>
      <link>https://cambroise.github.io/publication/sanou2023inference/</link>
      <pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/sanou2023inference/</guid>
      <description></description>
    </item>
    
    <item>
      <title> A Sparse Mixture-of-Experts Model With Screening of Genetic Associations to Guide Disease Subtyping </title>
      <link>https://cambroise.github.io/publication/courbariaux2022sparse/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/courbariaux2022sparse/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Compression structur{e}e de l&#39;information g{e}n{e}tique et {e}tude d&#39;association pang{e}nomique par mod{e}les additifs </title>
      <link>https://cambroise.github.io/publication/guinot2022compression/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/guinot2022compression/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Hierarchical correction of p-values via an ultrametric tree running Ornstein-Uhlenbeck process </title>
      <link>https://cambroise.github.io/publication/bichat2022hierarchical/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/bichat2022hierarchical/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Antibiogo</title>
      <link>https://cambroise.github.io/project/antibiogo/</link>
      <pubDate>Sat, 27 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/project/antibiogo/</guid>
      <description>&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;p&gt;Engineers and researchers from the University of Évry, CEA, CNRS, APHP, CHU Henri Mondor and Google, in collaboration with the Médecins Sans Frontière (MSF) foundation, have developped a mobile application capable of  antibiotic resistance diagnostic, which can be used free of charge anywhere in the world. These results are the subject of a publication in the journal &lt;a href=&#34;https://cambroise.github.io/publication/pascucci2021/&#34;&gt;Nature Communications&lt;/a&gt;.
For more details in french, see &lt;a href= &#34;https://dataanalyticspost.com/une-ia-qui-va-faire-reculer-lantibioresistance/&#34;&gt;this article&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;médecins-sans-frontière--presentation-of-the-project-and-code&#34;&gt;Médecins sans frontière:  presentation of the project and code&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://antibiogo.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://antibiogo.org/&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;project-description&#34;&gt;Project description&lt;/h2&gt;
&lt;p&gt;Antimicrobial resistance is a major global health threat and its development is promoted by antibiotic misuse. While disk diffusion antibiotic susceptibility testing (AST, also called antibiogram) is broadly used to test for antibiotic resistance in bacterial infections, it faces strong criticism because of inter-operator variability and the complexity of interpretative reading. Automatic reading systems address these issues, but are not always adapted or available to resource-limited settings. We present an artificial intelligence (AI)-based, offline smartphone application for antibiogram analysis. The application captures images with the phone’s camera, and the user is guided throughout the analysis on the same device by a user-friendly graphical interface. An embedded expert system validates the coherence of the antibiogram data and provides interpreted results. The fully automatic measurement procedure of our application’s reading system achieves an overall agreement of 90% on susceptibility categorization against a hospital-standard automatic system and 98% against manual measurement (gold standard), with reduced inter-operator variability. The application’s performance showed that the automatic reading of antibiotic resistance testing is entirely feasible on a smartphone. Moreover our application is suited for resource-limited settings, and therefore has the potential to significantly increase patients’ access to AST worldwide.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title> AI-based mobile application for antibiotic resistance testing </title>
      <link>https://cambroise.github.io/publication/pascucci2021/</link>
      <pubDate>Fri, 19 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/pascucci2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Accounting for missing actors in interaction network inference from abundance data </title>
      <link>https://cambroise.github.io/publication/momal2021accounting/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/momal2021accounting/</guid>
      <description></description>
    </item>
    
    <item>
      <title> How marine currents and environment shape plankton genomic differentiation: a mosaic view from Tara oceans metagenomic data </title>
      <link>https://cambroise.github.io/publication/laso2021marine/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/laso2021marine/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Tree-based Inference of Species Interaction Networks from Abundance Data </title>
      <link>https://cambroise.github.io/publication/momal2020tree/</link>
      <pubDate>Wed, 19 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/momal2020tree/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Exploring the link between additive heritability and prediction accuracy from a ridge regression perspective </title>
      <link>https://cambroise.github.io/publication/frouin2020exploring/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/frouin2020exploring/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Fast computation of genome-metagenome interaction effects </title>
      <link>https://cambroise.github.io/publication/guinot2020fast/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/guinot2020fast/</guid>
      <description></description>
    </item>
    
    <item>
      <title> High heritability does not imply accurate prediction under the small additive effects hypothesis </title>
      <link>https://cambroise.github.io/publication/frouin2020high/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/frouin2020high/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Incorporating phylogenetic information in microbiome differential abundance studies has no effect on detection power and FDR control </title>
      <link>https://cambroise.github.io/publication/bichat2020incorporating/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/bichat2020incorporating/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Investigating population-scale allelic differential expression in wild populations of Oithona similis (Cyclopoida, Claus, 1866) </title>
      <link>https://cambroise.github.io/publication/laso2020investigating/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/laso2020investigating/</guid>
      <description></description>
    </item>
    
    <item>
      <title> metaVaR: introducing metavariant species models for reference-free metagenomic-based population genomics </title>
      <link>https://cambroise.github.io/publication/laso2020metavar/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/laso2020metavar/</guid>
      <description></description>
    </item>
    
    <item>
      <title> PPanGGOLiN: depicting microbial diversity via a partitioned pangenome graph </title>
      <link>https://cambroise.github.io/publication/gautreau2020ppanggolin/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/gautreau2020ppanggolin/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Writing technical content in Academic</title>
      <link>https://cambroise.github.io/post/writing-technical-content/</link>
      <pubDate>Fri, 12 Jul 2019 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/post/writing-technical-content/</guid>
      <description>&lt;p&gt;Academic is designed to give technical content creators a seamless experience. You can focus on the content and Academic handles the rest.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Highlight your code snippets, take notes on math classes, and draw diagrams from textual representation.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;On this page, you&amp;rsquo;ll find some examples of the types of technical content that can be rendered with Academic.&lt;/p&gt;
&lt;h2 id=&#34;examples&#34;&gt;Examples&lt;/h2&gt;
&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;
&lt;p&gt;Academic supports a Markdown extension for highlighting code syntax. You can enable this feature by toggling the &lt;code&gt;highlight&lt;/code&gt; option in your &lt;code&gt;config/_default/params.toml&lt;/code&gt; file.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;```python
import pandas as pd
data = pd.read_csv(&amp;quot;data.csv&amp;quot;)
data.head()
```
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;renders as&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd
data = pd.read_csv(&amp;quot;data.csv&amp;quot;)
data.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;charts&#34;&gt;Charts&lt;/h3&gt;
&lt;p&gt;Academic supports the popular &lt;a href=&#34;https://plot.ly/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Plotly&lt;/a&gt; chart format.&lt;/p&gt;
&lt;p&gt;Save your Plotly JSON in your page folder, for example &lt;code&gt;chart.json&lt;/code&gt;, and then add the &lt;code&gt;{{&amp;lt; chart data=&amp;quot;chart&amp;quot; &amp;gt;}}&lt;/code&gt; shortcode where you would like the chart to appear.&lt;/p&gt;
&lt;p&gt;Demo:&lt;/p&gt;


&lt;div id=&#34;chart-234651789&#34; class=&#34;chart&#34;&gt;&lt;/div&gt;
&lt;script&gt;
  (function() {
    let a = setInterval( function() {
      if ( typeof window.Plotly === &#39;undefined&#39; ) {
        return;
      }
      clearInterval( a );

      Plotly.d3.json(&#34;./line-chart.json&#34;, function(chart) {
        Plotly.plot(&#39;chart-234651789&#39;, chart.data, chart.layout, {responsive: true});
      });
    }, 500 );
  })();
&lt;/script&gt;
&lt;p&gt;You might also find the &lt;a href=&#34;http://plotly-json-editor.getforge.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Plotly JSON Editor&lt;/a&gt; useful.&lt;/p&gt;
&lt;h3 id=&#34;math&#34;&gt;Math&lt;/h3&gt;
&lt;p&gt;Academic supports a Markdown extension for $\LaTeX$ math. You can enable this feature by toggling the &lt;code&gt;math&lt;/code&gt; option in your &lt;code&gt;config/_default/params.toml&lt;/code&gt; file.&lt;/p&gt;
&lt;p&gt;To render &lt;em&gt;inline&lt;/em&gt; or &lt;em&gt;block&lt;/em&gt; math, wrap your LaTeX math with &lt;code&gt;$...$&lt;/code&gt; or &lt;code&gt;$$...$$&lt;/code&gt;, respectively.&lt;/p&gt;
&lt;p&gt;Example &lt;strong&gt;math block&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-latex&#34;&gt;$$\gamma_{n} = \frac{ 
\left | \left (\mathbf x_{n} - \mathbf x_{n-1} \right )^T 
\left [\nabla F (\mathbf x_{n}) - \nabla F (\mathbf x_{n-1}) \right ] \right |}
{\left \|\nabla F(\mathbf{x}_{n}) - \nabla F(\mathbf{x}_{n-1}) \right \|^2}$$
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;renders as&lt;/p&gt;
&lt;p&gt;$$\gamma_{n} = \frac{ \left | \left (\mathbf x_{n} - \mathbf x_{n-1} \right )^T \left [\nabla F (\mathbf x_{n}) - \nabla F (\mathbf x_{n-1}) \right ] \right |}{\left |\nabla F(\mathbf{x}&lt;em&gt;{n}) - \nabla F(\mathbf{x}&lt;/em&gt;{n-1}) \right |^2}$$&lt;/p&gt;
&lt;p&gt;Example &lt;strong&gt;inline math&lt;/strong&gt; &lt;code&gt;$\nabla F(\mathbf{x}_{n})$&lt;/code&gt; renders as $\nabla F(\mathbf{x}_{n})$.&lt;/p&gt;
&lt;p&gt;Example &lt;strong&gt;multi-line math&lt;/strong&gt; using the &lt;code&gt;\\\\&lt;/code&gt; math linebreak:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-latex&#34;&gt;$$f(k;p_0^*) = \begin{cases} p_0^* &amp;amp; \text{if }k=1, \\\\
1-p_0^* &amp;amp; \text {if }k=0.\end{cases}$$
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;renders as&lt;/p&gt;
&lt;p&gt;$$f(k;p_0^&lt;em&gt;) = \begin{cases} p_0^&lt;/em&gt; &amp;amp; \text{if }k=1, \\
1-p_0^* &amp;amp; \text {if }k=0.\end{cases}$$&lt;/p&gt;
&lt;h3 id=&#34;diagrams&#34;&gt;Diagrams&lt;/h3&gt;
&lt;p&gt;Academic supports a Markdown extension for diagrams. You can enable this feature by toggling the &lt;code&gt;diagram&lt;/code&gt; option in your &lt;code&gt;config/_default/params.toml&lt;/code&gt; file or by adding &lt;code&gt;diagram: true&lt;/code&gt; to your page front matter.&lt;/p&gt;
&lt;p&gt;An example &lt;strong&gt;flowchart&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;```mermaid
graph TD
A[Hard] --&amp;gt;|Text| B(Round)
B --&amp;gt; C{Decision}
C --&amp;gt;|One| D[Result 1]
C --&amp;gt;|Two| E[Result 2]
```
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;renders as&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;graph TD
A[Hard] --&amp;gt;|Text| B(Round)
B --&amp;gt; C{Decision}
C --&amp;gt;|One| D[Result 1]
C --&amp;gt;|Two| E[Result 2]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;An example &lt;strong&gt;sequence diagram&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;```mermaid
sequenceDiagram
Alice-&amp;gt;&amp;gt;John: Hello John, how are you?
loop Healthcheck
    John-&amp;gt;&amp;gt;John: Fight against hypochondria
end
Note right of John: Rational thoughts!
John--&amp;gt;&amp;gt;Alice: Great!
John-&amp;gt;&amp;gt;Bob: How about you?
Bob--&amp;gt;&amp;gt;John: Jolly good!
```
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;renders as&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;sequenceDiagram
Alice-&amp;gt;&amp;gt;John: Hello John, how are you?
loop Healthcheck
    John-&amp;gt;&amp;gt;John: Fight against hypochondria
end
Note right of John: Rational thoughts!
John--&amp;gt;&amp;gt;Alice: Great!
John-&amp;gt;&amp;gt;Bob: How about you?
Bob--&amp;gt;&amp;gt;John: Jolly good!
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;An example &lt;strong&gt;Gantt diagram&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;```mermaid
gantt
section Section
Completed :done,    des1, 2014-01-06,2014-01-08
Active        :active,  des2, 2014-01-07, 3d
Parallel 1   :         des3, after des1, 1d
Parallel 2   :         des4, after des1, 1d
Parallel 3   :         des5, after des3, 1d
Parallel 4   :         des6, after des4, 1d
```
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;renders as&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;gantt
section Section
Completed :done,    des1, 2014-01-06,2014-01-08
Active        :active,  des2, 2014-01-07, 3d
Parallel 1   :         des3, after des1, 1d
Parallel 2   :         des4, after des1, 1d
Parallel 3   :         des5, after des3, 1d
Parallel 4   :         des6, after des4, 1d
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;An example &lt;strong&gt;class diagram&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;```mermaid
classDiagram
Class01 &amp;lt;|-- AveryLongClass : Cool
&amp;lt;&amp;lt;interface&amp;gt;&amp;gt; Class01
Class09 --&amp;gt; C2 : Where am i?
Class09 --* C3
Class09 --|&amp;gt; Class07
Class07 : equals()
Class07 : Object[] elementData
Class01 : size()
Class01 : int chimp
Class01 : int gorilla
class Class10 {
  &amp;lt;&amp;lt;service&amp;gt;&amp;gt;
  int id
  size()
}
```
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;renders as&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;classDiagram
Class01 &amp;lt;|-- AveryLongClass : Cool
&amp;lt;&amp;lt;interface&amp;gt;&amp;gt; Class01
Class09 --&amp;gt; C2 : Where am i?
Class09 --* C3
Class09 --|&amp;gt; Class07
Class07 : equals()
Class07 : Object[] elementData
Class01 : size()
Class01 : int chimp
Class01 : int gorilla
class Class10 {
  &amp;lt;&amp;lt;service&amp;gt;&amp;gt;
  int id
  size()
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;An example &lt;strong&gt;state diagram&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;```mermaid
stateDiagram
[*] --&amp;gt; Still
Still --&amp;gt; [*]
Still --&amp;gt; Moving
Moving --&amp;gt; Still
Moving --&amp;gt; Crash
Crash --&amp;gt; [*]
```
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;renders as&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;stateDiagram
[*] --&amp;gt; Still
Still --&amp;gt; [*]
Still --&amp;gt; Moving
Moving --&amp;gt; Still
Moving --&amp;gt; Crash
Crash --&amp;gt; [*]
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;todo-lists&#34;&gt;Todo lists&lt;/h3&gt;
&lt;p&gt;You can even write your todo lists in Academic too:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;- [x] Write math example
- [x] Write diagram example
- [ ] Do something else
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;renders as&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Write math example&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Write diagram example&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Do something else&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;tables&#34;&gt;Tables&lt;/h3&gt;
&lt;p&gt;Represent your data in tables:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;| First Header  | Second Header |
| ------------- | ------------- |
| Content Cell  | Content Cell  |
| Content Cell  | Content Cell  |
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;renders as&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;First Header&lt;/th&gt;
&lt;th&gt;Second Header&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Content Cell&lt;/td&gt;
&lt;td&gt;Content Cell&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Content Cell&lt;/td&gt;
&lt;td&gt;Content Cell&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;callouts&#34;&gt;Callouts&lt;/h3&gt;
&lt;p&gt;Academic supports a &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/#alerts&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;shortcode for callouts&lt;/a&gt;, also referred to as &lt;em&gt;asides&lt;/em&gt;, &lt;em&gt;hints&lt;/em&gt;, or &lt;em&gt;alerts&lt;/em&gt;. By wrapping a paragraph in &lt;code&gt;{{% alert note %}} ... {{% /alert %}}&lt;/code&gt;, it will render as an aside.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{% alert note %}}
A Markdown aside is useful for displaying notices, hints, or definitions to your readers.
{{% /alert %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;renders as&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    A Markdown aside is useful for displaying notices, hints, or definitions to your readers.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&#34;spoilers&#34;&gt;Spoilers&lt;/h3&gt;
&lt;p&gt;Add a spoiler to a page to reveal text, such as an answer to a question, after a button is clicked.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; spoiler text=&amp;quot;Click to view the spoiler&amp;quot; &amp;gt;}}
You found me!
{{&amp;lt; /spoiler &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;renders as&lt;/p&gt;
&lt;div class=&#34;spoiler &#34; &gt;
  &lt;p&gt;
    &lt;a class=&#34;btn btn-primary&#34; data-toggle=&#34;collapse&#34; href=&#34;#spoiler-2&#34; role=&#34;button&#34; aria-expanded=&#34;false&#34; aria-controls=&#34;spoiler-2&#34;&gt;
      Click to view the spoiler
    &lt;/a&gt;
  &lt;/p&gt;
  &lt;div class=&#34;collapse card &#34; id=&#34;spoiler-2&#34;&gt;
    &lt;div class=&#34;card-body&#34;&gt;
      You found me!
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&#34;icons&#34;&gt;Icons&lt;/h3&gt;
&lt;p&gt;Academic enables you to use a wide range of &lt;a href=&#34;https://sourcethemes.com/academic/docs/page-builder/#icons&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;icons from &lt;em&gt;Font Awesome&lt;/em&gt; and &lt;em&gt;Academicons&lt;/em&gt;&lt;/a&gt; in addition to &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/#emojis&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;emojis&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Here are some examples using the &lt;code&gt;icon&lt;/code&gt; shortcode to render icons:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; icon name=&amp;quot;terminal&amp;quot; pack=&amp;quot;fas&amp;quot; &amp;gt;}} Terminal  
{{&amp;lt; icon name=&amp;quot;python&amp;quot; pack=&amp;quot;fab&amp;quot; &amp;gt;}} Python  
{{&amp;lt; icon name=&amp;quot;r-project&amp;quot; pack=&amp;quot;fab&amp;quot; &amp;gt;}} R
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;renders as&lt;/p&gt;
&lt;p&gt;
  &lt;i class=&#34;fas fa-terminal  pr-1 fa-fw&#34;&gt;&lt;/i&gt; Terminal&lt;br&gt;

  &lt;i class=&#34;fab fa-python  pr-1 fa-fw&#34;&gt;&lt;/i&gt; Python&lt;br&gt;

  &lt;i class=&#34;fab fa-r-project  pr-1 fa-fw&#34;&gt;&lt;/i&gt; R&lt;/p&gt;
&lt;h3 id=&#34;did-you-find-this-page-helpful-consider-sharing-it-&#34;&gt;Did you find this page helpful? Consider sharing it 🙌&lt;/h3&gt;
</description>
    </item>
    
    <item>
      <title>Display Jupyter Notebooks with Academic</title>
      <link>https://cambroise.github.io/post/jupyter/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/post/jupyter/</guid>
      <description>&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from IPython.core.display import Image
Image(&#39;https://www.python.org/static/community_logos/python-logo-master-v3-TM-flattened.png&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./index_1_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(&amp;quot;Welcome to Academic!&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Welcome to Academic!
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;install-python-and-jupyterlab&#34;&gt;Install Python and JupyterLab&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.anaconda.com/distribution/#download-section&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Install Anaconda&lt;/a&gt; which includes Python 3 and JupyterLab.&lt;/p&gt;
&lt;p&gt;Alternatively, install JupyterLab with &lt;code&gt;pip3 install jupyterlab&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;create-or-upload-a-jupyter-notebook&#34;&gt;Create or upload a Jupyter notebook&lt;/h2&gt;
&lt;p&gt;Run the following commands in your Terminal, substituting &lt;code&gt;&amp;lt;MY-WEBSITE-FOLDER&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;SHORT-POST-TITLE&amp;gt;&lt;/code&gt; with the file path to your Academic website folder and a short title for your blog post (use hyphens instead of spaces), respectively:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir -p &amp;lt;MY-WEBSITE-FOLDER&amp;gt;/content/post/&amp;lt;SHORT-POST-TITLE&amp;gt;/
cd &amp;lt;MY-WEBSITE-FOLDER&amp;gt;/content/post/&amp;lt;SHORT-POST-TITLE&amp;gt;/
jupyter lab index.ipynb
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;jupyter&lt;/code&gt; command above will launch the JupyterLab editor, allowing us to add Academic metadata and write the content.&lt;/p&gt;
&lt;h2 id=&#34;edit-your-post-metadata&#34;&gt;Edit your post metadata&lt;/h2&gt;
&lt;p&gt;The first cell of your Jupter notebook will contain your post metadata (&lt;a href=&#34;https://sourcethemes.com/academic/docs/front-matter/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;front matter&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;In Jupter, choose &lt;em&gt;Markdown&lt;/em&gt; as the type of the first cell and wrap your Academic metadata in three dashes, indicating that it is YAML front matter:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;---
title: My post&#39;s title
date: 2019-09-01

# Put any other Academic metadata here...
---
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Edit the metadata of your post, using the &lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;documentation&lt;/a&gt; as a guide to the available options.&lt;/p&gt;
&lt;p&gt;To set a &lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#featured-image&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;featured image&lt;/a&gt;, place an image named &lt;code&gt;featured&lt;/code&gt; into your post&amp;rsquo;s folder.&lt;/p&gt;
&lt;p&gt;For other tips, such as using math, see the guide on &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;writing content with Academic&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;convert-notebook-to-markdown&#34;&gt;Convert notebook to Markdown&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;jupyter nbconvert index.ipynb --to markdown --NbConvertApp.output_files_dir=.
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;
&lt;p&gt;This post was created with Jupyter. The orginal files can be found at &lt;a href=&#34;https://github.com/gcushen/hugo-academic/tree/master/exampleSite/content/post/jupyter&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/gcushen/hugo-academic/tree/master/exampleSite/content/post/jupyter&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>https://cambroise.github.io/slides/example/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/slides/example/</guid>
      <description>&lt;h1 id=&#34;create-slides-in-markdown-with-wowchemy&#34;&gt;Create slides in Markdown with Wowchemy&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wowchemy&lt;/a&gt; | &lt;a href=&#34;https://owchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/hakimel/reveal.js#pdf-export&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF Export&lt;/a&gt;: &lt;code&gt;E&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;
&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Code block:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;porridge = &amp;quot;blueberry&amp;quot;
if porridge == &amp;quot;blueberry&amp;quot;:
    print(&amp;quot;Eating...&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;
&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;
&lt;p&gt;Block math:&lt;/p&gt;
&lt;p&gt;$$
f\left( x \right) = ;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;
&lt;p&gt;Make content appear incrementally&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;
&lt;span class=&#34;fragment &#34; &gt;
   One 
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
   **Two** 
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
   Three 
&lt;/span&gt;
&lt;hr&gt;
&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;
&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{% speaker_note %}}
- Only the speaker can read these notes
- Press `S` key to view
{{% /speaker_note %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/media/boards.jpg&#34;
  &gt;

&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;
&lt;p&gt;Customize the slide style and background&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; slide background-image=&amp;quot;/media/boards.jpg&amp;quot; &amp;gt;}}
{{&amp;lt; slide background-color=&amp;quot;#0000FF&amp;quot; &amp;gt;}}
{{&amp;lt; slide class=&amp;quot;my-style&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;
&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.reveal section h1,
.reveal section h2,
.reveal section h3 {
  color: navy;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/wowchemy/wowchemy-hugo-modules/discussions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title> A generalized statistical framework to assess mixing ability from incomplete mixing designs using binary or higher order variety mixtures and application to wheat </title>
      <link>https://cambroise.github.io/publication/forst2019generalized/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/forst2019generalized/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Adjacency-constrained hierarchical clustering of a band similarity matrix with application to genomics </title>
      <link>https://cambroise.github.io/publication/ambroise2019adjacency/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/ambroise2019adjacency/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Applications in Genomics </title>
      <link>https://cambroise.github.io/publication/robin2019applications/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/robin2019applications/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Incomplete graphical model inference via latent tree aggregation </title>
      <link>https://cambroise.github.io/publication/robin2019incomplete/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/robin2019incomplete/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Systematic analysis of TruSeq, SMARTer and SMARTer Ultra-Low RNA-seq kits for standard, low and ultra-low quantity samples </title>
      <link>https://cambroise.github.io/publication/palomares2019systematic/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/palomares2019systematic/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Clarifying the role of DNA methylation in tree phenotypic plasticity </title>
      <link>https://cambroise.github.io/publication/sow2018clarifying/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/sow2018clarifying/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Epigenetics in forest trees: state of the art and potential implications for breeding and management in a context of climate change </title>
      <link>https://cambroise.github.io/publication/sow2018epigenetics/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/sow2018epigenetics/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Estimation of mixing ability for variety mixtures: Statistical models and experimental results </title>
      <link>https://cambroise.github.io/publication/forst2018estimation/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/forst2018estimation/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Learning the optimal scale for GWAS through hierarchical SNP aggregation </title>
      <link>https://cambroise.github.io/publication/guinot2018learning/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/guinot2018learning/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Quantify Genomic Heritability Through a Prediction Measure </title>
      <link>https://cambroise.github.io/publication/frouin2018quantify/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/frouin2018quantify/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Beyond support in two-stage variable selection </title>
      <link>https://cambroise.github.io/publication/becu2017beyond/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/becu2017beyond/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Eigen-epistasis for detecting gene-gene interactions </title>
      <link>https://cambroise.github.io/publication/stanislas2017eigen/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/stanislas2017eigen/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Estimating Narrow-Sense Heritability with Ridge Regression </title>
      <link>https://cambroise.github.io/publication/frouin2017estimating/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/frouin2017estimating/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Quantify Genomic Heritability Through a Prediction Measure </title>
      <link>https://cambroise.github.io/publication/frouin2017quantify/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/frouin2017quantify/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Welcome to Wowchemy, the website builder for Hugo</title>
      <link>https://cambroise.github.io/post/getting-started/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/post/getting-started/</guid>
      <description>&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;The Wowchemy website builder for Hugo, along with its starter templates, is designed for professional creators, educators, and teams/organizations - although it can be used to create any kind of site&lt;/li&gt;
&lt;li&gt;The template can be modified and customised to suit your needs. It&amp;rsquo;s a good platform for anyone looking to take control of their data and online identity whilst having the convenience to start off with a &lt;strong&gt;no-code solution (write in Markdown and customize with YAML parameters)&lt;/strong&gt; and having &lt;strong&gt;flexibility to later add even deeper personalization with HTML and CSS&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;You can work with all your favourite tools and apps with hundreds of plugins and integrations to speed up your workflows, interact with your readers, and much more&lt;/li&gt;
&lt;/ol&gt;


















&lt;figure id=&#34;figure-the-template-is-mobile-first-with-a-responsive-design-to-ensure-that-your-site-looks-stunning-on-every-device&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://raw.githubusercontent.com/wowchemy/wowchemy-hugo-modules/master/academic.png&#34; data-caption=&#34;The template is mobile first with a responsive design to ensure that your site looks stunning on every device.&#34;&gt;


  &lt;img src=&#34;https://raw.githubusercontent.com/wowchemy/wowchemy-hugo-modules/master/academic.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    The template is mobile first with a responsive design to ensure that your site looks stunning on every device.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h2 id=&#34;get-started&#34;&gt;Get Started&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;👉 &lt;a href=&#34;https://wowchemy.com/templates/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Create a new site&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;📚 &lt;a href=&#34;https://wowchemy.com/docs/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Personalize your site&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;💬 &lt;a href=&#34;https://discord.gg/z8wNYzb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chat with the &lt;strong&gt;Wowchemy community&lt;/strong&gt;&lt;/a&gt; or &lt;a href=&#34;https://discourse.gohugo.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Hugo community&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;🐦 Twitter: &lt;a href=&#34;https://twitter.com/wowchemy&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@wowchemy&lt;/a&gt; &lt;a href=&#34;https://twitter.com/GeorgeCushen&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@GeorgeCushen&lt;/a&gt; &lt;a href=&#34;https://twitter.com/search?q=%28%23MadeWithWowchemy%20OR%20%23MadeWithAcademic%29&amp;amp;src=typed_query&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;#MadeWithWowchemy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;💡 &lt;a href=&#34;https://github.com/wowchemy/wowchemy-hugo-modules/issues&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Request a &lt;strong&gt;feature&lt;/strong&gt; or report a &lt;strong&gt;bug&lt;/strong&gt; for &lt;em&gt;Wowchemy&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;⬆️ &lt;strong&gt;Updating Wowchemy?&lt;/strong&gt; View the &lt;a href=&#34;https://wowchemy.com/docs/update/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Update Guide&lt;/a&gt; and &lt;a href=&#34;https://wowchemy.com/updates/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Release Notes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;crowd-funded-open-source-software&#34;&gt;Crowd-funded open-source software&lt;/h2&gt;
&lt;p&gt;To help us develop this template and software sustainably under the MIT license, we ask all individuals and businesses that use it to help support its ongoing maintenance and development via sponsorship.&lt;/p&gt;
&lt;h3 id=&#34;-click-here-to-become-a-sponsor-and-help-support-wowchemys-future-httpswowchemycomplans&#34;&gt;&lt;a href=&#34;https://wowchemy.com/plans/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;❤️ Click here to become a sponsor and help support Wowchemy&amp;rsquo;s future ❤️&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;As a token of appreciation for sponsoring, you can &lt;strong&gt;unlock &lt;a href=&#34;https://wowchemy.com/plans/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;these&lt;/a&gt; awesome rewards and extra features 🦄✨&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;ecosystem&#34;&gt;Ecosystem&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/wowchemy/wowchemy-admin/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wowchemy Admin&lt;/a&gt;:&lt;/strong&gt; An admin tool to automatically import publications from BibTeX&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;inspiration&#34;&gt;Inspiration&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://academic-demo.netlify.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Check out the latest &lt;strong&gt;demo&lt;/strong&gt;&lt;/a&gt; of what you&amp;rsquo;ll get in less than 10 minutes, or &lt;a href=&#34;https://wowchemy.com/user-stories/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;view the &lt;strong&gt;showcase&lt;/strong&gt;&lt;/a&gt; of personal, project, and business sites.&lt;/p&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Page builder&lt;/strong&gt; - Create &lt;em&gt;anything&lt;/em&gt; with &lt;a href=&#34;https://wowchemy.com/docs/page-builder/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;widgets&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&#34;https://wowchemy.com/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;elements&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Edit any type of content&lt;/strong&gt; - Blog posts, publications, talks, slides, projects, and more!&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Create content&lt;/strong&gt; in &lt;a href=&#34;https://wowchemy.com/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Markdown&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://wowchemy.com/docs/import/jupyter/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Jupyter&lt;/strong&gt;&lt;/a&gt;, or &lt;a href=&#34;https://wowchemy.com/docs/install-locally/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;RStudio&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Plugin System&lt;/strong&gt; - Fully customizable &lt;a href=&#34;https://wowchemy.com/docs/customization/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;color&lt;/strong&gt; and &lt;strong&gt;font themes&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Display Code and Math&lt;/strong&gt; - Code highlighting and &lt;a href=&#34;https://en.wikibooks.org/wiki/LaTeX/Mathematics&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LaTeX math&lt;/a&gt; supported&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Integrations&lt;/strong&gt; - &lt;a href=&#34;https://analytics.google.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Analytics&lt;/a&gt;, &lt;a href=&#34;https://disqus.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Disqus commenting&lt;/a&gt;, Maps, Contact Forms, and more!&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Beautiful Site&lt;/strong&gt; - Simple and refreshing one page design&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Industry-Leading SEO&lt;/strong&gt; - Help get your website found on search engines and social media&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Media Galleries&lt;/strong&gt; - Display your images and videos with captions in a customizable gallery&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mobile Friendly&lt;/strong&gt; - Look amazing on every screen with a mobile friendly version of your site&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-language&lt;/strong&gt; - 34+ language packs including English, 中文, and Português&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-user&lt;/strong&gt; - Each author gets their own profile page&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Privacy Pack&lt;/strong&gt; - Assists with GDPR&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Stand Out&lt;/strong&gt; - Bring your site to life with animation, parallax backgrounds, and scroll effects&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;One-Click Deployment&lt;/strong&gt; - No servers. No databases. Only files.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;p&gt;Wowchemy and its templates come with &lt;strong&gt;automatic day (light) and night (dark) mode&lt;/strong&gt; built-in. Alternatively, visitors can choose their preferred mode - click the moon icon in the top right of the &lt;a href=&#34;https://academic-demo.netlify.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Demo&lt;/a&gt; to see it in action! Day/night mode can also be disabled by the site admin in &lt;code&gt;params.toml&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/docs/customization&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Choose a stunning &lt;strong&gt;theme&lt;/strong&gt; and &lt;strong&gt;font&lt;/strong&gt;&lt;/a&gt; for your site. Themes are fully customizable.&lt;/p&gt;
&lt;h2 id=&#34;license&#34;&gt;License&lt;/h2&gt;
&lt;p&gt;Copyright 2016-present &lt;a href=&#34;https://georgecushen.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;George Cushen&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Released under the &lt;a href=&#34;https://github.com/wowchemy/wowchemy-hugo-modules/blob/master/LICENSE.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MIT&lt;/a&gt; license.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hello R Markdown</title>
      <link>https://cambroise.github.io/post/2015-07-23-r-rmarkdown/</link>
      <pubDate>Thu, 23 Jul 2015 21:13:14 -0500</pubDate>
      <guid>https://cambroise.github.io/post/2015-07-23-r-rmarkdown/</guid>
      <description>
&lt;script src=&#34;https://cambroise.github.io/post/2015-07-23-r-rmarkdown/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;r-markdown&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;R Markdown&lt;/h1&gt;
&lt;p&gt;This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see &lt;a href=&#34;http://rmarkdown.rstudio.com&#34; class=&#34;uri&#34;&gt;http://rmarkdown.rstudio.com&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;You can embed an R code chunk like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(cars)
##      speed           dist       
##  Min.   : 4.0   Min.   :  2.00  
##  1st Qu.:12.0   1st Qu.: 26.00  
##  Median :15.0   Median : 36.00  
##  Mean   :15.4   Mean   : 42.98  
##  3rd Qu.:19.0   3rd Qu.: 56.00  
##  Max.   :25.0   Max.   :120.00
fit &amp;lt;- lm(dist ~ speed, data = cars)
fit
## 
## Call:
## lm(formula = dist ~ speed, data = cars)
## 
## Coefficients:
## (Intercept)        speed  
##     -17.579        3.932&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;including-plots&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Including Plots&lt;/h1&gt;
&lt;p&gt;You can also embed plots. See Figure &lt;a href=&#34;#fig:pie&#34;&gt;1&lt;/a&gt; for example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mar = c(0, 1, 0, 1))
pie(
  c(280, 60, 20),
  c(&amp;#39;Sky&amp;#39;, &amp;#39;Sunny side of pyramid&amp;#39;, &amp;#39;Shady side of pyramid&amp;#39;),
  col = c(&amp;#39;#0292D8&amp;#39;, &amp;#39;#F7EA39&amp;#39;, &amp;#39;#C4B632&amp;#39;),
  init.angle = -50, border = NA
)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:pie&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://cambroise.github.io/post/2015-07-23-r-rmarkdown/index_files/figure-html/pie-1.png&#34; alt=&#34;A fancy pie chart.&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: A fancy pie chart.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title> A greedy great approach to learn with complementary structured datasets </title>
      <link>https://cambroise.github.io/publication/chiquet2015greedy/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/chiquet2015greedy/</guid>
      <description></description>
    </item>
    
    <item>
      <title> AB0004 Characterization of Putative Pre-RA Signatures by Transcriptome Analysis </title>
      <link>https://cambroise.github.io/publication/petit2015ab0004/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/petit2015ab0004/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Performance of a blockwise approach in variable selection using linkage disequilibrium information </title>
      <link>https://cambroise.github.io/publication/dehman2015performance/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/dehman2015performance/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Significance testing for variable selection in high-dimension </title>
      <link>https://cambroise.github.io/publication/becu2015significance/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/becu2015significance/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Model selection in overlapping stochastic block models </title>
      <link>https://cambroise.github.io/publication/latouche2014model/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/latouche2014model/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Overlapping clustering methods for networks </title>
      <link>https://cambroise.github.io/publication/latouche2014overlapping/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/latouche2014overlapping/</guid>
      <description></description>
    </item>
    
    <item>
      <title> BP 529 F-60205 Compi{e}gne cedea-France </title>
      <link>https://cambroise.github.io/publication/ambroise2013bp/</link>
      <pubDate>Tue, 01 Jan 2013 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/ambroise2013bp/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Incorporating linkage disequilibrium blocks in Genome-Wide Association Studies </title>
      <link>https://cambroise.github.io/publication/dehman2013incorporating/</link>
      <pubDate>Tue, 01 Jan 2013 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/dehman2013incorporating/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Clustering by maximizing a fuzzy </title>
      <link>https://cambroise.github.io/publication/ambroise2012clustering/</link>
      <pubDate>Sun, 01 Jan 2012 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/ambroise2012clustering/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Discriminant analysis </title>
      <link>https://cambroise.github.io/publication/mclachlan2012discriminant/</link>
      <pubDate>Sun, 01 Jan 2012 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/mclachlan2012discriminant/</guid>
      <description></description>
    </item>
    
    <item>
      <title> SHIPS: spectral hierarchical clustering for the inference of population structure </title>
      <link>https://cambroise.github.io/publication/bouaziz2012ships/</link>
      <pubDate>Sun, 01 Jan 2012 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/bouaziz2012ships/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Sparsity by Worst-Case Penalties </title>
      <link>https://cambroise.github.io/publication/grandvalet2012sparsity/</link>
      <pubDate>Sun, 01 Jan 2012 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/grandvalet2012sparsity/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Variational Bayesian inference and complexity control for stochastic block models </title>
      <link>https://cambroise.github.io/publication/latouche2012variational/</link>
      <pubDate>Sun, 01 Jan 2012 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/latouche2012variational/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Accounting for population stratification in practice: a comparison of the main strategies dedicated to genome-wide association studies </title>
      <link>https://cambroise.github.io/publication/bouaziz2011accounting/</link>
      <pubDate>Sat, 01 Jan 2011 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/bouaziz2011accounting/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Defining a robust biological prior from pathway analysis to drive network inference </title>
      <link>https://cambroise.github.io/publication/jeanmougin2011defining/</link>
      <pubDate>Sat, 01 Jan 2011 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/jeanmougin2011defining/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Inferring multiple graphical structures </title>
      <link>https://cambroise.github.io/publication/chiquet2011inferring/</link>
      <pubDate>Sat, 01 Jan 2011 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/chiquet2011inferring/</guid>
      <description></description>
    </item>
    
    <item>
      <title> New consistent and asymptotically normal parameter estimates for random-graph mixture models </title>
      <link>https://cambroise.github.io/publication/ambroise2011new/</link>
      <pubDate>Sat, 01 Jan 2011 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/ambroise2011new/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Overlapping stochastic block models with application to the french political blogosphere </title>
      <link>https://cambroise.github.io/publication/latouche2011overlapping/</link>
      <pubDate>Sat, 01 Jan 2011 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/latouche2011overlapping/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Clustering based on random graph model embedding vertex features </title>
      <link>https://cambroise.github.io/publication/zanghi2010clustering/</link>
      <pubDate>Fri, 01 Jan 2010 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/zanghi2010clustering/</guid>
      <description></description>
    </item>
    
    <item>
      <title> New consistent and asymptotically normal estimators for random graph mixture models </title>
      <link>https://cambroise.github.io/publication/ambroise2010new/</link>
      <pubDate>Fri, 01 Jan 2010 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/ambroise2010new/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Strategies for online inference of model-based clustering in large and growing networks </title>
      <link>https://cambroise.github.io/publication/zanghi2010strategies/</link>
      <pubDate>Fri, 01 Jan 2010 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/zanghi2010strategies/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Weighted-LASSO for structured network inference from time course data </title>
      <link>https://cambroise.github.io/publication/charbonnier2010weighted/</link>
      <pubDate>Fri, 01 Jan 2010 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/charbonnier2010weighted/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Bayesian methods for graph clustering </title>
      <link>https://cambroise.github.io/publication/latouche2009bayesian/</link>
      <pubDate>Thu, 01 Jan 2009 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/latouche2009bayesian/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Inferring sparse Gaussian graphical models with latent structure </title>
      <link>https://cambroise.github.io/publication/ambroise2009inferring/</link>
      <pubDate>Thu, 01 Jan 2009 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/ambroise2009inferring/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Overlapping stochastic block models </title>
      <link>https://cambroise.github.io/publication/latouche2009overlapping/</link>
      <pubDate>Thu, 01 Jan 2009 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/latouche2009overlapping/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Simone: Statistical inference for modular networks </title>
      <link>https://cambroise.github.io/publication/chiquet2009simone/</link>
      <pubDate>Thu, 01 Jan 2009 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/chiquet2009simone/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Spatial Data Clustering </title>
      <link>https://cambroise.github.io/publication/ambroise2009spatial/</link>
      <pubDate>Thu, 01 Jan 2009 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/ambroise2009spatial/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Fast online graph clustering via Erdos--R{e}nyi mixture </title>
      <link>https://cambroise.github.io/publication/zanghi2008fast/</link>
      <pubDate>Tue, 01 Jan 2008 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/zanghi2008fast/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Identification of functional modules based on transcriptional regulation structure </title>
      <link>https://cambroise.github.io/publication/birmele2008identification/</link>
      <pubDate>Tue, 01 Jan 2008 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/birmele2008identification/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Strategies for online inference of network mixture </title>
      <link>https://cambroise.github.io/publication/zanghi2008strategies/</link>
      <pubDate>Tue, 01 Jan 2008 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/zanghi2008strategies/</guid>
      <description></description>
    </item>
    
    <item>
      <title> An online classification EM algorithm based on the mixture model </title>
      <link>https://cambroise.github.io/publication/same2007online/</link>
      <pubDate>Mon, 01 Jan 2007 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/same2007online/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Parsimonious additive models </title>
      <link>https://cambroise.github.io/publication/avalos2007parsimonious/</link>
      <pubDate>Mon, 01 Jan 2007 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/avalos2007parsimonious/</guid>
      <description></description>
    </item>
    
    <item>
      <title> A classification EM algorithm for binned data </title>
      <link>https://cambroise.github.io/publication/same2006classification/</link>
      <pubDate>Sun, 01 Jan 2006 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/same2006classification/</guid>
      <description></description>
    </item>
    
    <item>
      <title> A Content-Based Image Retrieval System for Osteo-Articular Applications </title>
      <link>https://cambroise.github.io/publication/najjar2006content/</link>
      <pubDate>Sun, 01 Jan 2006 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/najjar2006content/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Driving Hierarchy Construction via Supervised Learning: Application to Osteo-Articular Medical Images Database </title>
      <link>https://cambroise.github.io/publication/yousfi2006driving/</link>
      <pubDate>Sun, 01 Jan 2006 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/yousfi2006driving/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Feature selection in robust clustering based on Laplace mixture </title>
      <link>https://cambroise.github.io/publication/cord2006feature/</link>
      <pubDate>Sun, 01 Jan 2006 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/cord2006feature/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Selection bias in working with the top genes in supervised classification of tissue samples </title>
      <link>https://cambroise.github.io/publication/zhu2006selection/</link>
      <pubDate>Sun, 01 Jan 2006 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/zhu2006selection/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Supervised learning for guiding hierarchy construction: Application to osteo-articular medical images database </title>
      <link>https://cambroise.github.io/publication/yousfi2006supervised/</link>
      <pubDate>Sun, 01 Jan 2006 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/yousfi2006supervised/</guid>
      <description></description>
    </item>
    
    <item>
      <title> A decision tree classifier for vehicle failure isolation </title>
      <link>https://cambroise.github.io/publication/charkaoui2005decision/</link>
      <pubDate>Sat, 01 Jan 2005 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/charkaoui2005decision/</guid>
      <description></description>
    </item>
    
    <item>
      <title> A mixture model-based on-line CEM algorithm </title>
      <link>https://cambroise.github.io/publication/same2005mixture/</link>
      <pubDate>Sat, 01 Jan 2005 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/same2005mixture/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Analyse de Donn{e}es et Data Mining </title>
      <link>https://cambroise.github.io/publication/govaert2005analyse/</link>
      <pubDate>Sat, 01 Jan 2005 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/govaert2005analyse/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Analyzing microarray gene expression data </title>
      <link>https://cambroise.github.io/publication/mclachlan2005analyzing/</link>
      <pubDate>Sat, 01 Jan 2005 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/mclachlan2005analyzing/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Books for review If you would like to review a book, and thereby to retain it for your collection, please contact the Book Reviews Editor, whose details can be found by clicking on &lt;U&#43;2018&gt;books currently available&lt;U&#43;2019&gt;in the information on the Royal Statistical Society&lt;U&#43;2019&gt;s Web site </title>
      <link>https://cambroise.github.io/publication/chaumont2005books/</link>
      <pubDate>Sat, 01 Jan 2005 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/chaumont2005books/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Data Analysis Tools for DNA Microarrays/Analyzing Microarray Gene Expression Data </title>
      <link>https://cambroise.github.io/publication/ziegel2005data/</link>
      <pubDate>Sat, 01 Jan 2005 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/ziegel2005data/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Discrimination par mod{e}les additifs parcimonieux </title>
      <link>https://cambroise.github.io/publication/avalos2005discrimination/</link>
      <pubDate>Sat, 01 Jan 2005 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/avalos2005discrimination/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Model selection via penalization in the additive Cox model </title>
      <link>https://cambroise.github.io/publication/avalos2005model/</link>
      <pubDate>Sat, 01 Jan 2005 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/avalos2005model/</guid>
      <description></description>
    </item>
    
    <item>
      <title> P{e}nalisation l1 pour les MAG </title>
      <link>https://cambroise.github.io/publication/avalos2005penalisation/</link>
      <pubDate>Sat, 01 Jan 2005 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/avalos2005penalisation/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Pattern recognition method for off-board automotive vehicle failure isolation </title>
      <link>https://cambroise.github.io/publication/charkaoui2005pattern/</link>
      <pubDate>Sat, 01 Jan 2005 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/charkaoui2005pattern/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Use of Micro Array Data via Model-based Classification in the Study and Prediction of Survival from Lung Cancer </title>
      <link>https://cambroise.github.io/publication/jones2005use/</link>
      <pubDate>Sat, 01 Jan 2005 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/jones2005use/</guid>
      <description></description>
    </item>
    
    <item>
      <title> A mixture model approach for on-line clustering </title>
      <link>https://cambroise.github.io/publication/same2004mixture/</link>
      <pubDate>Thu, 01 Jan 2004 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/same2004mixture/</guid>
      <description></description>
    </item>
    
    <item>
      <title> G{e}n{e}ralisation du lasso aux modeles additifs </title>
      <link>https://cambroise.github.io/publication/avalos2004generalisation/</link>
      <pubDate>Thu, 01 Jan 2004 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/avalos2004generalisation/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Microarrays in gene expression studies </title>
      <link>https://cambroise.github.io/publication/mclachlan2004microarrays/</link>
      <pubDate>Thu, 01 Jan 2004 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/mclachlan2004microarrays/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Mixture model approach for acoustic emission control of pressure equipment </title>
      <link>https://cambroise.github.io/publication/hamdan2004mixture/</link>
      <pubDate>Thu, 01 Jan 2004 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/hamdan2004mixture/</guid>
      <description></description>
    </item>
    
    <item>
      <title> On the simultaneous use of clinical and microarray expression data in the cluster analysis of tissue samples </title>
      <link>https://cambroise.github.io/publication/mclachlan2004simultaneous/</link>
      <pubDate>Thu, 01 Jan 2004 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/mclachlan2004simultaneous/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Penalized additive logistic regression for cardiovascular risk prediction </title>
      <link>https://cambroise.github.io/publication/avalos2004penalized/</link>
      <pubDate>Thu, 01 Jan 2004 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/avalos2004penalized/</guid>
      <description></description>
    </item>
    
    <item>
      <title> A mixture model approach for binned data clustering </title>
      <link>https://cambroise.github.io/publication/same2003mixture/</link>
      <pubDate>Wed, 01 Jan 2003 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/same2003mixture/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Feature selection for semisupervised learning applied to image retrieval </title>
      <link>https://cambroise.github.io/publication/najjar2003feature/</link>
      <pubDate>Wed, 01 Jan 2003 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/najjar2003feature/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Image retrieval using mixture models and em algorithm </title>
      <link>https://cambroise.github.io/publication/najjar2003image/</link>
      <pubDate>Wed, 01 Jan 2003 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/najjar2003image/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Regularization methods for additive models </title>
      <link>https://cambroise.github.io/publication/avalos2003regularization/</link>
      <pubDate>Wed, 01 Jan 2003 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/avalos2003regularization/</guid>
      <description></description>
    </item>
    
    <item>
      <title> A Semi-supervised Learning Approach to Image Retrieval </title>
      <link>https://cambroise.github.io/publication/najjar2002semi/</link>
      <pubDate>Tue, 01 Jan 2002 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/najjar2002semi/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Clustering and Models </title>
      <link>https://cambroise.github.io/publication/ambroise2002clustering/</link>
      <pubDate>Tue, 01 Jan 2002 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/ambroise2002clustering/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Selection bias in gene extraction on the basis of microarray gene-expression data </title>
      <link>https://cambroise.github.io/publication/ambroise2002selection/</link>
      <pubDate>Tue, 01 Jan 2002 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/ambroise2002selection/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Boosting mixture models for semi-supervised learning </title>
      <link>https://cambroise.github.io/publication/grandvalet2001boosting/</link>
      <pubDate>Mon, 01 Jan 2001 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/grandvalet2001boosting/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Learning from an imprecise teacher: probabilistic and evidential approaches </title>
      <link>https://cambroise.github.io/publication/ambroise2001learning/</link>
      <pubDate>Mon, 01 Jan 2001 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/ambroise2001learning/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Prediction of ozone peaks by mixture models </title>
      <link>https://cambroise.github.io/publication/ambroise2001prediction/</link>
      <pubDate>Mon, 01 Jan 2001 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/ambroise2001prediction/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Semi-supervised marginboost </title>
      <link>https://cambroise.github.io/publication/d2001semi/</link>
      <pubDate>Mon, 01 Jan 2001 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/d2001semi/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Semi-supervised marginboost </title>
      <link>https://cambroise.github.io/publication/grandvalet2001semi/</link>
      <pubDate>Mon, 01 Jan 2001 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/grandvalet2001semi/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Clustering by maximizing a fuzzy classification maximum likelihood criterion </title>
      <link>https://cambroise.github.io/publication/ambroise2000clustering/</link>
      <pubDate>Sat, 01 Jan 2000 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/ambroise2000clustering/</guid>
      <description></description>
    </item>
    
    <item>
      <title> EM algorithm for partially known labels </title>
      <link>https://cambroise.github.io/publication/ambroise2000algorithm/</link>
      <pubDate>Sat, 01 Jan 2000 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/ambroise2000algorithm/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Hierarchical clustering of self-organizing maps for cloud classification </title>
      <link>https://cambroise.github.io/publication/ambroise2000hierarchical/</link>
      <pubDate>Sat, 01 Jan 2000 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/ambroise2000hierarchical/</guid>
      <description></description>
    </item>
    
    <item>
      <title> An iterative algorithm for spatial clustering </title>
      <link>https://cambroise.github.io/publication/ambroise1998iterative/</link>
      <pubDate>Thu, 01 Jan 1998 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/ambroise1998iterative/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Convergence of an EM-type algorithm for spatial clustering </title>
      <link>https://cambroise.github.io/publication/ambroise1998convergence/</link>
      <pubDate>Thu, 01 Jan 1998 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/ambroise1998convergence/</guid>
      <description></description>
    </item>
    
    <item>
      <title> The EM Algorithm and Extensions, by GM McLachlan and T. Krishnan </title>
      <link>https://cambroise.github.io/publication/ambroise1998algorithm/</link>
      <pubDate>Thu, 01 Jan 1998 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/ambroise1998algorithm/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Clustering of spatial data by the EM algorithm </title>
      <link>https://cambroise.github.io/publication/ambroise1997clustering/</link>
      <pubDate>Wed, 01 Jan 1997 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/ambroise1997clustering/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Analyzing dissimilarity matrices via Kohonen maps </title>
      <link>https://cambroise.github.io/publication/ambroise1996analyzing/</link>
      <pubDate>Mon, 01 Jan 1996 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/ambroise1996analyzing/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Approche probabiliste en classification automatique et contraintes de voisinage </title>
      <link>https://cambroise.github.io/publication/ambroise1996approche/</link>
      <pubDate>Mon, 01 Jan 1996 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/ambroise1996approche/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Constrained clustering and Kohonen self-organizing maps </title>
      <link>https://cambroise.github.io/publication/ambroise1996constrained/</link>
      <pubDate>Mon, 01 Jan 1996 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/publication/ambroise1996constrained/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://cambroise.github.io/courses/introduction-machine-learning/jupyter-gaussian/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/courses/introduction-machine-learning/jupyter-gaussian/</guid>
      <description>&lt;h2 id=&#34;simulations-gaussiennes&#34;&gt;Simulations Gaussiennes&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import multivariate_normal
from matplotlib.colors import ListedColormap

mycolormap = ListedColormap([&#39;#FF0000&#39;, &#39;#0000FF&#39;])

Exchoice = 1  # Choisir l&#39;exemple

##############################################
# Data set generation using scipy.stats.multivariate_normal
#############################################

def generate_data(n1, n2, mu1, cov1, mu2, cov2):
    &amp;quot;&amp;quot;&amp;quot;
    Generates simulated data from two multivariate normal distributions.

    Parameters:
    - n1: Size of sample 1
    - n2: Size of sample 2
    - mu1: Mean vector for group 1 (in 2 dimensions)
    - cov1: Covariance matrix for group 1 (in 2 dimensions)
    - mu2: Mean vector for group 2 (in 2 dimensions)
    - cov2: Covariance matrix for group 2 (in 2 dimensions)
    
    Returns:
    - X: Concatenated data matrix for both groups (n1 + n2, 2)
    - Y: Associated class vector (0 for group 1, 1 for group 2)
    &amp;quot;&amp;quot;&amp;quot;
    # Generate data for each group
    xG1 = multivariate_normal(mean=mu1, cov=cov1).rvs(n1)
    xG2 = multivariate_normal(mean=mu2, cov=cov2).rvs(n2)
    
    # Concatenate data from both groups
    X = np.concatenate((xG1, xG2), axis=0)
    
    # Create class vector (labels)
    Y = np.array([0] * n1 + [1] * n2)
    
    return X, Y

n1, n2 = 50, 150
mu1 = [2, 2]  # Moyenne G1
cov1 = [[1, 0], [0, 1]]  # Matrice de covariance G1 (indépendance)
    
mu2 = [0, 0]  # Moyenne G2
cov2 = [[1, 0], [0, 2]]  # Matrice de covariance G2 (indépendance mais variance différente)
    
X, Y = generate_data(n1, n2, mu1, cov1, mu2, cov2)


# Visualisation des données générées
plt.scatter(X[:, 0], X[:, 1], c=Y, cmap=mycolormap)
plt.title(&#39;Raw data&#39;)
plt.grid()
plt.xlabel(&#39;x1&#39;)
plt.ylabel(&#39;x2&#39;)
plt.show()




&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./index_1_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;simulation-de-lexemple-2-gaussiennes-côte-côte-puis-affichage-de-la-posterior&#34;&gt;Simulation de l&amp;rsquo;exemple 2 (gaussiennes côte-côte) puis affichage de la posterior&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$P(X = x \mid Y = 1)$ est une gaussienne bivariée avec une moyenne $\mu_1 = (\mu_{11}, \mu_{12})$ et une matrice de covariance $\Sigma_1$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;La densité a posteriori $P(Y = 1 \mid X = x)$ peut être calculée à l&amp;rsquo;aide du théorème de Bayes :&lt;/p&gt;
&lt;p&gt;$$
P(Y = 1 \mid X = x) = \frac{P(X = x \mid Y = 1) P(Y = 1)}{P(X = x)}
$$&lt;/p&gt;
&lt;p&gt;où :&lt;/p&gt;
&lt;p&gt;$$
P(X = x) = P(X = x \mid Y = 1) P(Y = 1) + P(X = x \mid Y = 0) P(Y = 0)
$$&lt;/p&gt;
&lt;p&gt;Cela permet de calculer la probabilité a posteriori en fonction des probabilités conditionnelles et des probabilités a priori.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import multivariate_normal

class Bayes_Classifier:
    def __init__(self, mu1, cov1, mu2, cov2, p1=0.5):
        &amp;quot;&amp;quot;&amp;quot;
        Initializes the Bayes Classifier with parameters of two bivariate Gaussian distributions.

        Parameters:
        - mu1: Mean vector for class 1 (in 2 dimensions)
        - cov1: Covariance matrix for class 1 (in 2 dimensions)
        - mu2: Mean vector for class 0 (in 2 dimensions)
        - cov2: Covariance matrix for class 0 (in 2 dimensions)
        - p1: Prior probability of class 1 (default is 0.5)
        &amp;quot;&amp;quot;&amp;quot;
        self.mu1 = mu1
        self.cov1 = cov1
        self.mu2 = mu2
        self.cov2 = cov2
        self.p1 = p1
        self.p2 = 1 - p1
        self.rv1 = multivariate_normal(mu1, cov1, allow_singular=True)
        self.rv2 = multivariate_normal(mu2, cov2, allow_singular=True)

    def predict_proba(self, X):
        &amp;quot;&amp;quot;&amp;quot;
        Predicts the posterior probability of class 1 for the given data points.

        Parameters:
        - X: Data points (n_samples, 2)

        Returns:
        - p1_x: Posterior probability of class 1 for each data point
        &amp;quot;&amp;quot;&amp;quot;
        # Calculate likelihoods P(X | Y=1) and P(X | Y=0)
        px_1 = self.rv1.pdf(X)
        px_2 = self.rv2.pdf(X)

        # Total density P(X=x)
        px = px_1 * self.p1 + px_2 * self.p2

        # Posterior probability P(Y=1 | X=x) using Bayes&#39; theorem
        p1_x = (px_1 * self.p1) / px

        return p1_x


def display_posterior_density(X, Y, classifier):
    &amp;quot;&amp;quot;&amp;quot;
    Displays the posterior density P(Y=1 | X=x) using a contour plot.

    Parameters:
    - X: Data points (n_samples, 2)
    - Y: Class labels (n_samples,)
    - classifier: Bayes_Classifier object used to predict the posterior probability
    &amp;quot;&amp;quot;&amp;quot;
        # Define the grid for evaluating the posterior probability
    min_x1, min_x2 = np.min(X, axis=0) - 1
    max_x1, max_x2 = np.max(X, axis=0) + 1

    # Generate a grid of 2D points (mesh)
    x1, x2 = np.mgrid[min_x1:max_x1:.01, min_x2:max_x2:.01]
    pos = np.dstack((x1, x2))

    # Predict posterior probability of first class for each point in the grid
    p1_x = classifier.predict_proba(pos.reshape(-1, 2))
    if p1_x.ndim &amp;gt; 1 and p1_x.shape[1]&amp;gt;1:
        p1_x=p1_x[:,0]

    p1_x=p1_x.reshape(x1.shape)      

    # Display the posterior density in 2D
    plt.contourf(x1, x2, p1_x, levels=20, cmap=&#39;coolwarm&#39;)
    plt.colorbar(label=&#39;P(Y=1 | X=x)&#39;)
    plt.contour(x1, x2, p1_x, levels=[0.5], colors=&#39;black&#39;, linewidths=2)
    plt.title(&amp;quot;Posterior Density P(Y=1 | X=x)&amp;quot;)
    plt.xlabel(&amp;quot;x1&amp;quot;)
    plt.ylabel(&amp;quot;x2&amp;quot;)
    plt.scatter(X[:, 0], X[:, 1], marker=&#39;o&#39;, c=Y, cmap=&#39;coolwarm&#39;, label=&#39;Data Points&#39;)

    # Customize the plot
    plt.title(&amp;quot;Posterior Density P(Y=1 | X=x) with Data Points&amp;quot;)
    plt.xlabel(&amp;quot;x1&amp;quot;)
    plt.ylabel(&amp;quot;x2&amp;quot;)
    plt.legend()

    # Show the plot
    plt.show()

# Example usage
n1, n2 = 200, 200
mu1 = [2, 2]
cov1 = [[1, 0], [0, 1]]
mu2 = [0, 0]
cov2 = [[1, 0], [0, 2]]

X, Y = generate_data(n1, n2, mu1, cov1, mu2, cov2)
classifier = Bayes_Classifier(mu1, cov1, mu2, cov2)

# Display posterior density
display_posterior_density(X, Y, classifier)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./index_3_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;mon-classifieur-de-bayes-naif&#34;&gt;Mon Classifieur de Bayes Naif&lt;/h2&gt;
&lt;p&gt;Classifieur de Bayes Naif&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np

class BayesNaifGaussien:
    def __init__(self):
        self.mu_0 = None  # Moyenne pour la classe 0
        self.mu_1 = None  # Moyenne pour la classe 1
        self.sigma_0 = None  # Variance pour la classe 0
        self.sigma_1 = None  # Variance pour la classe 1
        self.p_y0 = None  # Probabilité a priori de Y=0
        self.p_y1 = None  # Probabilité a priori de Y=1

    def fit(self, X, Y):
        &amp;quot;&amp;quot;&amp;quot; Calcule les paramètres du modèle à partir des données X et Y &amp;quot;&amp;quot;&amp;quot;
        # Séparer les données selon les classes
        X_0 = X[Y == 0]  # Données pour la classe 0
        X_1 = X[Y == 1]  # Données pour la classe 1
        
        # Calculer les moyennes et variances pour chaque classe
        self.mu_0 = np.mean(X_0, axis=0)
        self.mu_1 = np.mean(X_1, axis=0)
        self.sigma_0 = np.var(X_0, axis=0)
        self.sigma_1 = np.var(X_1, axis=0)
        
        # Probabilités a priori P(Y=0) et P(Y=1)
        self.p_y0 = len(X_0) / len(X)
        self.p_y1 = len(X_1) / len(X)

    def gaussienne(self, x, mu, sigma):
        &amp;quot;&amp;quot;&amp;quot; Calcul de la densité de probabilité gaussienne pour un point x &amp;quot;&amp;quot;&amp;quot;
        return (1 / np.sqrt(2 * np.pi * sigma)) * np.exp(-0.5 * ((x - mu) ** 2) / sigma)

    def predict_proba(self, X):
        &amp;quot;&amp;quot;&amp;quot; Calcule les probabilités a posteriori P(Y=1 | X) pour chaque individu dans X &amp;quot;&amp;quot;&amp;quot;
        proba_y1_given_x = []
        
        for x in X:
            # Calculer P(X | Y=1) et P(X | Y=0) pour chaque variable
            p_x_given_y1 = np.prod(self.gaussienne(x, self.mu_1, self.sigma_1))
            p_x_given_y0 = np.prod(self.gaussienne(x, self.mu_0, self.sigma_0))
            
            # Densité totale P(X=x)
            p_x = p_x_given_y1 * self.p_y1 + p_x_given_y0 * self.p_y0
            
            # Calculer la probabilité a posteriori P(Y=1 | X=x) (Théorème de Bayes)
            p_y1_x = (p_x_given_y1 * self.p_y1) / p_x
            
            # Ajouter la probabilité pour cet individu
            proba_y1_given_x.append(p_y1_x)
        
        return np.array(proba_y1_given_x)

    def predict(self, X):
        &amp;quot;&amp;quot;&amp;quot; Prédit la classe (0 ou 1) pour chaque individu en fonction de P(Y=1 | X) &amp;quot;&amp;quot;&amp;quot;
        proba_y1 = self.predict_proba(X)
        return (proba_y1 &amp;gt;= 0.5).astype(int)

# Exemple d&#39;utilisation :

# Générer des données aléatoires (2 variables)
#np.random.seed(42)
#X = np.random.randn(100, 2)  # 100 individus, 2 variables
#Y = np.random.choice([0, 1], size=100)  # Classes aléatoires 0 ou 1

# Initialiser et entraîner le modèle
model = BayesNaifGaussien()
model.fit(X, Y)

# Prédire les probabilités a posteriori P(Y=1 | X)
proba_y1_given_x = model.predict_proba(X)

# Afficher les probabilités et les paramètres du modèle
print(&amp;quot;Probabilités P(Y=1 | X):&amp;quot;, proba_y1_given_x)
print(&amp;quot;Moyenne classe 0:&amp;quot;, model.mu_0)
print(&amp;quot;Moyenne classe 1:&amp;quot;, model.mu_1)
print(&amp;quot;Variance classe 0:&amp;quot;, model.sigma_0)
print(&amp;quot;Variance classe 1:&amp;quot;, model.sigma_1)


# Afficher les performances
error=(model.predict(X)!=Y).sum()/len(Y)
print(&amp;quot;Erreur empirique&amp;quot;,error)

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Probabilités P(Y=1 | X): [4.77129432e-02 1.88198612e-01 1.19806441e-02 1.61156057e-03
 7.57724865e-01 5.47979333e-03 2.86561102e-02 5.28354680e-03
 4.19710179e-03 2.32670361e-03 4.25351100e-05 2.75755452e-02
 1.66745301e-03 2.49435759e-01 1.00766811e-03 6.33178333e-02
 7.11726400e-02 8.06725044e-03 3.49788468e-02 2.33123217e-01
 1.85679153e-01 1.61388217e-02 4.73890955e-04 3.40903507e-03
 5.48351905e-02 6.11117153e-01 7.97042638e-04 2.39487337e-02
 1.81606673e-01 7.51506787e-01 1.44906911e-01 3.20614819e-03
 1.49182986e-02 2.36029577e-02 1.10421848e-03 4.06797993e-02
 2.76919459e-01 9.80876216e-03 1.51080835e-03 1.19535527e-02
 7.54920408e-02 2.37668468e-03 1.34267871e-01 1.50892025e-04
 2.82689122e-04 3.97560524e-01 1.85964298e-01 2.89790882e-02
 2.61032354e-01 4.09004267e-01 2.28426189e-04 2.69845210e-01
 6.10553550e-03 8.50242142e-01 2.03655676e-01 2.20047876e-02
 2.75311642e-02 9.77504492e-02 3.40259695e-01 3.73255885e-01
 1.52570034e-03 2.83882103e-01 1.47553228e-03 1.59946985e-02
 3.12402875e-03 9.51981172e-03 1.26780125e-01 1.38245003e-03
 1.45565399e-01 2.11117520e-01 4.74854726e-04 6.81187545e-02
 3.17382874e-01 1.45887411e-01 3.72396824e-03 1.00399080e-04
 4.13313282e-03 1.17479805e-03 6.78963271e-01 1.77858266e-02
 3.18035514e-01 1.04417578e-01 3.76900434e-02 3.05594629e-01
 8.03794773e-05 6.33186245e-01 4.86832863e-03 2.00066529e-02
 4.04799919e-03 2.19082430e-01 3.34528481e-01 6.23729351e-03
 1.66303929e-02 8.95526616e-03 3.98065567e-02 1.98639470e-01
 4.04561745e-02 5.82954816e-02 3.39805011e-03 2.09663974e-02
 2.53093383e-02 2.21555326e-02 1.09365256e-02 3.74422495e-02
 3.43872900e-04 1.89273542e-02 1.38456942e-03 4.80891278e-02
 1.07933445e-01 3.05213147e-01 6.69535889e-02 1.44887853e-03
 2.20980438e-03 3.40678910e-02 1.56474474e-02 4.55360912e-04
 1.52599230e-04 3.38423666e-02 5.30316981e-02 3.95061749e-03
 1.66579442e-03 2.99502034e-01 4.89077220e-01 7.24637743e-02
 3.72126273e-03 9.97495673e-01 5.81488703e-02 2.71824520e-03
 3.31314879e-03 1.87599392e-01 2.84798275e-02 2.23632018e-01
 3.60488048e-01 2.38428084e-01 6.82397142e-02 1.70207819e-04
 1.65429076e-02 8.97641020e-02 1.47437064e-02 1.02971798e-02
 8.66017352e-04 2.90444112e-01 2.73443451e-03 3.12627059e-02
 4.67172669e-02 1.67625382e-02 3.70097236e-02 1.45051358e-03
 3.23307861e-01 3.18419139e-03 2.22863238e-02 1.22510786e-03
 9.82459480e-02 1.11912747e-02 1.20060399e-01 1.62403349e-03
 2.90996052e-01 5.85842940e-02 7.63932976e-03 2.20318786e-02
 1.71423565e-02 1.87681205e-02 3.41083199e-02 2.05334553e-03
 4.36902743e-01 3.31158281e-03 1.42633539e-04 9.32139342e-04
 1.73175620e-02 2.04639977e-03 2.40672704e-05 2.58175562e-03
 2.00897353e-01 7.09373320e-02 1.93387721e-02 3.43676984e-02
 6.39837963e-03 2.08496946e-02 5.02641687e-02 4.40223776e-02
 9.23455857e-01 8.50272970e-01 1.20956546e-01 1.36344422e-01
 1.65764476e-01 6.13372600e-02 5.06173937e-03 3.54062021e-02
 1.70972598e-03 3.92652867e-01 1.50124067e-01 2.12859692e-02
 1.16263615e-02 4.21556783e-02 7.13342266e-02 2.05523669e-02
 6.42687173e-02 1.95607635e-01 2.36928616e-01 9.28496158e-02
 9.92363237e-01 9.99347763e-01 9.91116855e-01 9.87181820e-01
 9.57392190e-01 8.68372052e-01 9.97952198e-01 9.70984948e-01
 9.28307824e-01 9.85646977e-01 9.84346244e-01 9.96345508e-01
 9.65157702e-01 9.97200756e-01 9.59341537e-01 9.90321766e-01
 9.97160385e-01 9.99963510e-01 9.99954006e-01 8.65862808e-01
 9.72073336e-01 9.99887907e-01 7.29077611e-01 9.95690771e-01
 9.55895128e-01 9.94207279e-01 9.86176885e-01 9.97203715e-01
 9.72794523e-01 9.95845219e-01 9.31445242e-01 9.33333563e-01
 9.99562104e-01 9.99866670e-01 9.99837689e-01 9.75130335e-01
 9.97995441e-01 9.99417007e-01 9.96072060e-01 9.32357076e-01
 9.55016044e-01 6.53713472e-01 6.78432054e-01 9.84099988e-01
 1.38612115e-01 9.86935547e-01 9.30047978e-01 9.98871069e-01
 9.73709329e-01 9.88313439e-01 9.96964639e-01 9.99237123e-01
 9.92546248e-01 9.88113710e-01 9.99551783e-01 1.73016802e-01
 9.99651727e-01 9.99983129e-01 9.99789445e-01 9.78577405e-01
 9.55891996e-01 1.46452492e-01 9.98577642e-01 9.21704460e-01
 9.99953932e-01 9.99837455e-01 9.47543861e-01 9.03746805e-01
 9.99103334e-01 9.99792672e-01 9.99956825e-01 9.99997796e-01
 8.57309795e-01 6.71868292e-01 5.29623647e-01 4.96240922e-01
 9.99619180e-01 8.82698551e-01 7.67662542e-01 9.95413843e-01
 9.99991943e-01 9.17265098e-01 9.99470345e-01 8.39243490e-01
 9.99937582e-01 9.99482108e-01 9.97125578e-01 6.24979920e-01
 9.34161919e-01 9.95496765e-01 9.94315135e-01 8.83557684e-01
 8.97947696e-01 5.10179180e-01 9.94743249e-01 9.61605183e-01
 9.99644538e-01 9.88320148e-01 8.76415088e-01 9.59577794e-01
 9.50858263e-01 9.78410036e-01 9.99913326e-01 9.70535075e-01
 9.99969452e-01 9.75022751e-01 8.64891438e-01 9.82382548e-01
 9.96648735e-01 9.99061819e-01 3.99177158e-01 9.98788237e-01
 9.73770146e-01 9.99608663e-01 8.59156079e-01 9.99982757e-01
 9.99654456e-01 8.38934579e-01 8.74529610e-01 9.99903513e-01
 9.99044151e-01 7.21647474e-01 9.96229908e-01 4.32779327e-02
 9.99652362e-01 9.96643859e-01 5.28982658e-01 9.99655356e-01
 7.02083645e-01 9.99986917e-01 5.33882446e-01 3.51556956e-01
 2.58009589e-01 9.93309344e-01 9.99923436e-01 9.99929086e-01
 9.94525560e-01 9.89293529e-01 9.95676851e-01 9.98902457e-01
 6.56435543e-01 9.99881910e-01 9.20536849e-01 9.96802987e-01
 9.99722059e-01 9.99930401e-01 9.99997011e-01 8.70526167e-01
 9.63482657e-01 9.81299644e-01 9.99988951e-01 9.99666706e-01
 9.86843060e-01 9.86592838e-01 9.99742465e-01 1.54492741e-01
 3.62478287e-01 9.99781860e-01 1.29421022e-02 5.74462107e-01
 2.94746510e-01 3.23258171e-01 9.91575605e-01 9.57284443e-01
 9.99659660e-01 9.92712669e-01 9.71220091e-01 9.99939050e-01
 9.90235282e-01 6.12219544e-01 3.16340570e-01 5.57344204e-01
 9.99994937e-01 9.94087895e-01 9.02534982e-01 2.28010496e-01
 9.96876362e-01 9.99703341e-01 7.93473528e-01 3.83625876e-01
 9.99988105e-01 9.76297207e-01 9.84924536e-01 9.99999680e-01
 9.99962153e-01 8.96720802e-01 9.98621979e-01 9.98555326e-01
 9.95912839e-01 9.51539421e-01 3.72359423e-01 9.84742990e-01
 9.94566778e-01 7.34086111e-01 1.14850155e-01 9.87775139e-01
 9.99788052e-01 9.45727676e-01 8.42356808e-01 9.95216255e-01]
Moyenne classe 0: [2.04616058 1.91278979]
Moyenne classe 1: [-0.07510028 -0.14203197]
Variance classe 0: [0.97936836 0.97850395]
Variance classe 1: [0.89293017 2.05696981]
Erreur empirique 0.0675
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;erreur-en-apprentissage-et-test-pour-le-classifieur-de-bayes-naïf&#34;&gt;Erreur en apprentissage et test pour le classifieur de Bayes naïf&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=1/3, random_state=3)

from sklearn.naive_bayes import GaussianNB
gnb = GaussianNB();
gnbmod=gnb.fit(X_train, y_train);
y_pred_test = gnbmod.predict(X_test)
y_pred_train = gnbmod.predict(X_train)

E_test=(y_test != y_pred_test).sum()/len(y_test)
E_train=(y_train != y_pred_train).sum()/len(y_train)

print(&amp;quot;Error on the test set=&amp;quot;,E_test)
print(&amp;quot;Error on the train set=&amp;quot;,E_train)

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Error on the test set= 0.05970149253731343
Error on the train set= 0.07142857142857142
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;courbe-roc-et-auc-pour-une-validation-en-5-plis&#34;&gt;Courbe ROC et AUC pour une validation en 5 plis&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#Etude MM 2022
y=Y
#############################################
# Cross-validation sur la courbe ROC
############################################
import numpy as np
import matplotlib.pyplot as plt

from sklearn import svm, datasets
from sklearn.metrics import auc
from sklearn.metrics import RocCurveDisplay
from sklearn.model_selection import StratifiedKFold

n_samples, n_features = X.shape

# Run classifier with cross-validation and plot ROC curves
cv = StratifiedKFold(n_splits=6)
classifier = GaussianNB();

tprs = []
aucs = []
mean_fpr = np.linspace(0, 1, 100)

fig, ax = plt.subplots()
for i, (train, test) in enumerate(cv.split(X, y)):
    y_train=[y[i] for i in train];
    y_test=[y[i] for i in test];
    classifier.fit(X[train], y_train)
    viz = RocCurveDisplay.from_estimator(
        classifier,             # Le modèle entraîné
        X[test],                # Les données de test
        y_test,                 # Les étiquettes de test
        name=&#39;ROC fold {}&#39;.format(i),  # Nom de la courbe pour chaque fold
        alpha=0.3,              # Transparence
        lw=1,                   # Épaisseur de la ligne
        ax=ax                   # L&#39;axe de la figure pour tracer la courbe
        )       
    interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)
    interp_tpr[0] = 0.0
    tprs.append(interp_tpr)
    aucs.append(viz.roc_auc)

ax.plot([0, 1], [0, 1], linestyle=&#39;--&#39;, lw=2, color=&#39;r&#39;,
        label=&#39;Chance&#39;, alpha=.8)

mean_tpr = np.mean(tprs, axis=0)
mean_tpr[-1] = 1.0
mean_auc = auc(mean_fpr, mean_tpr)
std_auc = np.std(aucs)
ax.plot(mean_fpr, mean_tpr, color=&#39;b&#39;,
        label=r&#39;Mean ROC (AUC = %0.2f $\pm$ %0.2f)&#39; % (mean_auc, std_auc),
        lw=2, alpha=.8)

std_tpr = np.std(tprs, axis=0)
tprs_upper = np.minimum(mean_tpr + std_tpr, 1)
tprs_lower = np.maximum(mean_tpr - std_tpr, 0)
ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color=&#39;grey&#39;, alpha=.2,
                label=r&#39;$\pm$ 1 std. dev.&#39;)

ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05],
       title=&amp;quot;Receiver operating characteristic example&amp;quot;)
ax.legend(loc=&amp;quot;lower right&amp;quot;)
plt.show()
    

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./index_10_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;analyse-discriminante-linéaire&#34;&gt;Analyse discriminante linéaire&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Example usage
from sklearn.model_selection import train_test_split
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.metrics import accuracy_score

# Generate data
n1, n2 = 200, 200
mu1 = [2, 2]
cov1 = [[1, 0], [0, 1]]
mu2 = [0, 0]
cov2 = [[1, 0], [0, 2]]

X, Y = generate_data(n1, n2, mu1, cov1, mu2, cov2)

# Split the data into train and test sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)

# Train LDA model
lda = LinearDiscriminantAnalysis()
lda.fit(X_train, Y_train)

# Predict on test set
Y_pred = lda.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(Y_test, Y_pred)
print(f&amp;quot;LDA Classification Accuracy: {accuracy:.2f}&amp;quot;)

# Display posterior density
display_posterior_density(X, Y, lda)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;LDA Classification Accuracy: 0.93
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./index_12_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Nous remarquons la frontière linéaire liée à l&amp;rsquo;égalité des matrices de covariance&lt;/p&gt;
&lt;h2 id=&#34;analyse-discriminante-quadratique&#34;&gt;Analyse discriminante quadratique&lt;/h2&gt;
&lt;p&gt;Les matrices de variances-covariances sont laissées libres et la frontière est une hyperbole (proche de la frontière obtenu par le classifieur de Bayes naïf)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Example usage with QDA
from sklearn.model_selection import train_test_split
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
from sklearn.metrics import accuracy_score

# Generate data
n1, n2 = 200, 200
mu1 = [2, 2]
cov1 = [[1, 0], [0, 1]]
mu2 = [0, 0]
cov2 = [[1, 0], [0, 2]]

X, Y = generate_data(n1, n2, mu1, cov1, mu2, cov2)

# Split the data into train and test sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)

# Train QDA model
qda = QuadraticDiscriminantAnalysis()
qda.fit(X_train, Y_train)

# Predict on test set
Y_pred = qda.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(Y_test, Y_pred)
print(f&amp;quot;QDA Classification Accuracy: {accuracy:.2f}&amp;quot;)

# Display posterior density
display_posterior_density(X, Y, qda)

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;QDA Classification Accuracy: 0.93
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./index_14_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;les-plus-proches-voisins&#34;&gt;Les plus proches voisins&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Example usage with KNN
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# Generate data
n1, n2 = 200, 200
mu1 = [2, 2]
cov1 = [[1, 0], [0, 1]]
mu2 = [0, 0]
cov2 = [[1, 0], [0, 2]]

X, Y = generate_data(n1, n2, mu1, cov1, mu2, cov2)

# Split the data into train and test sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)

# Train KNN model
knn = KNeighborsClassifier(n_neighbors=5)  # You can adjust the number of neighbors (k)
knn.fit(X_train, Y_train)

# Predict on test set
Y_pred = knn.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(Y_test, Y_pred)
print(f&amp;quot;KNN Classification Accuracy: {accuracy:.2f}&amp;quot;)

# Display posterior density
display_posterior_density(X, Y, knn)

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;KNN Classification Accuracy: 0.85
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./index_16_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;la-régression-logistique&#34;&gt;La régression logistique&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Example usage with Logistic Regression
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Generate data
n1, n2 = 200, 200
mu1 = [2, 2]
cov1 = [[1, 0], [0, 1]]
mu2 = [0, 0]
cov2 = [[1, 0], [0, 2]]

X, Y = generate_data(n1, n2, mu1, cov1, mu2, cov2)

# Split the data into train and test sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)

# Train Logistic Regression model
log_reg = LogisticRegression()
log_reg.fit(X_train, Y_train)

# Predict on test set
Y_pred = log_reg.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(Y_test, Y_pred)
print(f&amp;quot;Logistic Regression Classification Accuracy: {accuracy:.2f}&amp;quot;)

# Display posterior density
display_posterior_density(X, Y, log_reg)

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Logistic Regression Classification Accuracy: 0.88
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./index_18_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;comparaison-des-méthodes-par-auc-et-erreur-de-classification&#34;&gt;Comparaison des méthodes par AUC et erreur de classification&lt;/h2&gt;
&lt;p&gt;Remarquons les grandes différences entre ensemble d&amp;rsquo;apprentissage et test.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import roc_auc_score, make_scorer
from sklearn.datasets import make_classification

# Générer des données simulées
n1, n2 = 200, 200
X, Y = make_classification(n_samples=n1 + n2, n_features=2, n_informative=2, n_redundant=0, random_state=42)

# Split les données en train/test
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)

# Initialiser les modèles
models = {
    &#39;Naive Bayes&#39;: GaussianNB(),
    &#39;LDA&#39;: LinearDiscriminantAnalysis(),
    &#39;Logistic Regression&#39;: LogisticRegression(),
    &#39;QDA&#39;: QuadraticDiscriminantAnalysis(),
    &#39;KNN&#39;: KNeighborsClassifier(n_neighbors=5)
}

# Fonction pour calculer les AUC avec validation croisée (10-fold)
def calculate_auc_cv(model, X_train, Y_train, X_test, Y_test):
    scorer = make_scorer(roc_auc_score)
    train_scores = cross_val_score(model, X_train, Y_train, cv=10, scoring=scorer)
    test_scores = cross_val_score(model, X_test, Y_test, cv=10, scoring=scorer)
    return train_scores, test_scores

# Stocker les résultats des AUC
results_train = []
results_test = []
model_names = []

# Calculer l&#39;AUC pour chaque modèle
for name, model in models.items():
    train_auc, test_auc = calculate_auc_cv(model, X_train, Y_train, X_test, Y_test)
    results_train.append(train_auc)
    results_test.append(test_auc)
    model_names.append(name)

# Créer les boxplots pour visualiser les AUC en train et en test
fig, ax = plt.subplots(figsize=(12, 6))

# Préparer les données pour afficher les boxplots côte à côte
positions_train = np.arange(1, len(models) * 2, 2)  # Positions pour les boxplots en train
positions_test = np.arange(2, len(models) * 2 + 1, 2)  # Positions pour les boxplots en test

# Boxplot pour l&#39;AUC sur les données de train (en bleu)
bp_train = ax.boxplot(results_train, positions=positions_train, widths=0.6, patch_artist=True,
                      boxprops=dict(facecolor=&amp;quot;lightblue&amp;quot;), medianprops=dict(color=&amp;quot;blue&amp;quot;))

# Boxplot pour l&#39;AUC sur les données de test (en orange)
bp_test = ax.boxplot(results_test, positions=positions_test, widths=0.6, patch_artist=True,
                     boxprops=dict(facecolor=&amp;quot;orange&amp;quot;), medianprops=dict(color=&amp;quot;red&amp;quot;))

# Ajouter une légende pour les boxplots
ax.legend([bp_train[&amp;quot;boxes&amp;quot;][0], bp_test[&amp;quot;boxes&amp;quot;][0]], [&#39;Train&#39;, &#39;Test&#39;], loc=&#39;upper right&#39;)

# Ajuster les labels et le titre
ax.set_xticks(np.arange(1.5, len(models) * 2, 2))
ax.set_xticklabels(model_names)
ax.set_title(&amp;quot;Comparaison des modèles - AUC en Train et Test (10-fold CV)&amp;quot;)
ax.set_ylabel(&amp;quot;AUC Score&amp;quot;)

plt.tight_layout()
plt.show()

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./index_20_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import make_scorer
from sklearn.datasets import make_classification

# Générer des données simulées
n1, n2 = 200, 200
X, Y = make_classification(n_samples=n1 + n2, n_features=2, n_informative=2, n_redundant=0, random_state=42)

# Split les données en train/test
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)

# Initialiser les modèles
models = {
    &#39;Naive Bayes&#39;: GaussianNB(),
    &#39;LDA&#39;: LinearDiscriminantAnalysis(),
    &#39;Logistic Regression&#39;: LogisticRegression(),
    &#39;QDA&#39;: QuadraticDiscriminantAnalysis(),
    &#39;KNN&#39;: KNeighborsClassifier(n_neighbors=5)
}

# Fonction pour calculer les erreurs avec validation croisée (10-fold)
def calculate_error_cv(model, X_train, Y_train, X_test, Y_test):
    scorer = make_scorer(lambda y_true, y_pred: 1 - np.mean(y_true == y_pred))  # Calculer l&#39;erreur
    train_errors = cross_val_score(model, X_train, Y_train, cv=10, scoring=scorer)
    test_errors = cross_val_score(model, X_test, Y_test, cv=10, scoring=scorer)
    return train_errors, test_errors

# Stocker les résultats des erreurs
results_train = []
results_test = []
model_names = []

# Calculer l&#39;erreur pour chaque modèle
for name, model in models.items():
    train_error, test_error = calculate_error_cv(model, X_train, Y_train, X_test, Y_test)
    results_train.append(train_error)
    results_test.append(test_error)
    model_names.append(name)

# Créer les boxplots pour visualiser les erreurs en train et en test
fig, ax = plt.subplots(figsize=(12, 6))

# Préparer les données pour afficher les boxplots côte à côte
positions_train = np.arange(1, len(models) * 2, 2)  # Positions pour les boxplots en train
positions_test = np.arange(2, len(models) * 2 + 1, 2)  # Positions pour les boxplots en test

# Boxplot pour l&#39;erreur sur les données de train (en bleu)
bp_train = ax.boxplot(results_train, positions=positions_train, widths=0.6, patch_artist=True,
                      boxprops=dict(facecolor=&amp;quot;lightblue&amp;quot;), medianprops=dict(color=&amp;quot;blue&amp;quot;))

# Boxplot pour l&#39;erreur sur les données de test (en orange)
bp_test = ax.boxplot(results_test, positions=positions_test, widths=0.6, patch_artist=True,
                     boxprops=dict(facecolor=&amp;quot;orange&amp;quot;), medianprops=dict(color=&amp;quot;red&amp;quot;))

# Ajouter une légende pour les boxplots
ax.legend([bp_train[&amp;quot;boxes&amp;quot;][0], bp_test[&amp;quot;boxes&amp;quot;][0]], [&#39;Train&#39;, &#39;Test&#39;], loc=&#39;upper right&#39;)

# Ajuster les labels et le titre
ax.set_xticks(np.arange(1.5, len(models) * 2, 2))
ax.set_xticklabels(model_names)
ax.set_title(&amp;quot;Comparaison des modèles - Erreur en Train et Test (10-fold CV)&amp;quot;)
ax.set_ylabel(&amp;quot;Erreur de Classification&amp;quot;)

plt.tight_layout()
plt.show()

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./index_21_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://cambroise.github.io/courses/introduction-machine-learning/jupyter-tree/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://cambroise.github.io/courses/introduction-machine-learning/jupyter-tree/</guid>
      <description>&lt;h1 id=&#34;tree-bagging-et-forêts-aléatoires&#34;&gt;Tree, bagging et forêts aléatoires&lt;/h1&gt;
&lt;h2 id=&#34;simulation&#34;&gt;Simulation&lt;/h2&gt;
&lt;p&gt;Soit X un vecteur gaussien de loi $\mathcal N_p(\boldsymbol \mu, \boldsymbol \Sigma)$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import random as rd
from scipy.stats import multivariate_normal
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
import math

NG1, NG2 = 50, 150
mu1 = [2, 2]  # Moyenne G1
cov1 = [[1, 0], [0, 1]]  # Matrice de covariance G1 (indépendance)
    
mu2 = [0, 0]  # Moyenne G2
cov2 = [[1, 0], [0, 2]]  # Matrice de covariance G2 (indépendance mais variance différente)
    
# Génération des données
xG1 = multivariate_normal(mean=mu1, cov=cov1).rvs(NG1)
xG2 = multivariate_normal(mean=mu2, cov=cov2).rvs(NG2)
    
X = np.concatenate((xG1, xG2), axis=0)
Y = [0] * NG1 + [1] * NG2


# Visualisation des données générées
mycolormap = ListedColormap([&#39;#FF0000&#39;, &#39;#0000FF&#39;])
plt.scatter(X[:, 0], X[:, 1], c=Y, cmap=mycolormap)
plt.title(&#39;Raw data&#39;)
plt.grid()
plt.xlabel(&#39;x1&#39;)
plt.ylabel(&#39;x2&#39;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./index_2_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;classification-avec-arbre-de-décision&#34;&gt;Classification avec arbre de décision&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# Initialiser le classificateur
dt_classifier = DecisionTreeClassifier()

# Entraîner le modèle
dt_classifier.fit(X, Y)

# Prédire les probabilités sur le jeu d&#39;entraînement
pred_proba_train = dt_classifier.predict_proba(X)

# Obtenir les prédictions des classes
pred_class_train = np.argmax(pred_proba_train, axis=1)

# Calculer l&#39;accuracy sur le jeu d&#39;entraînement
train_accuracy = accuracy_score(Y, pred_class_train)

# Afficher l&#39;erreur d&#39;entraînement
train_error = 1 - train_accuracy
print(f&amp;quot;Erreur sur l&#39;ensemble d&#39;entraînement : {train_error:.2f}&amp;quot;)

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Erreur sur l&#39;ensemble d&#39;entraînement : 0.00
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;tracé-de-la-frontière-de-décision-avec-contourf&#34;&gt;Tracé de la frontière de décision avec contourf&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# ---- Traçage de la frontière de décision ----
# Créer une grille de points couvrant l&#39;espace des données
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),
                     np.arange(y_min, y_max, 0.1))

# Prédire la classe pour chaque point de la grille
Z = dt_classifier.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

# Tracer la frontière de décision avec contourf
plt.contourf(xx, yy, Z, alpha=0.4, cmap=mycolormap)

# Tracer les points de données
plt.scatter(X[:, 0], X[:, 1], c=Y, edgecolors=&#39;k&#39;, cmap=mycolormap)
plt.title(&#39;Decision Boundary of the Decision Tree&#39;)
plt.grid()
plt.xlabel(&#39;x1&#39;)
plt.ylabel(&#39;x2&#39;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./index_6_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;help(DecisionTreeClassifier)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Help on class DecisionTreeClassifier in module sklearn.tree._classes:

class DecisionTreeClassifier(sklearn.base.ClassifierMixin, BaseDecisionTree)
 |  DecisionTreeClassifier(*, criterion=&#39;gini&#39;, splitter=&#39;best&#39;, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, class_weight=None, ccp_alpha=0.0, monotonic_cst=None)
 |
 |  A decision tree classifier.
 |
 |  Read more in the :ref:`User Guide &amp;lt;tree&amp;gt;`.
 |
 |  Parameters
 |  ----------
 |  criterion : {&amp;quot;gini&amp;quot;, &amp;quot;entropy&amp;quot;, &amp;quot;log_loss&amp;quot;}, default=&amp;quot;gini&amp;quot;
 |      The function to measure the quality of a split. Supported criteria are
 |      &amp;quot;gini&amp;quot; for the Gini impurity and &amp;quot;log_loss&amp;quot; and &amp;quot;entropy&amp;quot; both for the
 |      Shannon information gain, see :ref:`tree_mathematical_formulation`.
 |
 |  splitter : {&amp;quot;best&amp;quot;, &amp;quot;random&amp;quot;}, default=&amp;quot;best&amp;quot;
 |      The strategy used to choose the split at each node. Supported
 |      strategies are &amp;quot;best&amp;quot; to choose the best split and &amp;quot;random&amp;quot; to choose
 |      the best random split.
 |
 |  max_depth : int, default=None
 |      The maximum depth of the tree. If None, then nodes are expanded until
 |      all leaves are pure or until all leaves contain less than
 |      min_samples_split samples.
 |
 |  min_samples_split : int or float, default=2
 |      The minimum number of samples required to split an internal node:
 |
 |      - If int, then consider `min_samples_split` as the minimum number.
 |      - If float, then `min_samples_split` is a fraction and
 |        `ceil(min_samples_split * n_samples)` are the minimum
 |        number of samples for each split.
 |
 |      .. versionchanged:: 0.18
 |         Added float values for fractions.
 |
 |  min_samples_leaf : int or float, default=1
 |      The minimum number of samples required to be at a leaf node.
 |      A split point at any depth will only be considered if it leaves at
 |      least ``min_samples_leaf`` training samples in each of the left and
 |      right branches.  This may have the effect of smoothing the model,
 |      especially in regression.
 |
 |      - If int, then consider `min_samples_leaf` as the minimum number.
 |      - If float, then `min_samples_leaf` is a fraction and
 |        `ceil(min_samples_leaf * n_samples)` are the minimum
 |        number of samples for each node.
 |
 |      .. versionchanged:: 0.18
 |         Added float values for fractions.
 |
 |  min_weight_fraction_leaf : float, default=0.0
 |      The minimum weighted fraction of the sum total of weights (of all
 |      the input samples) required to be at a leaf node. Samples have
 |      equal weight when sample_weight is not provided.
 |
 |  max_features : int, float or {&amp;quot;sqrt&amp;quot;, &amp;quot;log2&amp;quot;}, default=None
 |      The number of features to consider when looking for the best split:
 |
 |          - If int, then consider `max_features` features at each split.
 |          - If float, then `max_features` is a fraction and
 |            `max(1, int(max_features * n_features_in_))` features are considered at
 |            each split.
 |          - If &amp;quot;sqrt&amp;quot;, then `max_features=sqrt(n_features)`.
 |          - If &amp;quot;log2&amp;quot;, then `max_features=log2(n_features)`.
 |          - If None, then `max_features=n_features`.
 |
 |      Note: the search for a split does not stop until at least one
 |      valid partition of the node samples is found, even if it requires to
 |      effectively inspect more than ``max_features`` features.
 |
 |  random_state : int, RandomState instance or None, default=None
 |      Controls the randomness of the estimator. The features are always
 |      randomly permuted at each split, even if ``splitter`` is set to
 |      ``&amp;quot;best&amp;quot;``. When ``max_features &amp;lt; n_features``, the algorithm will
 |      select ``max_features`` at random at each split before finding the best
 |      split among them. But the best found split may vary across different
 |      runs, even if ``max_features=n_features``. That is the case, if the
 |      improvement of the criterion is identical for several splits and one
 |      split has to be selected at random. To obtain a deterministic behaviour
 |      during fitting, ``random_state`` has to be fixed to an integer.
 |      See :term:`Glossary &amp;lt;random_state&amp;gt;` for details.
 |
 |  max_leaf_nodes : int, default=None
 |      Grow a tree with ``max_leaf_nodes`` in best-first fashion.
 |      Best nodes are defined as relative reduction in impurity.
 |      If None then unlimited number of leaf nodes.
 |
 |  min_impurity_decrease : float, default=0.0
 |      A node will be split if this split induces a decrease of the impurity
 |      greater than or equal to this value.
 |
 |      The weighted impurity decrease equation is the following::
 |
 |          N_t / N * (impurity - N_t_R / N_t * right_impurity
 |                              - N_t_L / N_t * left_impurity)
 |
 |      where ``N`` is the total number of samples, ``N_t`` is the number of
 |      samples at the current node, ``N_t_L`` is the number of samples in the
 |      left child, and ``N_t_R`` is the number of samples in the right child.
 |
 |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,
 |      if ``sample_weight`` is passed.
 |
 |      .. versionadded:: 0.19
 |
 |  class_weight : dict, list of dict or &amp;quot;balanced&amp;quot;, default=None
 |      Weights associated with classes in the form ``{class_label: weight}``.
 |      If None, all classes are supposed to have weight one. For
 |      multi-output problems, a list of dicts can be provided in the same
 |      order as the columns of y.
 |
 |      Note that for multioutput (including multilabel) weights should be
 |      defined for each class of every column in its own dict. For example,
 |      for four-class multilabel classification weights should be
 |      [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of
 |      [{1:1}, {2:5}, {3:1}, {4:1}].
 |
 |      The &amp;quot;balanced&amp;quot; mode uses the values of y to automatically adjust
 |      weights inversely proportional to class frequencies in the input data
 |      as ``n_samples / (n_classes * np.bincount(y))``
 |
 |      For multi-output, the weights of each column of y will be multiplied.
 |
 |      Note that these weights will be multiplied with sample_weight (passed
 |      through the fit method) if sample_weight is specified.
 |
 |  ccp_alpha : non-negative float, default=0.0
 |      Complexity parameter used for Minimal Cost-Complexity Pruning. The
 |      subtree with the largest cost complexity that is smaller than
 |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See
 |      :ref:`minimal_cost_complexity_pruning` for details.
 |
 |      .. versionadded:: 0.22
 |
 |  monotonic_cst : array-like of int of shape (n_features), default=None
 |      Indicates the monotonicity constraint to enforce on each feature.
 |        - 1: monotonic increase
 |        - 0: no constraint
 |        - -1: monotonic decrease
 |
 |      If monotonic_cst is None, no constraints are applied.
 |
 |      Monotonicity constraints are not supported for:
 |        - multiclass classifications (i.e. when `n_classes &amp;gt; 2`),
 |        - multioutput classifications (i.e. when `n_outputs_ &amp;gt; 1`),
 |        - classifications trained on data with missing values.
 |
 |      The constraints hold over the probability of the positive class.
 |
 |      Read more in the :ref:`User Guide &amp;lt;monotonic_cst_gbdt&amp;gt;`.
 |
 |      .. versionadded:: 1.4
 |
 |  Attributes
 |  ----------
 |  classes_ : ndarray of shape (n_classes,) or list of ndarray
 |      The classes labels (single output problem),
 |      or a list of arrays of class labels (multi-output problem).
 |
 |  feature_importances_ : ndarray of shape (n_features,)
 |      The impurity-based feature importances.
 |      The higher, the more important the feature.
 |      The importance of a feature is computed as the (normalized)
 |      total reduction of the criterion brought by that feature.  It is also
 |      known as the Gini importance [4]_.
 |
 |      Warning: impurity-based feature importances can be misleading for
 |      high cardinality features (many unique values). See
 |      :func:`sklearn.inspection.permutation_importance` as an alternative.
 |
 |  max_features_ : int
 |      The inferred value of max_features.
 |
 |  n_classes_ : int or list of int
 |      The number of classes (for single output problems),
 |      or a list containing the number of classes for each
 |      output (for multi-output problems).
 |
 |  n_features_in_ : int
 |      Number of features seen during :term:`fit`.
 |
 |      .. versionadded:: 0.24
 |
 |  feature_names_in_ : ndarray of shape (`n_features_in_`,)
 |      Names of features seen during :term:`fit`. Defined only when `X`
 |      has feature names that are all strings.
 |
 |      .. versionadded:: 1.0
 |
 |  n_outputs_ : int
 |      The number of outputs when ``fit`` is performed.
 |
 |  tree_ : Tree instance
 |      The underlying Tree object. Please refer to
 |      ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and
 |      :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`
 |      for basic usage of these attributes.
 |
 |  See Also
 |  --------
 |  DecisionTreeRegressor : A decision tree regressor.
 |
 |  Notes
 |  -----
 |  The default values for the parameters controlling the size of the trees
 |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and
 |  unpruned trees which can potentially be very large on some data sets. To
 |  reduce memory consumption, the complexity and size of the trees should be
 |  controlled by setting those parameter values.
 |
 |  The :meth:`predict` method operates using the :func:`numpy.argmax`
 |  function on the outputs of :meth:`predict_proba`. This means that in
 |  case the highest predicted probabilities are tied, the classifier will
 |  predict the tied class with the lowest index in :term:`classes_`.
 |
 |  References
 |  ----------
 |
 |  .. [1] https://en.wikipedia.org/wiki/Decision_tree_learning
 |
 |  .. [2] L. Breiman, J. Friedman, R. Olshen, and C. Stone, &amp;quot;Classification
 |         and Regression Trees&amp;quot;, Wadsworth, Belmont, CA, 1984.
 |
 |  .. [3] T. Hastie, R. Tibshirani and J. Friedman. &amp;quot;Elements of Statistical
 |         Learning&amp;quot;, Springer, 2009.
 |
 |  .. [4] L. Breiman, and A. Cutler, &amp;quot;Random Forests&amp;quot;,
 |         https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm
 |
 |  Examples
 |  --------
 |  &amp;gt;&amp;gt;&amp;gt; from sklearn.datasets import load_iris
 |  &amp;gt;&amp;gt;&amp;gt; from sklearn.model_selection import cross_val_score
 |  &amp;gt;&amp;gt;&amp;gt; from sklearn.tree import DecisionTreeClassifier
 |  &amp;gt;&amp;gt;&amp;gt; clf = DecisionTreeClassifier(random_state=0)
 |  &amp;gt;&amp;gt;&amp;gt; iris = load_iris()
 |  &amp;gt;&amp;gt;&amp;gt; cross_val_score(clf, iris.data, iris.target, cv=10)
 |  ...                             # doctest: +SKIP
 |  ...
 |  array([ 1.     ,  0.93...,  0.86...,  0.93...,  0.93...,
 |          0.93...,  0.93...,  1.     ,  0.93...,  1.      ])
 |
 |  Method resolution order:
 |      DecisionTreeClassifier
 |      sklearn.base.ClassifierMixin
 |      BaseDecisionTree
 |      sklearn.base.MultiOutputMixin
 |      sklearn.base.BaseEstimator
 |      sklearn.utils._estimator_html_repr._HTMLDocumentationLinkMixin
 |      sklearn.utils._metadata_requests._MetadataRequester
 |      builtins.object
 |
 |  Methods defined here:
 |
 |  __init__(self, *, criterion=&#39;gini&#39;, splitter=&#39;best&#39;, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, class_weight=None, ccp_alpha=0.0, monotonic_cst=None)
 |      Initialize self.  See help(type(self)) for accurate signature.
 |
 |  fit(self, X, y, sample_weight=None, check_input=True)
 |      Build a decision tree classifier from the training set (X, y).
 |
 |      Parameters
 |      ----------
 |      X : {array-like, sparse matrix} of shape (n_samples, n_features)
 |          The training input samples. Internally, it will be converted to
 |          ``dtype=np.float32`` and if a sparse matrix is provided
 |          to a sparse ``csc_matrix``.
 |
 |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)
 |          The target values (class labels) as integers or strings.
 |
 |      sample_weight : array-like of shape (n_samples,), default=None
 |          Sample weights. If None, then samples are equally weighted. Splits
 |          that would create child nodes with net zero or negative weight are
 |          ignored while searching for a split in each node. Splits are also
 |          ignored if they would result in any single class carrying a
 |          negative weight in either child node.
 |
 |      check_input : bool, default=True
 |          Allow to bypass several input checking.
 |          Don&#39;t use this parameter unless you know what you&#39;re doing.
 |
 |      Returns
 |      -------
 |      self : DecisionTreeClassifier
 |          Fitted estimator.
 |
 |  predict_log_proba(self, X)
 |      Predict class log-probabilities of the input samples X.
 |
 |      Parameters
 |      ----------
 |      X : {array-like, sparse matrix} of shape (n_samples, n_features)
 |          The input samples. Internally, it will be converted to
 |          ``dtype=np.float32`` and if a sparse matrix is provided
 |          to a sparse ``csr_matrix``.
 |
 |      Returns
 |      -------
 |      proba : ndarray of shape (n_samples, n_classes) or list of n_outputs             such arrays if n_outputs &amp;gt; 1
 |          The class log-probabilities of the input samples. The order of the
 |          classes corresponds to that in the attribute :term:`classes_`.
 |
 |  predict_proba(self, X, check_input=True)
 |      Predict class probabilities of the input samples X.
 |
 |      The predicted class probability is the fraction of samples of the same
 |      class in a leaf.
 |
 |      Parameters
 |      ----------
 |      X : {array-like, sparse matrix} of shape (n_samples, n_features)
 |          The input samples. Internally, it will be converted to
 |          ``dtype=np.float32`` and if a sparse matrix is provided
 |          to a sparse ``csr_matrix``.
 |
 |      check_input : bool, default=True
 |          Allow to bypass several input checking.
 |          Don&#39;t use this parameter unless you know what you&#39;re doing.
 |
 |      Returns
 |      -------
 |      proba : ndarray of shape (n_samples, n_classes) or list of n_outputs             such arrays if n_outputs &amp;gt; 1
 |          The class probabilities of the input samples. The order of the
 |          classes corresponds to that in the attribute :term:`classes_`.
 |
 |  set_fit_request(self: sklearn.tree._classes.DecisionTreeClassifier, *, check_input: Union[bool, NoneType, str] = &#39;$UNCHANGED$&#39;, sample_weight: Union[bool, NoneType, str] = &#39;$UNCHANGED$&#39;) -&amp;gt; sklearn.tree._classes.DecisionTreeClassifier
 |      Request metadata passed to the ``fit`` method.
 |
 |      Note that this method is only relevant if
 |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).
 |      Please see :ref:`User Guide &amp;lt;metadata_routing&amp;gt;` on how the routing
 |      mechanism works.
 |
 |      The options for each parameter are:
 |
 |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.
 |
 |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.
 |
 |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
 |
 |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.
 |
 |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the
 |      existing request. This allows you to change the request for some
 |      parameters and not others.
 |
 |      .. versionadded:: 1.3
 |
 |      .. note::
 |          This method is only relevant if this estimator is used as a
 |          sub-estimator of a meta-estimator, e.g. used inside a
 |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.
 |
 |      Parameters
 |      ----------
 |      check_input : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED
 |          Metadata routing for ``check_input`` parameter in ``fit``.
 |
 |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED
 |          Metadata routing for ``sample_weight`` parameter in ``fit``.
 |
 |      Returns
 |      -------
 |      self : object
 |          The updated object.
 |
 |  set_predict_proba_request(self: sklearn.tree._classes.DecisionTreeClassifier, *, check_input: Union[bool, NoneType, str] = &#39;$UNCHANGED$&#39;) -&amp;gt; sklearn.tree._classes.DecisionTreeClassifier
 |      Request metadata passed to the ``predict_proba`` method.
 |
 |      Note that this method is only relevant if
 |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).
 |      Please see :ref:`User Guide &amp;lt;metadata_routing&amp;gt;` on how the routing
 |      mechanism works.
 |
 |      The options for each parameter are:
 |
 |      - ``True``: metadata is requested, and passed to ``predict_proba`` if provided. The request is ignored if metadata is not provided.
 |
 |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``predict_proba``.
 |
 |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
 |
 |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.
 |
 |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the
 |      existing request. This allows you to change the request for some
 |      parameters and not others.
 |
 |      .. versionadded:: 1.3
 |
 |      .. note::
 |          This method is only relevant if this estimator is used as a
 |          sub-estimator of a meta-estimator, e.g. used inside a
 |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.
 |
 |      Parameters
 |      ----------
 |      check_input : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED
 |          Metadata routing for ``check_input`` parameter in ``predict_proba``.
 |
 |      Returns
 |      -------
 |      self : object
 |          The updated object.
 |
 |  set_predict_request(self: sklearn.tree._classes.DecisionTreeClassifier, *, check_input: Union[bool, NoneType, str] = &#39;$UNCHANGED$&#39;) -&amp;gt; sklearn.tree._classes.DecisionTreeClassifier
 |      Request metadata passed to the ``predict`` method.
 |
 |      Note that this method is only relevant if
 |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).
 |      Please see :ref:`User Guide &amp;lt;metadata_routing&amp;gt;` on how the routing
 |      mechanism works.
 |
 |      The options for each parameter are:
 |
 |      - ``True``: metadata is requested, and passed to ``predict`` if provided. The request is ignored if metadata is not provided.
 |
 |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``predict``.
 |
 |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
 |
 |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.
 |
 |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the
 |      existing request. This allows you to change the request for some
 |      parameters and not others.
 |
 |      .. versionadded:: 1.3
 |
 |      .. note::
 |          This method is only relevant if this estimator is used as a
 |          sub-estimator of a meta-estimator, e.g. used inside a
 |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.
 |
 |      Parameters
 |      ----------
 |      check_input : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED
 |          Metadata routing for ``check_input`` parameter in ``predict``.
 |
 |      Returns
 |      -------
 |      self : object
 |          The updated object.
 |
 |  set_score_request(self: sklearn.tree._classes.DecisionTreeClassifier, *, sample_weight: Union[bool, NoneType, str] = &#39;$UNCHANGED$&#39;) -&amp;gt; sklearn.tree._classes.DecisionTreeClassifier
 |      Request metadata passed to the ``score`` method.
 |
 |      Note that this method is only relevant if
 |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).
 |      Please see :ref:`User Guide &amp;lt;metadata_routing&amp;gt;` on how the routing
 |      mechanism works.
 |
 |      The options for each parameter are:
 |
 |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.
 |
 |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.
 |
 |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
 |
 |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.
 |
 |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the
 |      existing request. This allows you to change the request for some
 |      parameters and not others.
 |
 |      .. versionadded:: 1.3
 |
 |      .. note::
 |          This method is only relevant if this estimator is used as a
 |          sub-estimator of a meta-estimator, e.g. used inside a
 |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.
 |
 |      Parameters
 |      ----------
 |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED
 |          Metadata routing for ``sample_weight`` parameter in ``score``.
 |
 |      Returns
 |      -------
 |      self : object
 |          The updated object.
 |
 |  ----------------------------------------------------------------------
 |  Data and other attributes defined here:
 |
 |  __abstractmethods__ = frozenset()
 |
 |  __annotations__ = {&#39;_parameter_constraints&#39;: &amp;lt;class &#39;dict&#39;&amp;gt;}
 |
 |  ----------------------------------------------------------------------
 |  Methods inherited from sklearn.base.ClassifierMixin:
 |
 |  score(self, X, y, sample_weight=None)
 |      Return the mean accuracy on the given test data and labels.
 |
 |      In multi-label classification, this is the subset accuracy
 |      which is a harsh metric since you require for each sample that
 |      each label set be correctly predicted.
 |
 |      Parameters
 |      ----------
 |      X : array-like of shape (n_samples, n_features)
 |          Test samples.
 |
 |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)
 |          True labels for `X`.
 |
 |      sample_weight : array-like of shape (n_samples,), default=None
 |          Sample weights.
 |
 |      Returns
 |      -------
 |      score : float
 |          Mean accuracy of ``self.predict(X)`` w.r.t. `y`.
 |
 |  ----------------------------------------------------------------------
 |  Data descriptors inherited from sklearn.base.ClassifierMixin:
 |
 |  __dict__
 |      dictionary for instance variables
 |
 |  __weakref__
 |      list of weak references to the object
 |
 |  ----------------------------------------------------------------------
 |  Methods inherited from BaseDecisionTree:
 |
 |  apply(self, X, check_input=True)
 |      Return the index of the leaf that each sample is predicted as.
 |
 |      .. versionadded:: 0.17
 |
 |      Parameters
 |      ----------
 |      X : {array-like, sparse matrix} of shape (n_samples, n_features)
 |          The input samples. Internally, it will be converted to
 |          ``dtype=np.float32`` and if a sparse matrix is provided
 |          to a sparse ``csr_matrix``.
 |
 |      check_input : bool, default=True
 |          Allow to bypass several input checking.
 |          Don&#39;t use this parameter unless you know what you&#39;re doing.
 |
 |      Returns
 |      -------
 |      X_leaves : array-like of shape (n_samples,)
 |          For each datapoint x in X, return the index of the leaf x
 |          ends up in. Leaves are numbered within
 |          ``[0; self.tree_.node_count)``, possibly with gaps in the
 |          numbering.
 |
 |  cost_complexity_pruning_path(self, X, y, sample_weight=None)
 |      Compute the pruning path during Minimal Cost-Complexity Pruning.
 |
 |      See :ref:`minimal_cost_complexity_pruning` for details on the pruning
 |      process.
 |
 |      Parameters
 |      ----------
 |      X : {array-like, sparse matrix} of shape (n_samples, n_features)
 |          The training input samples. Internally, it will be converted to
 |          ``dtype=np.float32`` and if a sparse matrix is provided
 |          to a sparse ``csc_matrix``.
 |
 |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)
 |          The target values (class labels) as integers or strings.
 |
 |      sample_weight : array-like of shape (n_samples,), default=None
 |          Sample weights. If None, then samples are equally weighted. Splits
 |          that would create child nodes with net zero or negative weight are
 |          ignored while searching for a split in each node. Splits are also
 |          ignored if they would result in any single class carrying a
 |          negative weight in either child node.
 |
 |      Returns
 |      -------
 |      ccp_path : :class:`~sklearn.utils.Bunch`
 |          Dictionary-like object, with the following attributes.
 |
 |          ccp_alphas : ndarray
 |              Effective alphas of subtree during pruning.
 |
 |          impurities : ndarray
 |              Sum of the impurities of the subtree leaves for the
 |              corresponding alpha value in ``ccp_alphas``.
 |
 |  decision_path(self, X, check_input=True)
 |      Return the decision path in the tree.
 |
 |      .. versionadded:: 0.18
 |
 |      Parameters
 |      ----------
 |      X : {array-like, sparse matrix} of shape (n_samples, n_features)
 |          The input samples. Internally, it will be converted to
 |          ``dtype=np.float32`` and if a sparse matrix is provided
 |          to a sparse ``csr_matrix``.
 |
 |      check_input : bool, default=True
 |          Allow to bypass several input checking.
 |          Don&#39;t use this parameter unless you know what you&#39;re doing.
 |
 |      Returns
 |      -------
 |      indicator : sparse matrix of shape (n_samples, n_nodes)
 |          Return a node indicator CSR matrix where non zero elements
 |          indicates that the samples goes through the nodes.
 |
 |  get_depth(self)
 |      Return the depth of the decision tree.
 |
 |      The depth of a tree is the maximum distance between the root
 |      and any leaf.
 |
 |      Returns
 |      -------
 |      self.tree_.max_depth : int
 |          The maximum depth of the tree.
 |
 |  get_n_leaves(self)
 |      Return the number of leaves of the decision tree.
 |
 |      Returns
 |      -------
 |      self.tree_.n_leaves : int
 |          Number of leaves.
 |
 |  predict(self, X, check_input=True)
 |      Predict class or regression value for X.
 |
 |      For a classification model, the predicted class for each sample in X is
 |      returned. For a regression model, the predicted value based on X is
 |      returned.
 |
 |      Parameters
 |      ----------
 |      X : {array-like, sparse matrix} of shape (n_samples, n_features)
 |          The input samples. Internally, it will be converted to
 |          ``dtype=np.float32`` and if a sparse matrix is provided
 |          to a sparse ``csr_matrix``.
 |
 |      check_input : bool, default=True
 |          Allow to bypass several input checking.
 |          Don&#39;t use this parameter unless you know what you&#39;re doing.
 |
 |      Returns
 |      -------
 |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)
 |          The predicted classes, or the predict values.
 |
 |  ----------------------------------------------------------------------
 |  Readonly properties inherited from BaseDecisionTree:
 |
 |  feature_importances_
 |      Return the feature importances.
 |
 |      The importance of a feature is computed as the (normalized) total
 |      reduction of the criterion brought by that feature.
 |      It is also known as the Gini importance.
 |
 |      Warning: impurity-based feature importances can be misleading for
 |      high cardinality features (many unique values). See
 |      :func:`sklearn.inspection.permutation_importance` as an alternative.
 |
 |      Returns
 |      -------
 |      feature_importances_ : ndarray of shape (n_features,)
 |          Normalized total reduction of criteria by feature
 |          (Gini importance).
 |
 |  ----------------------------------------------------------------------
 |  Methods inherited from sklearn.base.BaseEstimator:
 |
 |  __getstate__(self)
 |      Helper for pickle.
 |
 |  __repr__(self, N_CHAR_MAX=700)
 |      Return repr(self).
 |
 |  __setstate__(self, state)
 |
 |  __sklearn_clone__(self)
 |
 |  get_params(self, deep=True)
 |      Get parameters for this estimator.
 |
 |      Parameters
 |      ----------
 |      deep : bool, default=True
 |          If True, will return the parameters for this estimator and
 |          contained subobjects that are estimators.
 |
 |      Returns
 |      -------
 |      params : dict
 |          Parameter names mapped to their values.
 |
 |  set_params(self, **params)
 |      Set the parameters of this estimator.
 |
 |      The method works on simple estimators as well as on nested objects
 |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have
 |      parameters of the form ``&amp;lt;component&amp;gt;__&amp;lt;parameter&amp;gt;`` so that it&#39;s
 |      possible to update each component of a nested object.
 |
 |      Parameters
 |      ----------
 |      **params : dict
 |          Estimator parameters.
 |
 |      Returns
 |      -------
 |      self : estimator instance
 |          Estimator instance.
 |
 |  ----------------------------------------------------------------------
 |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:
 |
 |  get_metadata_routing(self)
 |      Get metadata routing of this object.
 |
 |      Please check :ref:`User Guide &amp;lt;metadata_routing&amp;gt;` on how the routing
 |      mechanism works.
 |
 |      Returns
 |      -------
 |      routing : MetadataRequest
 |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating
 |          routing information.
 |
 |  ----------------------------------------------------------------------
 |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:
 |
 |  __init_subclass__(**kwargs) from abc.ABCMeta
 |      Set the ``set_{method}_request`` methods.
 |
 |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It
 |      looks for the information available in the set default values which are
 |      set using ``__metadata_request__*`` class attributes, or inferred
 |      from method signatures.
 |
 |      The ``__metadata_request__*`` class attributes are used when a method
 |      does not explicitly accept a metadata through its arguments or if the
 |      developer would like to specify a request value for those metadata
 |      which are different from the default ``None``.
 |
 |      References
 |      ----------
 |      .. [1] https://www.python.org/dev/peps/pep-0487
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Les paramètres de l&amp;rsquo;arbre de décision sont fixés mais peuvent être changés. Pour obtenir un arbre plus simple, il est par exemple possible d&amp;rsquo;augmenter le nombre de noeuds minimum par feuille.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dt_classifier = DecisionTreeClassifier()

# Afficher les paramètres du modèle
params = dt_classifier.get_params()
print(params)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;{&#39;ccp_alpha&#39;: 0.0, &#39;class_weight&#39;: None, &#39;criterion&#39;: &#39;gini&#39;, &#39;max_depth&#39;: None, &#39;max_features&#39;: None, &#39;max_leaf_nodes&#39;: None, &#39;min_impurity_decrease&#39;: 0.0, &#39;min_samples_leaf&#39;: 1, &#39;min_samples_split&#39;: 2, &#39;min_weight_fraction_leaf&#39;: 0.0, &#39;monotonic_cst&#39;: None, &#39;random_state&#39;: None, &#39;splitter&#39;: &#39;best&#39;}
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dt_classifier.set_params(min_samples_split= 30)
&lt;/code&gt;&lt;/pre&gt;
&lt;style&gt;#sk-container-id-2 {
  /* Definition of color scheme common for light and dark mode */
  --sklearn-color-text: black;
  --sklearn-color-line: gray;
  /* Definition of color scheme for unfitted estimators */
  --sklearn-color-unfitted-level-0: #fff5e6;
  --sklearn-color-unfitted-level-1: #f6e4d2;
  --sklearn-color-unfitted-level-2: #ffe0b3;
  --sklearn-color-unfitted-level-3: chocolate;
  /* Definition of color scheme for fitted estimators */
  --sklearn-color-fitted-level-0: #f0f8ff;
  --sklearn-color-fitted-level-1: #d4ebff;
  --sklearn-color-fitted-level-2: #b3dbfd;
  --sklearn-color-fitted-level-3: cornflowerblue;

  /* Specific color for light theme */
  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));
  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-icon: #696969;

  @media (prefers-color-scheme: dark) {
    /* Redefinition of color scheme for dark theme */
    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));
    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-icon: #878787;
  }
}

#sk-container-id-2 {
  color: var(--sklearn-color-text);
}

#sk-container-id-2 pre {
  padding: 0;
}

#sk-container-id-2 input.sk-hidden--visually {
  border: 0;
  clip: rect(1px 1px 1px 1px);
  clip: rect(1px, 1px, 1px, 1px);
  height: 1px;
  margin: -1px;
  overflow: hidden;
  padding: 0;
  position: absolute;
  width: 1px;
}

#sk-container-id-2 div.sk-dashed-wrapped {
  border: 1px dashed var(--sklearn-color-line);
  margin: 0 0.4em 0.5em 0.4em;
  box-sizing: border-box;
  padding-bottom: 0.4em;
  background-color: var(--sklearn-color-background);
}

#sk-container-id-2 div.sk-container {
  /* jupyter&#39;s `normalize.less` sets `[hidden] { display: none; }`
     but bootstrap.min.css set `[hidden] { display: none !important; }`
     so we also need the `!important` here to be able to override the
     default hidden behavior on the sphinx rendered scikit-learn.org.
     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */
  display: inline-block !important;
  position: relative;
}

#sk-container-id-2 div.sk-text-repr-fallback {
  display: none;
}

div.sk-parallel-item,
div.sk-serial,
div.sk-item {
  /* draw centered vertical line to link estimators */
  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));
  background-size: 2px 100%;
  background-repeat: no-repeat;
  background-position: center center;
}

/* Parallel-specific style estimator block */

#sk-container-id-2 div.sk-parallel-item::after {
  content: &#34;&#34;;
  width: 100%;
  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);
  flex-grow: 1;
}

#sk-container-id-2 div.sk-parallel {
  display: flex;
  align-items: stretch;
  justify-content: center;
  background-color: var(--sklearn-color-background);
  position: relative;
}

#sk-container-id-2 div.sk-parallel-item {
  display: flex;
  flex-direction: column;
}

#sk-container-id-2 div.sk-parallel-item:first-child::after {
  align-self: flex-end;
  width: 50%;
}

#sk-container-id-2 div.sk-parallel-item:last-child::after {
  align-self: flex-start;
  width: 50%;
}

#sk-container-id-2 div.sk-parallel-item:only-child::after {
  width: 0;
}

/* Serial-specific style estimator block */

#sk-container-id-2 div.sk-serial {
  display: flex;
  flex-direction: column;
  align-items: center;
  background-color: var(--sklearn-color-background);
  padding-right: 1em;
  padding-left: 1em;
}


/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is
clickable and can be expanded/collapsed.
- Pipeline and ColumnTransformer use this feature and define the default style
- Estimators will overwrite some part of the style using the `sk-estimator` class
*/

/* Pipeline and ColumnTransformer style (default) */

#sk-container-id-2 div.sk-toggleable {
  /* Default theme specific background. It is overwritten whether we have a
  specific estimator or a Pipeline/ColumnTransformer */
  background-color: var(--sklearn-color-background);
}

/* Toggleable label */
#sk-container-id-2 label.sk-toggleable__label {
  cursor: pointer;
  display: block;
  width: 100%;
  margin-bottom: 0;
  padding: 0.5em;
  box-sizing: border-box;
  text-align: center;
}

#sk-container-id-2 label.sk-toggleable__label-arrow:before {
  /* Arrow on the left of the label */
  content: &#34;▸&#34;;
  float: left;
  margin-right: 0.25em;
  color: var(--sklearn-color-icon);
}

#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {
  color: var(--sklearn-color-text);
}

/* Toggleable content - dropdown */

#sk-container-id-2 div.sk-toggleable__content {
  max-height: 0;
  max-width: 0;
  overflow: hidden;
  text-align: left;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-2 div.sk-toggleable__content.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-2 div.sk-toggleable__content pre {
  margin: 0.2em;
  border-radius: 0.25em;
  color: var(--sklearn-color-text);
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-2 div.sk-toggleable__content.fitted pre {
  /* unfitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {
  /* Expand drop-down */
  max-height: 200px;
  max-width: 100%;
  overflow: auto;
}

#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {
  content: &#34;▾&#34;;
}

/* Pipeline/ColumnTransformer-specific style */

#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator-specific style */

/* Colorize estimator box */
#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

#sk-container-id-2 div.sk-label label.sk-toggleable__label,
#sk-container-id-2 div.sk-label label {
  /* The background is the default theme color */
  color: var(--sklearn-color-text-on-default-background);
}

/* On hover, darken the color of the background */
#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

/* Label box, darken color on hover, fitted */
#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator label */

#sk-container-id-2 div.sk-label label {
  font-family: monospace;
  font-weight: bold;
  display: inline-block;
  line-height: 1.2em;
}

#sk-container-id-2 div.sk-label-container {
  text-align: center;
}

/* Estimator-specific */
#sk-container-id-2 div.sk-estimator {
  font-family: monospace;
  border: 1px dotted var(--sklearn-color-border-box);
  border-radius: 0.25em;
  box-sizing: border-box;
  margin-bottom: 0.5em;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-2 div.sk-estimator.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

/* on hover */
#sk-container-id-2 div.sk-estimator:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-2 div.sk-estimator.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Specification for estimator info (e.g. &#34;i&#34; and &#34;?&#34;) */

/* Common style for &#34;i&#34; and &#34;?&#34; */

.sk-estimator-doc-link,
a:link.sk-estimator-doc-link,
a:visited.sk-estimator-doc-link {
  float: right;
  font-size: smaller;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1em;
  height: 1em;
  width: 1em;
  text-decoration: none !important;
  margin-left: 1ex;
  /* unfitted */
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
  color: var(--sklearn-color-unfitted-level-1);
}

.sk-estimator-doc-link.fitted,
a:link.sk-estimator-doc-link.fitted,
a:visited.sk-estimator-doc-link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
div.sk-estimator:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover,
div.sk-label-container:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover,
div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

/* Span, style for the box shown on hovering the info icon */
.sk-estimator-doc-link span {
  display: none;
  z-index: 9999;
  position: relative;
  font-weight: normal;
  right: .2ex;
  padding: .5ex;
  margin: .5ex;
  width: min-content;
  min-width: 20ex;
  max-width: 50ex;
  color: var(--sklearn-color-text);
  box-shadow: 2pt 2pt 4pt #999;
  /* unfitted */
  background: var(--sklearn-color-unfitted-level-0);
  border: .5pt solid var(--sklearn-color-unfitted-level-3);
}

.sk-estimator-doc-link.fitted span {
  /* fitted */
  background: var(--sklearn-color-fitted-level-0);
  border: var(--sklearn-color-fitted-level-3);
}

.sk-estimator-doc-link:hover span {
  display: block;
}

/* &#34;?&#34;-specific style due to the `&lt;a&gt;` HTML tag */

#sk-container-id-2 a.estimator_doc_link {
  float: right;
  font-size: 1rem;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1rem;
  height: 1rem;
  width: 1rem;
  text-decoration: none;
  /* unfitted */
  color: var(--sklearn-color-unfitted-level-1);
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
}

#sk-container-id-2 a.estimator_doc_link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
#sk-container-id-2 a.estimator_doc_link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

#sk-container-id-2 a.estimator_doc_link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
}
&lt;/style&gt;&lt;div id=&#34;sk-container-id-2&#34; class=&#34;sk-top-container&#34;&gt;&lt;div class=&#34;sk-text-repr-fallback&#34;&gt;&lt;pre&gt;DecisionTreeClassifier(min_samples_split=30)&lt;/pre&gt;&lt;b&gt;In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. &lt;br /&gt;On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.&lt;/b&gt;&lt;/div&gt;&lt;div class=&#34;sk-container&#34; hidden&gt;&lt;div class=&#34;sk-item&#34;&gt;&lt;div class=&#34;sk-estimator fitted sk-toggleable&#34;&gt;&lt;input class=&#34;sk-toggleable__control sk-hidden--visually&#34; id=&#34;sk-estimator-id-2&#34; type=&#34;checkbox&#34; checked&gt;&lt;label for=&#34;sk-estimator-id-2&#34; class=&#34;sk-toggleable__label fitted sk-toggleable__label-arrow fitted&#34;&gt;&amp;nbsp;&amp;nbsp;DecisionTreeClassifier&lt;a class=&#34;sk-estimator-doc-link fitted&#34; rel=&#34;noreferrer&#34; target=&#34;_blank&#34; href=&#34;https://scikit-learn.org/1.5/modules/generated/sklearn.tree.DecisionTreeClassifier.html&#34;&gt;?&lt;span&gt;Documentation for DecisionTreeClassifier&lt;/span&gt;&lt;/a&gt;&lt;span class=&#34;sk-estimator-doc-link fitted&#34;&gt;i&lt;span&gt;Fitted&lt;/span&gt;&lt;/span&gt;&lt;/label&gt;&lt;div class=&#34;sk-toggleable__content fitted&#34;&gt;&lt;pre&gt;DecisionTreeClassifier(min_samples_split=30)&lt;/pre&gt;&lt;/div&gt; &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Entraîner le modèle
dt_classifier.fit(X, Y)
Z = dt_classifier.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

# Tracer la frontière de décision avec contourf
plt.contourf(xx, yy, Z, alpha=0.4, cmap=mycolormap)

# Tracer les points de données
plt.scatter(X[:, 0], X[:, 1], c=Y, edgecolors=&#39;k&#39;, cmap=mycolormap)
plt.title(&#39;Decision Boundary of the Decision Tree&#39;)
plt.grid()
plt.xlabel(&#39;x1&#39;)
plt.ylabel(&#39;x2&#39;)
plt.show()


&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./index_11_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;la-méthode-cart-avec-lélagage-pour-obtenir-les-valeurs-de-coût-complexité&#34;&gt;La méthode CART avec l&amp;rsquo;élagage pour obtenir les valeurs de coût-complexité&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Générer des données exemple (X, Y) - Tes données
# Assure-toi d&#39;avoir X et Y définis avant ceci
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=1)

# Initialiser le classificateur sans élagage pour obtenir le chemin d&#39;élagage
dt_classifier = DecisionTreeClassifier(random_state=1)
path = dt_classifier.cost_complexity_pruning_path(X_train, Y_train)
ccp_alphas = path.ccp_alphas  # Liste des valeurs de ccp_alpha
impurities = path.impurities  # Coût total de l&#39;arbre pour chaque alpha

# Stocker les erreurs pour chaque ccp_alpha
train_errors = []
test_errors = []

# Entraîner un arbre pour chaque valeur de ccp_alpha et calculer les erreurs
for ccp_alpha in ccp_alphas:
    dt_classifier_pruned = DecisionTreeClassifier(random_state=1, ccp_alpha=ccp_alpha)
    dt_classifier_pruned.fit(X_train, Y_train)
    
    # Prédiction sur l&#39;ensemble d&#39;entraînement et de test
    train_pred = dt_classifier_pruned.predict(X_train)
    test_pred = dt_classifier_pruned.predict(X_test)
    
    # Calculer l&#39;erreur (1 - précision)
    train_error = 1 - accuracy_score(Y_train, train_pred)
    test_error = 1 - accuracy_score(Y_test, test_pred)
    
    # Stocker les erreurs
    train_errors.append(train_error)
    test_errors.append(test_error)

# Tracer les erreurs d&#39;apprentissage et de test en fonction de ccp_alpha
plt.figure(figsize=(10, 6))
plt.plot(ccp_alphas, train_errors, marker=&#39;o&#39;, label=&amp;quot;Erreur d&#39;apprentissage&amp;quot;, drawstyle=&amp;quot;steps-post&amp;quot;)
plt.plot(ccp_alphas, test_errors, marker=&#39;o&#39;, label=&amp;quot;Erreur de test&amp;quot;, drawstyle=&amp;quot;steps-post&amp;quot;)
plt.xlabel(&amp;quot;Valeur de ccp_alpha&amp;quot;)
plt.ylabel(&amp;quot;Erreur&amp;quot;)
plt.title(&amp;quot;Erreur d&#39;apprentissage et de test en fonction de ccp_alpha&amp;quot;)
plt.legend()
plt.grid(True)
plt.show()

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./index_14_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;bagging&#34;&gt;Bagging&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.metrics import accuracy_score

# Initialiser le modèle d&#39;arbre de décision
treemod = DecisionTreeClassifier()

# Initialiser le modèle Bagging avec l&#39;arbre de décision comme estimateur de base
bagmod = BaggingClassifier(estimator=treemod, n_estimators=100, random_state=0)

# Entraîner les modèles (arbre et bagging) sur les données
treemodfit = treemod.fit(X, Y)  # Facultatif ici si tu utilises le modèle bagging
bagmodfit = bagmod.fit(X, Y)

# Prédire les classes sur l&#39;ensemble d&#39;entraînement avec Bagging
pY_train = bagmodfit.predict(X)  # Utiliser predict() pour obtenir les classes

# Calculer l&#39;erreur d&#39;entraînement
train_error = 1 - accuracy_score(Y, pY_train)
print(&amp;quot;L&#39;erreur en apprentissage du Bagging est &amp;quot;, train_error)


Z = bagmodfit.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

# Tracer la frontière de décision avec contourf
plt.contourf(xx, yy, Z, alpha=0.4, cmap=mycolormap)

# Tracer les points de données
plt.scatter(X[:, 0], X[:, 1], c=Y, edgecolors=&#39;k&#39;, cmap=mycolormap)
plt.title(&#39;Decision Boundary of the Bagged Trees&#39;)
plt.grid()
plt.xlabel(&#39;x1&#39;)
plt.ylabel(&#39;x2&#39;)
plt.show()

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;L&#39;erreur en apprentissage du Bagging est  0.0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./index_16_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;forêt-aléatoire&#34;&gt;Forêt aléatoire&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap

# Remplace le BaggingClassifier par le RandomForestClassifier
forestmod = RandomForestClassifier(n_estimators=100, random_state=0)

# Entraîner le modèle de forêt aléatoire sur les données
forestmodfit = forestmod.fit(X, Y)

# Prédire les classes sur l&#39;ensemble d&#39;entraînement avec RandomForest
pY_train = forestmodfit.predict(X)

# Calculer l&#39;erreur d&#39;entraînement
train_error = 1 - accuracy_score(Y, pY_train)
print(&amp;quot;L&#39;erreur en apprentissage de la forêt aléatoire est &amp;quot;, train_error)


# Prédire les classes sur la grille de points avec RandomForest
Z = forestmodfit.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

# Tracer la frontière de décision avec contourf
mycolormap = ListedColormap([&#39;#FF0000&#39;, &#39;#0000FF&#39;])
plt.contourf(xx, yy, Z, alpha=0.4, cmap=mycolormap)

# Tracer les points de données
plt.scatter(X[:, 0], X[:, 1], c=Y, edgecolors=&#39;k&#39;, cmap=mycolormap)
plt.title(&#39;Decision Boundary of the Random Forest&#39;)
plt.grid()
plt.xlabel(&#39;x1&#39;)
plt.ylabel(&#39;x2&#39;)
plt.show()

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;L&#39;erreur en apprentissage de la forêt aléatoire est  0.0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./index_18_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
