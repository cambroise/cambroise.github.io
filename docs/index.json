[{"authors":"Christophe Ambroise","categories":null,"content":"I am professor of statistics at University of Evry val d\u0026rsquo;Essone. My research interests include machine learning, statistics in high-dimension and applications in medicine and genomics. I lead the Statistic and Genome group of the LaMME (Laboratory of Mathematic and Modeling of Evry val d\u0026rsquo;Essone).\nmy Google scholar.\n","date":1549324800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1607817600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://cambroise.github.io/author/christophe-ambroise/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/christophe-ambroise/","section":"authors","summary":"I am professor of statistics at University of Evry val d\u0026rsquo;Essone. My research interests include machine learning, statistics in high-dimension and applications in medicine and genomics. I lead the Statistic and Genome group of the LaMME (Laboratory of Mathematic and Modeling of Evry val d\u0026rsquo;Essone).","tags":null,"title":"Christophe Ambroise","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"4729b5c1b6023198eacf1e2d35f33b48","permalink":"https://cambroise.github.io/author/catherine-matias/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/catherine-matias/","section":"authors","summary":"","tags":null,"title":"Catherine Matias","type":"authors"},{"authors":null,"categories":null,"content":"Plan Introduction\nSystèmes d’équations, pivot de gauss\nNotions d’espace vectoriel\nEquation de vecteur\nCalcul matriciel\nApplication linéaire\nNoyau Image Géométrie - Produit scalaire, - distance et norme Orthogonalité, - théorème de Pythagore - Projection orthogonale\nvaleur propre - Vecteur propre\nAnalyse en composantes principales\nTransparents Cours 1 et 2 Cours 3 et 4 Géométrie ACP ","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"e35b109ddd3b7e8c689b452188e3dee0","permalink":"https://cambroise.github.io/courses/algebrel3bio/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/algebrel3bio/","section":"courses","summary":"Algèbre linéaire pour comprendre les bases de l'analyse de données","tags":null,"title":"Algèbre linéaire (L3 Bio)","type":"docs"},{"authors":null,"categories":null,"content":"Outline Factor Analysis Variational Auto-encoder Neurons Variational EM (VEM) Visualisation SVD t-SNE UMAP Introduction to NLP Reference document The lecture closely follows and largely borrows material from \u0026ldquo;Machine Learning: A Probabilistic Perspective\u0026rdquo; (MLAPP) from Kevin P. Murphy, chapters:\nLectures Notes Factor Analysis Neurons Variational Auto Encoders Singular Value Decomposition Document and Links Reference books about machine learning Machine Learning: A Probabilistic Perspective from Kevin P. Murphy Pattern Recognition and Machine Learning from Chris M Bishop R base Official manuals about R base can be retrieved from\nhttps://cran.r-project.org/manuals.html\nContribution by the community can be retrieved from\nhttps://cran.r-project.org/other-docs.html\nThe short introduction from Emmanuel Paradis allows a quick start\n\u0026lsquo;\u0026lsquo;R for Beginners\u0026rsquo;\u0026rsquo; by Emmanuel Paradis. Longer book allow a deepening. See for example\n\u0026lsquo;\u0026lsquo;Using R for Data Analysis and Graphics - Introduction, Examples and Commentary\u0026rsquo;\u0026rsquo; by John Maindonald. R from RStudio developers R for Data Science The book of Wickam about more recent R development for data science And if you want more see https://www.rstudio.com/resources/books/\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"aab700e23e1366aff8a99ed82252f0c4","permalink":"https://cambroise.github.io/courses/unsupervised/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/unsupervised/","section":"courses","summary":"Visualisation and dimension reduction for M2 Data Science Evry","tags":null,"title":"Visualisation and dimension reduction","type":"docs"},{"authors":null,"categories":null,"content":"Reference document The lecture closely follows and largely borrows material from \u0026ldquo;Machine Learning: A Probabilistic Perspective\u0026rdquo; (MLAPP) from Kevin P. Murphy, chapters:\nChapter 10: Directed graphical models (Bayes nets) Chapter 19: Undirected graphical models (Markov random fields) Chapter 20: Exact inference for graphical models Chapter 26: Graphical model structure learning Directed models Directed Graphical Models Undirected models Undirected Graphical Models ","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"abb887d13bd93af64d53e71eafbc503b","permalink":"https://cambroise.github.io/courses/graphicalmodels/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/graphicalmodels/","section":"courses","summary":"Introduction to graphical models","tags":null,"title":"Graphical models","type":"docs"},{"authors":null,"categories":null,"content":"Projects to come Lectures Notes Clustering Exemples Gaussian Classifier Tree Exercices Document and Links Reference books about machine learning Probabilistic Machine Learning An Introduction from Kevin P. Murphy\nChapter 21: Clustering Chapter 20: Dimension Reduction Chapter 5: Decision Theory Chapter 9: Linear Discriminant Analysis Chapter 18: Trees, Forests, Bagging, and Boosting Pattern Recognition and Machine Learning from Chris M Bishop\nR base Official manuals about R base can be retrieved from\nhttps://cran.r-project.org/manuals.html\nContribution by the community can be retrieved from\nhttps://cran.r-project.org/other-docs.html\nThe short introduction from Emmanuel Paradis allows a quick start\n\u0026lsquo;\u0026lsquo;R for Beginners\u0026rsquo;\u0026rsquo; by Emmanuel Paradis. Longer book allow a deepening. See for example\n\u0026lsquo;\u0026lsquo;Using R for Data Analysis and Graphics - Introduction, Examples and Commentary\u0026rsquo;\u0026rsquo; by John Maindonald. R from RStudio developers R for Data Science The book of Wickam about more recent R development for data science And if you want more see https://www.rstudio.com/resources/books/\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"c7a407c467d3831e04596889dae53caa","permalink":"https://cambroise.github.io/courses/introduction-machine-learning/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/introduction-machine-learning/","section":"courses","summary":"Introduction to Machine Learning for Master 1 Students in Mathematic and interactions","tags":null,"title":"Introduction to machine learning","type":"docs"},{"authors":null,"categories":null,"content":" Transparents de cours Exercices Projets R base Official manuals about R base can be retrieved from\nhttps://cran.r-project.org/manuals.html\nContribution by the community can be retrieved from\nhttps://cran.r-project.org/other-docs.html\nThe short introduction from Emmanuel Paradis allows a quick start\n\u0026lsquo;\u0026lsquo;R for Beginners\u0026rsquo;\u0026rsquo; by Emmanuel Paradis. Longer book allow a deepening. See for example\n\u0026lsquo;\u0026lsquo;Using R for Data Analysis and Graphics - Introduction, Examples and Commentary\u0026rsquo;\u0026rsquo; by John Maindonald. R from RStudio developers R for Data Science The book of Wickam about more recent R development for data science And if you want more see https://www.rstudio.com/resources/books/\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"348600c9e9dfac99c6b25e1f958e452c","permalink":"https://cambroise.github.io/courses/statdes/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/statdes/","section":"courses","summary":"Statistiques Descriptives avec R pour étudiants L2 math","tags":null,"title":"","type":"docs"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"24939f284a5b9d71d0f4030db325cd13","permalink":"https://cambroise.github.io/co-workers/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/co-workers/","section":"","summary":"past and present PhD Students","tags":null,"title":"PhD Student","type":"widget_page"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature. Slides can be added in a few ways:\nCreate slides using Wowchemy\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"https://cambroise.github.io/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":["Sow M","Rogier O","Lesur I","Daviaud C","Mardoc E","Sanou E","Duvaux L","Civan P","Delaunay A","Lesage-Descauses M","others"],"categories":null,"content":"","date":1704067200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1704067200,"objectID":"df200b35011496b581035b1e00d64879","permalink":"https://cambroise.github.io/publication/sow2023epigenetic/","publishdate":"2024-01-01T00:00:00Z","relpermalink":"/publication/sow2023epigenetic/","section":"publication","summary":"","tags":null,"title":" Epigenetic Variation in Tree Evolution: a case study in black poplar (Populus nigra) ","type":"publication"},{"authors":["Santiago K","Szafranski M","Ambroise C"],"categories":null,"content":"","date":1704067200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1704067200,"objectID":"41ed4ff20bbaef20e0c886c6a55a8d4f","permalink":"https://cambroise.github.io/publication/desantiago2024mixture/","publishdate":"2024-01-01T00:00:00Z","relpermalink":"/publication/desantiago2024mixture/","section":"publication","summary":"","tags":null,"title":" Mixture of multilayer stochastic block models for multiview clustering ","type":"publication"},{"authors":["Laso-Jadart R","O’Malley M","Sykulski A","Ambroise C","Madoui M"],"categories":null,"content":"","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672531200,"objectID":"3ac3c586a486187679105592af51f24a","permalink":"https://cambroise.github.io/publication/laso2023holistic/","publishdate":"2023-01-01T00:00:00Z","relpermalink":"/publication/laso2023holistic/","section":"publication","summary":"","tags":null,"title":" Holistic view of the seascape dynamics and environment impact on macro-scale genetic connectivity of marine plankton populations ","type":"publication"},{"authors":["Sanou D","Ambroise C","Robin G"],"categories":null,"content":"","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672531200,"objectID":"150a6d5fc83c66019e26835cbce4337b","permalink":"https://cambroise.github.io/publication/sanou2023inference/","publishdate":"2023-01-01T00:00:00Z","relpermalink":"/publication/sanou2023inference/","section":"publication","summary":"","tags":null,"title":" Inference of Multiscale Gaussian Graphical Model ","type":"publication"},{"authors":["Courbariaux M","De Santiago K","Dalmasso C","Danjou F","Bekadar S","Corvol J","Martinez M","Szafranski M","Ambroise C"],"categories":null,"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640995200,"objectID":"3f6810a2eecca74339ab4decaee0afa0","permalink":"https://cambroise.github.io/publication/courbariaux2022sparse/","publishdate":"2022-01-01T00:00:00Z","relpermalink":"/publication/courbariaux2022sparse/","section":"publication","summary":"","tags":null,"title":" A Sparse Mixture-of-Experts Model With Screening of Genetic Associations to Guide Disease Subtyping ","type":"publication"},{"authors":["GUINOT F","SZAFRANSKI M","AMBROISE C"],"categories":null,"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640995200,"objectID":"3af1dee988e508848b93c90284cd3acc","permalink":"https://cambroise.github.io/publication/guinot2022compression/","publishdate":"2022-01-01T00:00:00Z","relpermalink":"/publication/guinot2022compression/","section":"publication","summary":"","tags":null,"title":" Compression structur{e}e de l'information g{e}n{e}tique et {e}tude d'association pang{e}nomique par mod{e}les additifs ","type":"publication"},{"authors":["Bichat A","Ambroise C","Mariadassou M"],"categories":null,"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640995200,"objectID":"af62caeb237427de527b92b07772a1fa","permalink":"https://cambroise.github.io/publication/bichat2022hierarchical/","publishdate":"2022-01-01T00:00:00Z","relpermalink":"/publication/bichat2022hierarchical/","section":"publication","summary":"","tags":null,"title":" Hierarchical correction of p-values via an ultrametric tree running Ornstein-Uhlenbeck process ","type":"publication"},{"authors":null,"categories":null,"content":"Summary Engineers and researchers from the University of Évry, CEA, CNRS, APHP, CHU Henri Mondor and Google, in collaboration with the Médecins Sans Frontière (MSF) foundation, have developped a mobile application capable of antibiotic resistance diagnostic, which can be used free of charge anywhere in the world. These results are the subject of a publication in the journal Nature Communications. For more details in french, see this article.\nMédecins sans frontière: presentation of the project and code https://antibiogo.org/\nProject description Antimicrobial resistance is a major global health threat and its development is promoted by antibiotic misuse. While disk diffusion antibiotic susceptibility testing (AST, also called antibiogram) is broadly used to test for antibiotic resistance in bacterial infections, it faces strong criticism because of inter-operator variability and the complexity of interpretative reading. Automatic reading systems address these issues, but are not always adapted or available to resource-limited settings. We present an artificial intelligence (AI)-based, offline smartphone application for antibiogram analysis. The application captures images with the phone’s camera, and the user is guided throughout the analysis on the same device by a user-friendly graphical interface. An embedded expert system validates the coherence of the antibiogram data and provides interpreted results. The fully automatic measurement procedure of our application’s reading system achieves an overall agreement of 90% on susceptibility categorization against a hospital-standard automatic system and 98% against manual measurement (gold standard), with reduced inter-operator variability. The application’s performance showed that the automatic reading of antibiotic resistance testing is entirely feasible on a smartphone. Moreover our application is suited for resource-limited settings, and therefore has the potential to significantly increase patients’ access to AST worldwide.\n","date":1614384000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614384000,"objectID":"d3ac572cc67d402ec2d02cba652bfc4d","permalink":"https://cambroise.github.io/project/antibiogo/","publishdate":"2021-02-27T00:00:00Z","relpermalink":"/project/antibiogo/","section":"project","summary":"AI-based mobile application to fight antibiotic resistance","tags":["MSF"],"title":"Antibiogo","type":"project"},{"authors":["Pascucci M","Royer G","Adamek J","Aristizabal D","Blanche L","Bezzarga A","Boniface-Chang G","Brunner A","Curel C","Dulac-Arnold G","Fakhri R","Malou, N","Nordon C","Runge  V","Samson F","Sebastian E","Soukieh D","Vert JP","Ambroise C","Madoui A"],"categories":null,"content":"","date":1613692800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613692800,"objectID":"30b1e87c19ed66b0a22210261188fb41","permalink":"https://cambroise.github.io/publication/pascucci2021/","publishdate":"2021-02-19T00:00:00Z","relpermalink":"/publication/pascucci2021/","section":"publication","summary":"","tags":null,"title":" AI-based mobile application for antibiotic resistance testing ","type":"publication"},{"authors":["Momal R","Robin S","Ambroise C"],"categories":null,"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"96c10afb9683978e05eb9c6b49b4e289","permalink":"https://cambroise.github.io/publication/momal2021accounting/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/publication/momal2021accounting/","section":"publication","summary":"","tags":null,"title":" Accounting for missing actors in interaction network inference from abundance data ","type":"publication"},{"authors":["Laso-Jadart R","O’Malley M","Sykulski A","Ambroise C","Madoui M"],"categories":null,"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"8ae612dcd88e4a2fe62e5055bd244ea0","permalink":"https://cambroise.github.io/publication/laso2021marine/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/publication/laso2021marine/","section":"publication","summary":"","tags":null,"title":" How marine currents and environment shape plankton genomic differentiation: a mosaic view from Tara oceans metagenomic data ","type":"publication"},{"authors":["Momal R","Robin S","Ambroise C"],"categories":null,"content":"","date":1582070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582070400,"objectID":"9307a5a1b855fc9b2c9d56ebd609e46d","permalink":"https://cambroise.github.io/publication/momal2020tree/","publishdate":"2020-02-19T00:00:00Z","relpermalink":"/publication/momal2020tree/","section":"publication","summary":"","tags":null,"title":" Tree-based Inference of Species Interaction Networks from Abundance Data ","type":"publication"},{"authors":["Frouin A","Dandine-Roulland C","Pierre-Jean M","Deleuze J","Ambroise C","Le Floch E"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"b0c6770cfa0e048d3bf66551d4a33911","permalink":"https://cambroise.github.io/publication/frouin2020exploring/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/publication/frouin2020exploring/","section":"publication","summary":"","tags":null,"title":" Exploring the link between additive heritability and prediction accuracy from a ridge regression perspective ","type":"publication"},{"authors":["Guinot F","Szafranski M","Chiquet J","Zancarini A","Le Signor C","Mougel C","Ambroise C"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"0139c8b5b0697b0f918141b88c584688","permalink":"https://cambroise.github.io/publication/guinot2020fast/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/publication/guinot2020fast/","section":"publication","summary":"","tags":null,"title":" Fast computation of genome-metagenome interaction effects ","type":"publication"},{"authors":["Frouin A","Dandine-Roulland C","Pierre-Jean M","Deleuze J","Ambroise C","Floch E"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"f08473808735d37ae9a222c043ba141f","permalink":"https://cambroise.github.io/publication/frouin2020high/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/publication/frouin2020high/","section":"publication","summary":"","tags":null,"title":" High heritability does not imply accurate prediction under the small additive effects hypothesis ","type":"publication"},{"authors":["Bichat A","Plassais J","Ambroise C","Mariadassou M"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"9e583ba52b75e38ce47cb670130c88d6","permalink":"https://cambroise.github.io/publication/bichat2020incorporating/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/publication/bichat2020incorporating/","section":"publication","summary":"","tags":null,"title":" Incorporating phylogenetic information in microbiome differential abundance studies has no effect on detection power and FDR control ","type":"publication"},{"authors":["Laso-Jadart R","Sugier K","Petit E","Labadie K","Peterlongo P","Ambroise C","Wincker P","Jamet J","Madoui M"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"8eb509a5ef38631e2608b33798a82f7a","permalink":"https://cambroise.github.io/publication/laso2020investigating/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/publication/laso2020investigating/","section":"publication","summary":"","tags":null,"title":" Investigating population-scale allelic differential expression in wild populations of Oithona similis (Cyclopoida, Claus, 1866) ","type":"publication"},{"authors":["Laso-Jadart R","Ambroise C","Peterlongo P","Madoui M"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"7646ba5149c84becd29e36919736df96","permalink":"https://cambroise.github.io/publication/laso2020metavar/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/publication/laso2020metavar/","section":"publication","summary":"","tags":null,"title":" metaVaR: introducing metavariant species models for reference-free metagenomic-based population genomics ","type":"publication"},{"authors":["Gautreau G","Bazin A","Gachet M","Planel R","Burlot L","Dubois M","Perrin A","Medigue C","Calteau A","Cruveiller S","others"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"983407079e87848c75fa4fbf9bb43180","permalink":"https://cambroise.github.io/publication/gautreau2020ppanggolin/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/publication/gautreau2020ppanggolin/","section":"publication","summary":"","tags":null,"title":" PPanGGOLiN: depicting microbial diversity via a partitioned pangenome graph ","type":"publication"},{"authors":null,"categories":null,"content":"Academic is designed to give technical content creators a seamless experience. You can focus on the content and Academic handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.\nOn this page, you\u0026rsquo;ll find some examples of the types of technical content that can be rendered with Academic.\nExamples Code Academic supports a Markdown extension for highlighting code syntax. You can enable this feature by toggling the highlight option in your config/_default/params.toml file.\n```python import pandas as pd data = pd.read_csv(\u0026quot;data.csv\u0026quot;) data.head() ``` renders as\nimport pandas as pd data = pd.read_csv(\u0026quot;data.csv\u0026quot;) data.head() Charts Academic supports the popular Plotly chart format.\nSave your Plotly JSON in your page folder, for example chart.json, and then add the {{\u0026lt; chart data=\u0026quot;chart\u0026quot; \u0026gt;}} shortcode where you would like the chart to appear.\nDemo:\nYou might also find the Plotly JSON Editor useful.\nMath Academic supports a Markdown extension for $\\LaTeX$ math. You can enable this feature by toggling the math option in your config/_default/params.toml file.\nTo render inline or block math, wrap your LaTeX math with $...$ or $$...$$, respectively.\nExample math block:\n$$\\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |} {\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2}$$ renders as\n$$\\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |}{\\left |\\nabla F(\\mathbf{x}{n}) - \\nabla F(\\mathbf{x}{n-1}) \\right |^2}$$\nExample inline math $\\nabla F(\\mathbf{x}_{n})$ renders as $\\nabla F(\\mathbf{x}_{n})$.\nExample multi-line math using the \\\\\\\\ math linebreak:\n$$f(k;p_0^*) = \\begin{cases} p_0^* \u0026amp; \\text{if }k=1, \\\\\\\\ 1-p_0^* \u0026amp; \\text {if }k=0.\\end{cases}$$ renders as\n$$f(k;p_0^) = \\begin{cases} p_0^ \u0026amp; \\text{if }k=1, \\\\ 1-p_0^* \u0026amp; \\text {if }k=0.\\end{cases}$$\nDiagrams Academic supports a Markdown extension for diagrams. You can enable this feature by toggling the diagram option in your config/_default/params.toml file or by adding diagram: true to your page front matter.\nAn example flowchart:\n```mermaid graph TD A[Hard] --\u0026gt;|Text| B(Round) B --\u0026gt; C{Decision} C --\u0026gt;|One| D[Result 1] C --\u0026gt;|Two| E[Result 2] ``` renders as\ngraph TD A[Hard] --\u0026gt;|Text| B(Round) B --\u0026gt; C{Decision} C --\u0026gt;|One| D[Result 1] C --\u0026gt;|Two| E[Result 2] An example sequence diagram:\n```mermaid sequenceDiagram Alice-\u0026gt;\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts! John--\u0026gt;\u0026gt;Alice: Great! John-\u0026gt;\u0026gt;Bob: How about you? Bob--\u0026gt;\u0026gt;John: Jolly good! ``` renders as\nsequenceDiagram Alice-\u0026gt;\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts! John--\u0026gt;\u0026gt;Alice: Great! John-\u0026gt;\u0026gt;Bob: How about you? Bob--\u0026gt;\u0026gt;John: Jolly good! An example Gantt diagram:\n```mermaid gantt section Section Completed :done, des1, 2014-01-06,2014-01-08 Active :active, des2, 2014-01-07, 3d Parallel 1 : des3, after des1, 1d Parallel 2 : des4, after des1, 1d Parallel 3 : des5, after des3, 1d Parallel 4 : des6, after des4, 1d ``` renders as\ngantt section Section Completed :done, des1, 2014-01-06,2014-01-08 Active :active, des2, 2014-01-07, 3d Parallel 1 : des3, after des1, 1d Parallel 2 : des4, after des1, 1d Parallel 3 : des5, after des3, 1d Parallel 4 : des6, after des4, 1d An example class diagram:\n```mermaid classDiagram Class01 \u0026lt;|-- AveryLongClass : Cool \u0026lt;\u0026lt;interface\u0026gt;\u0026gt; Class01 Class09 --\u0026gt; C2 : Where am i? Class09 --* C3 Class09 --|\u0026gt; Class07 Class07 : equals() Class07 : Object[] elementData Class01 : size() Class01 : int chimp Class01 : int gorilla class Class10 { \u0026lt;\u0026lt;service\u0026gt;\u0026gt; int id size() } ``` renders as\nclassDiagram Class01 \u0026lt;|-- AveryLongClass : Cool \u0026lt;\u0026lt;interface\u0026gt;\u0026gt; Class01 Class09 --\u0026gt; C2 : Where am i? Class09 --* C3 Class09 --|\u0026gt; Class07 Class07 : equals() Class07 : Object[] elementData Class01 : size() Class01 : int chimp Class01 : int gorilla class Class10 { \u0026lt;\u0026lt;service\u0026gt;\u0026gt; int id size() } An example state diagram:\n```mermaid stateDiagram [*] --\u0026gt; Still Still --\u0026gt; [*] Still --\u0026gt; Moving Moving --\u0026gt; Still Moving --\u0026gt; Crash Crash --\u0026gt; [*] ``` renders as\nstateDiagram [*] --\u0026gt; Still Still --\u0026gt; [*] Still --\u0026gt; Moving Moving --\u0026gt; Still Moving --\u0026gt; Crash Crash --\u0026gt; [*] Todo lists You can even write your todo lists in Academic too:\n- [x] Write math example - [x] Write diagram example - [ ] Do something else renders as\nWrite math example Write diagram example Do something else Tables Represent your data in tables:\n| First Header | Second Header | | ------------- | ------------- | | Content Cell | Content Cell | | Content Cell | Content Cell | renders as\nFirst Header Second Header Content Cell Content Cell Content Cell Content Cell Callouts Academic supports a shortcode for callouts, also referred to as asides, hints, or alerts. By wrapping a paragraph in {{% alert note %}} ... {{% /alert %}}, it will render as an aside.\n{{% alert note %}} A Markdown aside is useful for displaying notices, hints, or definitions to your readers. {{% /alert %}} renders as\nA Markdown aside is useful for displaying notices, hints, or definitions to your readers. Spoilers Add a spoiler to a page to reveal text, such as an answer to a question, after a button is clicked.\n{{\u0026lt; spoiler text=\u0026quot;Click to view the spoiler\u0026quot; \u0026gt;}} You found me! {{\u0026lt; /spoiler \u0026gt;}} renders as\nClick to view the spoiler You found me! Icons Academic enables you to use a wide range of icons from Font Awesome and Academicons in addition to emojis.\nHere are some examples using the icon shortcode to render icons:\n{{\u0026lt; icon name=\u0026quot;terminal\u0026quot; pack=\u0026quot;fas\u0026quot; \u0026gt;}} Terminal {{\u0026lt; icon name=\u0026quot;python\u0026quot; pack=\u0026quot;fab\u0026quot; \u0026gt;}} Python {{\u0026lt; icon name=\u0026quot;r-project\u0026quot; pack=\u0026quot;fab\u0026quot; \u0026gt;}} R renders as\nTerminal\nPython\nR\nDid you find this page helpful? Consider sharing it 🙌 ","date":1562889600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562889600,"objectID":"07e02bccc368a192a0c76c44918396c3","permalink":"https://cambroise.github.io/post/writing-technical-content/","publishdate":"2019-07-12T00:00:00Z","relpermalink":"/post/writing-technical-content/","section":"post","summary":"Academic is designed to give technical content creators a seamless experience. You can focus on the content and Academic handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.","tags":null,"title":"Writing technical content in Academic","type":"post"},{"authors":["Christophe Ambroise"],"categories":[],"content":"from IPython.core.display import Image Image('https://www.python.org/static/community_logos/python-logo-master-v3-TM-flattened.png') print(\u0026quot;Welcome to Academic!\u0026quot;) Welcome to Academic! Install Python and JupyterLab Install Anaconda which includes Python 3 and JupyterLab.\nAlternatively, install JupyterLab with pip3 install jupyterlab.\nCreate or upload a Jupyter notebook Run the following commands in your Terminal, substituting \u0026lt;MY-WEBSITE-FOLDER\u0026gt; and \u0026lt;SHORT-POST-TITLE\u0026gt; with the file path to your Academic website folder and a short title for your blog post (use hyphens instead of spaces), respectively:\nmkdir -p \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ cd \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ jupyter lab index.ipynb The jupyter command above will launch the JupyterLab editor, allowing us to add Academic metadata and write the content.\nEdit your post metadata The first cell of your Jupter notebook will contain your post metadata (front matter).\nIn Jupter, choose Markdown as the type of the first cell and wrap your Academic metadata in three dashes, indicating that it is YAML front matter:\n--- title: My post's title date: 2019-09-01 # Put any other Academic metadata here... --- Edit the metadata of your post, using the documentation as a guide to the available options.\nTo set a featured image, place an image named featured into your post\u0026rsquo;s folder.\nFor other tips, such as using math, see the guide on writing content with Academic.\nConvert notebook to Markdown jupyter nbconvert index.ipynb --to markdown --NbConvertApp.output_files_dir=. Example This post was created with Jupyter. The orginal files can be found at https://github.com/gcushen/hugo-academic/tree/master/exampleSite/content/post/jupyter\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567641600,"objectID":"6e929dc84ed3ef80467b02e64cd2ed64","permalink":"https://cambroise.github.io/post/jupyter/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/post/jupyter/","section":"post","summary":"Learn how to blog in Academic using Jupyter notebooks","tags":[],"title":"Display Jupyter Notebooks with Academic","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne **Two** Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}} Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://cambroise.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Forst E","Enjalbert J","Allard V","Ambroise C","Krissaane I","Mary-Huard T","Robin S","Goldringer I"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"f09124dbeceb458fc28c04afec759d77","permalink":"https://cambroise.github.io/publication/forst2019generalized/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/publication/forst2019generalized/","section":"publication","summary":"","tags":null,"title":" A generalized statistical framework to assess mixing ability from incomplete mixing designs using binary or higher order variety mixtures and application to wheat ","type":"publication"},{"authors":["Ambroise C","Dehman A","Neuvial P","Rigaill G","Vialaneix N"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"e4c36315b5bbcc3bc956a9f97cf6df0c","permalink":"https://cambroise.github.io/publication/ambroise2019adjacency/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/publication/ambroise2019adjacency/","section":"publication","summary":"","tags":null,"title":" Adjacency-constrained hierarchical clustering of a band similarity matrix with application to genomics ","type":"publication"},{"authors":["Robin S","Ambroise C"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"c8793b84a73ce3d1f2b7f24a57aba257","permalink":"https://cambroise.github.io/publication/robin2019applications/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/publication/robin2019applications/","section":"publication","summary":"","tags":null,"title":" Applications in Genomics ","type":"publication"},{"authors":["Robin G","Ambroise C","Robin S"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"e2829e455745d281f80cbdd23e5f90bd","permalink":"https://cambroise.github.io/publication/robin2019incomplete/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/publication/robin2019incomplete/","section":"publication","summary":"","tags":null,"title":" Incomplete graphical model inference via latent tree aggregation ","type":"publication"},{"authors":["Palomares M","Dalmasso C","Bonnet E","Derbois C","Brohard-Julien S","Ambroise C","Battail C","Deleuze J","Olaso R"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"ce054e09e1c1b263fc36314f1e3447ff","permalink":"https://cambroise.github.io/publication/palomares2019systematic/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/publication/palomares2019systematic/","section":"publication","summary":"","tags":null,"title":" Systematic analysis of TruSeq, SMARTer and SMARTer Ultra-Low RNA-seq kits for standard, low and ultra-low quantity samples ","type":"publication"},{"authors":["Sow M","Le Gac A","Placette C","Delaunay A","Le Jan I","Fichot R","Maury S","Mirouze M","Lanciano S","Tost J","others"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"3e17a0e66d1647ef1913feb9ba14c6a6","permalink":"https://cambroise.github.io/publication/sow2018clarifying/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/publication/sow2018clarifying/","section":"publication","summary":"","tags":null,"title":" Clarifying the role of DNA methylation in tree phenotypic plasticity ","type":"publication"},{"authors":["Sow M","Allona I","Ambroise C","Conde D","Fichot R","Gribkova S","Jorge V","Le-Provost G","Paques L","Plomion C","others"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"d3dd7548ab4e41c6dc48c6fa8e86613a","permalink":"https://cambroise.github.io/publication/sow2018epigenetics/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/publication/sow2018epigenetics/","section":"publication","summary":"","tags":null,"title":" Epigenetics in forest trees: state of the art and potential implications for breeding and management in a context of climate change ","type":"publication"},{"authors":["Forst E","Enjalbert J","Allard V","Ambroise C","Krissaane I","Marry-Huard T","Robin S","Goldringer I"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"e57496ccff1848be6741841787fefe6e","permalink":"https://cambroise.github.io/publication/forst2018estimation/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/publication/forst2018estimation/","section":"publication","summary":"","tags":null,"title":" Estimation of mixing ability for variety mixtures: Statistical models and experimental results ","type":"publication"},{"authors":["Guinot F","Szafranski M","Ambroise C","Samson F"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"aea16579d5cf6c589a60b336b4a2cb58","permalink":"https://cambroise.github.io/publication/guinot2018learning/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/publication/guinot2018learning/","section":"publication","summary":"","tags":null,"title":" Learning the optimal scale for GWAS through hierarchical SNP aggregation ","type":"publication"},{"authors":["Frouin A","Le Flocch E","Ambroise C"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"8cc0bf381139fae738781939c738927c","permalink":"https://cambroise.github.io/publication/frouin2018quantify/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/publication/frouin2018quantify/","section":"publication","summary":"","tags":null,"title":" Quantify Genomic Heritability Through a Prediction Measure ","type":"publication"},{"authors":["Becu J","Grandvalet Y","Ambroise C","Dalmasso C"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"a6ee44d670efd308c6e8bb31eec6a93b","permalink":"https://cambroise.github.io/publication/becu2017beyond/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/becu2017beyond/","section":"publication","summary":"","tags":null,"title":" Beyond support in two-stage variable selection ","type":"publication"},{"authors":["Stanislas V","Dalmasso C","Ambroise C"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"bd55345b00d1c0a65fb83fbddd964a3e","permalink":"https://cambroise.github.io/publication/stanislas2017eigen/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/stanislas2017eigen/","section":"publication","summary":"","tags":null,"title":" Eigen-epistasis for detecting gene-gene interactions ","type":"publication"},{"authors":["Frouin A","Le Floch E","Ambroise C","Deleuze J"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"8afe8efefbf737e1603bbc92cc403e8e","permalink":"https://cambroise.github.io/publication/frouin2017estimating/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/frouin2017estimating/","section":"publication","summary":"","tags":null,"title":" Estimating Narrow-Sense Heritability with Ridge Regression ","type":"publication"},{"authors":["Frouin A","Le Floch E","Ambroise C"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"3821ec622e403b955b0168563d16ea79","permalink":"https://cambroise.github.io/publication/frouin2017quantify/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/frouin2017quantify/","section":"publication","summary":"","tags":null,"title":" Quantify Genomic Heritability Through a Prediction Measure ","type":"publication"},{"authors":["Christophe Ambroise","吳恩達"],"categories":["Demo","教程"],"content":"Overview The Wowchemy website builder for Hugo, along with its starter templates, is designed for professional creators, educators, and teams/organizations - although it can be used to create any kind of site The template can be modified and customised to suit your needs. It\u0026rsquo;s a good platform for anyone looking to take control of their data and online identity whilst having the convenience to start off with a no-code solution (write in Markdown and customize with YAML parameters) and having flexibility to later add even deeper personalization with HTML and CSS You can work with all your favourite tools and apps with hundreds of plugins and integrations to speed up your workflows, interact with your readers, and much more The template is mobile first with a responsive design to ensure that your site looks stunning on every device. Get Started 👉 Create a new site 📚 Personalize your site 💬 Chat with the Wowchemy community or Hugo community 🐦 Twitter: @wowchemy @GeorgeCushen #MadeWithWowchemy 💡 Request a feature or report a bug for Wowchemy ⬆️ Updating Wowchemy? View the Update Guide and Release Notes Crowd-funded open-source software To help us develop this template and software sustainably under the MIT license, we ask all individuals and businesses that use it to help support its ongoing maintenance and development via sponsorship.\n❤️ Click here to become a sponsor and help support Wowchemy\u0026rsquo;s future ❤️ As a token of appreciation for sponsoring, you can unlock these awesome rewards and extra features 🦄✨\nEcosystem Wowchemy Admin: An admin tool to automatically import publications from BibTeX Inspiration Check out the latest demo of what you\u0026rsquo;ll get in less than 10 minutes, or view the showcase of personal, project, and business sites.\nFeatures Page builder - Create anything with widgets and elements Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown, Jupyter, or RStudio Plugin System - Fully customizable color and font themes Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics, Disqus commenting, Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 34+ language packs including English, 中文, and Português Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files. Themes Wowchemy and its templates come with automatic day (light) and night (dark) mode built-in. Alternatively, visitors can choose their preferred mode - click the moon icon in the top right of the Demo to see it in action! Day/night mode can also be disabled by the site admin in params.toml.\nChoose a stunning theme and font for your site. Themes are fully customizable.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1461110400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607817600,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"https://cambroise.github.io/post/getting-started/","publishdate":"2016-04-20T00:00:00Z","relpermalink":"/post/getting-started/","section":"post","summary":"Welcome 👋 We know that first impressions are important, so we've populated your new site with some initial content to help you get familiar with everything in no time.","tags":["Academic","开源"],"title":"Welcome to Wowchemy, the website builder for Hugo","type":"post"},{"authors":null,"categories":["R"],"content":" R Markdown This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nYou can embed an R code chunk like this:\nsummary(cars) ## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00 fit \u0026lt;- lm(dist ~ speed, data = cars) fit ## ## Call: ## lm(formula = dist ~ speed, data = cars) ## ## Coefficients: ## (Intercept) speed ## -17.579 3.932 Including Plots You can also embed plots. See Figure 1 for example:\npar(mar = c(0, 1, 0, 1)) pie( c(280, 60, 20), c(\u0026#39;Sky\u0026#39;, \u0026#39;Sunny side of pyramid\u0026#39;, \u0026#39;Shady side of pyramid\u0026#39;), col = c(\u0026#39;#0292D8\u0026#39;, \u0026#39;#F7EA39\u0026#39;, \u0026#39;#C4B632\u0026#39;), init.angle = -50, border = NA ) Figure 1: A fancy pie chart. ","date":1437703994,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1437703994,"objectID":"a1d1cdab917947aeb3793d754f710476","permalink":"https://cambroise.github.io/post/2015-07-23-r-rmarkdown/","publishdate":"2015-07-23T21:13:14-05:00","relpermalink":"/post/2015-07-23-r-rmarkdown/","section":"post","summary":"R Markdown This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.","tags":["R Markdown","plot","regression"],"title":"Hello R Markdown","type":"post"},{"authors":["Chiquet J","Szafranski M","Ambroise C"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"12804e6e3afe5595873128d6539b1976","permalink":"https://cambroise.github.io/publication/chiquet2015greedy/","publishdate":"2015-01-01T00:00:00Z","relpermalink":"/publication/chiquet2015greedy/","section":"publication","summary":"","tags":null,"title":" A greedy great approach to learn with complementary structured datasets ","type":"publication"},{"authors":["Petit-Teixeira E","Derbois C","Dalmasso C","Olaso R","Ambroise C","Kilani M","Deleuze J","Gabay C","Cornelis F","Finckh A"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"0baf4a954743099b114e18d9709f7389","permalink":"https://cambroise.github.io/publication/petit2015ab0004/","publishdate":"2015-01-01T00:00:00Z","relpermalink":"/publication/petit2015ab0004/","section":"publication","summary":"","tags":null,"title":" AB0004 Characterization of Putative Pre-RA Signatures by Transcriptome Analysis ","type":"publication"},{"authors":["Dehman A","Ambroise C","Neuvial P"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"dc48686b9d75f741f0d4ab765f982fed","permalink":"https://cambroise.github.io/publication/dehman2015performance/","publishdate":"2015-01-01T00:00:00Z","relpermalink":"/publication/dehman2015performance/","section":"publication","summary":"","tags":null,"title":" Performance of a blockwise approach in variable selection using linkage disequilibrium information ","type":"publication"},{"authors":["Becu J","Ambroise C","Grandvalet Y","Dalmasso C"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"1ad1c397ca84bf9625f4967d8da80d37","permalink":"https://cambroise.github.io/publication/becu2015significance/","publishdate":"2015-01-01T00:00:00Z","relpermalink":"/publication/becu2015significance/","section":"publication","summary":"","tags":null,"title":" Significance testing for variable selection in high-dimension ","type":"publication"},{"authors":["Latouche P","Birmele E","Ambroise C","others"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"8ecc99cea8d2c9d46f7af256351f1b91","permalink":"https://cambroise.github.io/publication/latouche2014model/","publishdate":"2014-01-01T00:00:00Z","relpermalink":"/publication/latouche2014model/","section":"publication","summary":"","tags":null,"title":" Model selection in overlapping stochastic block models ","type":"publication"},{"authors":["Latouche P","Birmele E","Ambroise C"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"46c4a9dcadc5adde75593d65e7b94d9a","permalink":"https://cambroise.github.io/publication/latouche2014overlapping/","publishdate":"2014-01-01T00:00:00Z","relpermalink":"/publication/latouche2014overlapping/","section":"publication","summary":"","tags":null,"title":" Overlapping clustering methods for networks ","type":"publication"},{"authors":["AMBROISE C","DANG M","GOVAERT G"],"categories":null,"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356998400,"objectID":"8deec12e9bb5458e5a5afa9e0e60da60","permalink":"https://cambroise.github.io/publication/ambroise2013bp/","publishdate":"2013-01-01T00:00:00Z","relpermalink":"/publication/ambroise2013bp/","section":"publication","summary":"","tags":null,"title":" BP 529 F-60205 Compi{e}gne cedea-France ","type":"publication"},{"authors":["Dehman A","Ambroise C","Neuvial P"],"categories":null,"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356998400,"objectID":"52096dd466b8ddcf50b49d49c6f58237","permalink":"https://cambroise.github.io/publication/dehman2013incorporating/","publishdate":"2013-01-01T00:00:00Z","relpermalink":"/publication/dehman2013incorporating/","section":"publication","summary":"","tags":null,"title":" Incorporating linkage disequilibrium blocks in Genome-Wide Association Studies ","type":"publication"},{"authors":["Ambroise C","Govaert G"],"categories":null,"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376000,"objectID":"253ba86bbc6b1362151ff21148317621","permalink":"https://cambroise.github.io/publication/ambroise2012clustering/","publishdate":"2012-01-01T00:00:00Z","relpermalink":"/publication/ambroise2012clustering/","section":"publication","summary":"","tags":null,"title":" Clustering by maximizing a fuzzy ","type":"publication"},{"authors":["McLachlan G"],"categories":null,"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376000,"objectID":"987641641a10382d2865a3fc6ffcbcc1","permalink":"https://cambroise.github.io/publication/mclachlan2012discriminant/","publishdate":"2012-01-01T00:00:00Z","relpermalink":"/publication/mclachlan2012discriminant/","section":"publication","summary":"","tags":null,"title":" Discriminant analysis ","type":"publication"},{"authors":["Bouaziz M"],"categories":null,"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376000,"objectID":"d08b962eb213ef3c4e8e11802ff05332","permalink":"https://cambroise.github.io/publication/bouaziz2012ships/","publishdate":"2012-01-01T00:00:00Z","relpermalink":"/publication/bouaziz2012ships/","section":"publication","summary":"","tags":null,"title":" SHIPS: spectral hierarchical clustering for the inference of population structure ","type":"publication"},{"authors":["Grandvalet Y","Chiquet J","Ambroise C"],"categories":null,"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376000,"objectID":"1a2855382b91aa8a21e1452ddfa939a9","permalink":"https://cambroise.github.io/publication/grandvalet2012sparsity/","publishdate":"2012-01-01T00:00:00Z","relpermalink":"/publication/grandvalet2012sparsity/","section":"publication","summary":"","tags":null,"title":" Sparsity by Worst-Case Penalties ","type":"publication"},{"authors":["Latouche P","Birmele E","Ambroise C"],"categories":null,"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376000,"objectID":"8dd0e08134b67698e8730fef15f588e4","permalink":"https://cambroise.github.io/publication/latouche2012variational/","publishdate":"2012-01-01T00:00:00Z","relpermalink":"/publication/latouche2012variational/","section":"publication","summary":"","tags":null,"title":" Variational Bayesian inference and complexity control for stochastic block models ","type":"publication"},{"authors":["Bouaziz M","Ambroise C","Guedj M"],"categories":null,"content":"","date":1293840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1293840000,"objectID":"5f3b18776db9f5714ec02a0812a8d439","permalink":"https://cambroise.github.io/publication/bouaziz2011accounting/","publishdate":"2011-01-01T00:00:00Z","relpermalink":"/publication/bouaziz2011accounting/","section":"publication","summary":"","tags":null,"title":" Accounting for population stratification in practice: a comparison of the main strategies dedicated to genome-wide association studies ","type":"publication"},{"authors":["Jeanmougin M","Guedj M","Ambroise C"],"categories":null,"content":"","date":1293840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1293840000,"objectID":"b4b5515fe06b65b76fd36a61eb50027d","permalink":"https://cambroise.github.io/publication/jeanmougin2011defining/","publishdate":"2011-01-01T00:00:00Z","relpermalink":"/publication/jeanmougin2011defining/","section":"publication","summary":"","tags":null,"title":" Defining a robust biological prior from pathway analysis to drive network inference ","type":"publication"},{"authors":["Chiquet J","Grandvalet Y","Ambroise C"],"categories":null,"content":"","date":1293840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1293840000,"objectID":"69f382b93a0b9691286a81a110d4e944","permalink":"https://cambroise.github.io/publication/chiquet2011inferring/","publishdate":"2011-01-01T00:00:00Z","relpermalink":"/publication/chiquet2011inferring/","section":"publication","summary":"","tags":null,"title":" Inferring multiple graphical structures ","type":"publication"},{"authors":["Ambroise C","Matias C"],"categories":null,"content":"","date":1293840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1293840000,"objectID":"9e845a60f82acc282713d39f8b24fc37","permalink":"https://cambroise.github.io/publication/ambroise2011new/","publishdate":"2011-01-01T00:00:00Z","relpermalink":"/publication/ambroise2011new/","section":"publication","summary":"","tags":null,"title":" New consistent and asymptotically normal parameter estimates for random-graph mixture models ","type":"publication"},{"authors":["Latouche P","Birmele E","Ambroise C","others"],"categories":null,"content":"","date":1293840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1293840000,"objectID":"1a50262aae9f432b2a8395110e833149","permalink":"https://cambroise.github.io/publication/latouche2011overlapping/","publishdate":"2011-01-01T00:00:00Z","relpermalink":"/publication/latouche2011overlapping/","section":"publication","summary":"","tags":null,"title":" Overlapping stochastic block models with application to the french political blogosphere ","type":"publication"},{"authors":["Zanghi H","Volant S","Ambroise C"],"categories":null,"content":"","date":1262304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1262304000,"objectID":"32a5fe07aa5731c1d85ba316d4a614e2","permalink":"https://cambroise.github.io/publication/zanghi2010clustering/","publishdate":"2010-01-01T00:00:00Z","relpermalink":"/publication/zanghi2010clustering/","section":"publication","summary":"","tags":null,"title":" Clustering based on random graph model embedding vertex features ","type":"publication"},{"authors":["Ambroise C","Matias C"],"categories":null,"content":"","date":1262304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1262304000,"objectID":"4a1c5c5a55b51579b3623cf0f2df1478","permalink":"https://cambroise.github.io/publication/ambroise2010new/","publishdate":"2010-01-01T00:00:00Z","relpermalink":"/publication/ambroise2010new/","section":"publication","summary":"","tags":null,"title":" New consistent and asymptotically normal estimators for random graph mixture models ","type":"publication"},{"authors":["Zanghi H","Picard F","Miele V","Ambroise C"],"categories":null,"content":"","date":1262304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1262304000,"objectID":"85341f90676c822992dfe4d97018a00e","permalink":"https://cambroise.github.io/publication/zanghi2010strategies/","publishdate":"2010-01-01T00:00:00Z","relpermalink":"/publication/zanghi2010strategies/","section":"publication","summary":"","tags":null,"title":" Strategies for online inference of model-based clustering in large and growing networks ","type":"publication"},{"authors":["Charbonnier C","Chiquet J","Ambroise C"],"categories":null,"content":"","date":1262304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1262304000,"objectID":"a806f998e275f94f19e91dc48ff6c8dd","permalink":"https://cambroise.github.io/publication/charbonnier2010weighted/","publishdate":"2010-01-01T00:00:00Z","relpermalink":"/publication/charbonnier2010weighted/","section":"publication","summary":"","tags":null,"title":" Weighted-LASSO for structured network inference from time course data ","type":"publication"},{"authors":["Latouche P","Birmele E","Ambroise C"],"categories":null,"content":"","date":1230768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1230768000,"objectID":"033432305ce4e13ca42fc1a8e0fdf197","permalink":"https://cambroise.github.io/publication/latouche2009bayesian/","publishdate":"2009-01-01T00:00:00Z","relpermalink":"/publication/latouche2009bayesian/","section":"publication","summary":"","tags":null,"title":" Bayesian methods for graph clustering ","type":"publication"},{"authors":["Ambroise C","Chiquet J","Matias C","others"],"categories":null,"content":"","date":1230768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1230768000,"objectID":"fa585c8424ef99e3611d0be204f883dc","permalink":"https://cambroise.github.io/publication/ambroise2009inferring/","publishdate":"2009-01-01T00:00:00Z","relpermalink":"/publication/ambroise2009inferring/","section":"publication","summary":"","tags":null,"title":" Inferring sparse Gaussian graphical models with latent structure ","type":"publication"},{"authors":["Latouche P","Birmele E","Ambroise C"],"categories":null,"content":"","date":1230768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1230768000,"objectID":"cb272f659530000edae91deb692acada","permalink":"https://cambroise.github.io/publication/latouche2009overlapping/","publishdate":"2009-01-01T00:00:00Z","relpermalink":"/publication/latouche2009overlapping/","section":"publication","summary":"","tags":null,"title":" Overlapping stochastic block models ","type":"publication"},{"authors":["Chiquet J","Smith A","Grasseau G","Matias C","Ambroise C"],"categories":null,"content":"","date":1230768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1230768000,"objectID":"f25d91d0db3041b52fe02a1018ba77ce","permalink":"https://cambroise.github.io/publication/chiquet2009simone/","publishdate":"2009-01-01T00:00:00Z","relpermalink":"/publication/chiquet2009simone/","section":"publication","summary":"","tags":null,"title":" Simone: Statistical inference for modular networks ","type":"publication"},{"authors":["Ambroise C","Dang M"],"categories":null,"content":"","date":1230768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1230768000,"objectID":"e3ca82f4783d1dc1cded77149882a161","permalink":"https://cambroise.github.io/publication/ambroise2009spatial/","publishdate":"2009-01-01T00:00:00Z","relpermalink":"/publication/ambroise2009spatial/","section":"publication","summary":"","tags":null,"title":" Spatial Data Clustering ","type":"publication"},{"authors":["Zanghi H","Ambroise C","Miele V"],"categories":null,"content":"","date":1199145600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1199145600,"objectID":"eba79c30060879afdc99cd7e22851734","permalink":"https://cambroise.github.io/publication/zanghi2008fast/","publishdate":"2008-01-01T00:00:00Z","relpermalink":"/publication/zanghi2008fast/","section":"publication","summary":"","tags":null,"title":" Fast online graph clustering via Erdos--R{e}nyi mixture ","type":"publication"},{"authors":["Birmele E","Elati M","Rouveirol C","Ambroise C"],"categories":null,"content":"","date":1199145600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1199145600,"objectID":"9a9724e5aa5d542a5fbb4d5ab089cb98","permalink":"https://cambroise.github.io/publication/birmele2008identification/","publishdate":"2008-01-01T00:00:00Z","relpermalink":"/publication/birmele2008identification/","section":"publication","summary":"","tags":null,"title":" Identification of functional modules based on transcriptional regulation structure ","type":"publication"},{"authors":["Zanghi H","Picard F","Miele V","Ambroise C"],"categories":null,"content":"","date":1199145600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1199145600,"objectID":"7d06925eb7791f3c576a35a164eb1b97","permalink":"https://cambroise.github.io/publication/zanghi2008strategies/","publishdate":"2008-01-01T00:00:00Z","relpermalink":"/publication/zanghi2008strategies/","section":"publication","summary":"","tags":null,"title":" Strategies for online inference of network mixture ","type":"publication"},{"authors":["Same A","Ambroise C","Govaert G"],"categories":null,"content":"","date":1167609600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1167609600,"objectID":"03d844a90a5843b894bd2abf801624ef","permalink":"https://cambroise.github.io/publication/same2007online/","publishdate":"2007-01-01T00:00:00Z","relpermalink":"/publication/same2007online/","section":"publication","summary":"","tags":null,"title":" An online classification EM algorithm based on the mixture model ","type":"publication"},{"authors":["Avalos M","Grandvalet Y","Ambroise C"],"categories":null,"content":"","date":1167609600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1167609600,"objectID":"827e7f07773ce0f01499f4a967af2bb0","permalink":"https://cambroise.github.io/publication/avalos2007parsimonious/","publishdate":"2007-01-01T00:00:00Z","relpermalink":"/publication/avalos2007parsimonious/","section":"publication","summary":"","tags":null,"title":" Parsimonious additive models ","type":"publication"},{"authors":["Same A","Ambroise C","Govaert G"],"categories":null,"content":"","date":1136073600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1136073600,"objectID":"560131343a7c6afec88c883e47af057b","permalink":"https://cambroise.github.io/publication/same2006classification/","publishdate":"2006-01-01T00:00:00Z","relpermalink":"/publication/same2006classification/","section":"publication","summary":"","tags":null,"title":" A classification EM algorithm for binned data ","type":"publication"},{"authors":["Najjar M","Ambroise C","Cocquerez J","Cotten A","Eltabach M"],"categories":null,"content":"","date":1136073600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1136073600,"objectID":"d0c2596a0825ea8e86aad16c8a096cba","permalink":"https://cambroise.github.io/publication/najjar2006content/","publishdate":"2006-01-01T00:00:00Z","relpermalink":"/publication/najjar2006content/","section":"publication","summary":"","tags":null,"title":" A Content-Based Image Retrieval System for Osteo-Articular Applications ","type":"publication"},{"authors":["Yousfi K","Ambroise C","Cocquerez J","Chevelu J"],"categories":null,"content":"","date":1136073600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1136073600,"objectID":"f1465340a0c4185337288fe0d6977342","permalink":"https://cambroise.github.io/publication/yousfi2006driving/","publishdate":"2006-01-01T00:00:00Z","relpermalink":"/publication/yousfi2006driving/","section":"publication","summary":"","tags":null,"title":" Driving Hierarchy Construction via Supervised Learning: Application to Osteo-Articular Medical Images Database ","type":"publication"},{"authors":["Cord A","Ambroise C","Cocquerez J"],"categories":null,"content":"","date":1136073600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1136073600,"objectID":"94f2d31db5b644e99a308e403546afd8","permalink":"https://cambroise.github.io/publication/cord2006feature/","publishdate":"2006-01-01T00:00:00Z","relpermalink":"/publication/cord2006feature/","section":"publication","summary":"","tags":null,"title":" Feature selection in robust clustering based on Laplace mixture ","type":"publication"},{"authors":["Zhu X","Ambroise C","McLachlan G"],"categories":null,"content":"","date":1136073600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1136073600,"objectID":"c67c45b560093baf222d924494f1ccdc","permalink":"https://cambroise.github.io/publication/zhu2006selection/","publishdate":"2006-01-01T00:00:00Z","relpermalink":"/publication/zhu2006selection/","section":"publication","summary":"","tags":null,"title":" Selection bias in working with the top genes in supervised classification of tissue samples ","type":"publication"},{"authors":["Yousfi K","Ambroise C","Cocquerez J","Chevelu J"],"categories":null,"content":"","date":1136073600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1136073600,"objectID":"702f4201cb1efc65b79244d8b1762031","permalink":"https://cambroise.github.io/publication/yousfi2006supervised/","publishdate":"2006-01-01T00:00:00Z","relpermalink":"/publication/yousfi2006supervised/","section":"publication","summary":"","tags":null,"title":" Supervised learning for guiding hierarchy construction: Application to osteo-articular medical images database ","type":"publication"},{"authors":["Charkaoui N","Dubuisson B","Ambroise C","Millemann S"],"categories":null,"content":"","date":1104537600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1104537600,"objectID":"4868d31e195757a38670d410c720b565","permalink":"https://cambroise.github.io/publication/charkaoui2005decision/","publishdate":"2005-01-01T00:00:00Z","relpermalink":"/publication/charkaoui2005decision/","section":"publication","summary":"","tags":null,"title":" A decision tree classifier for vehicle failure isolation ","type":"publication"},{"authors":["Same A","Govaert G","Ambroise C"],"categories":null,"content":"","date":1104537600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1104537600,"objectID":"42b532389133a2967c80037fce485c98","permalink":"https://cambroise.github.io/publication/same2005mixture/","publishdate":"2005-01-01T00:00:00Z","relpermalink":"/publication/same2005mixture/","section":"publication","summary":"","tags":null,"title":" A mixture model-based on-line CEM algorithm ","type":"publication"},{"authors":["Govaert G","AMBROISE C"],"categories":null,"content":"","date":1104537600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1104537600,"objectID":"cbe031b742532717ccf0da94c7686086","permalink":"https://cambroise.github.io/publication/govaert2005analyse/","publishdate":"2005-01-01T00:00:00Z","relpermalink":"/publication/govaert2005analyse/","section":"publication","summary":"","tags":null,"title":" Analyse de Donn{e}es et Data Mining ","type":"publication"},{"authors":["McLachlan G","Do K","Ambroise C"],"categories":null,"content":"","date":1104537600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1104537600,"objectID":"603cddd74f0676c78cf0137218df9a21","permalink":"https://cambroise.github.io/publication/mclachlan2005analyzing/","publishdate":"2005-01-01T00:00:00Z","relpermalink":"/publication/mclachlan2005analyzing/","section":"publication","summary":"","tags":null,"title":" Analyzing microarray gene expression data ","type":"publication"},{"authors":["Chaumont L","Yor M","Finkenstadt B","Rootzen H","Franke J","Hardle W","Hafner C","Gentle J","Mori Y","Muller M","others"],"categories":null,"content":"","date":1104537600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1104537600,"objectID":"7f2027bc9e1edb2668fa315adf7fe754","permalink":"https://cambroise.github.io/publication/chaumont2005books/","publishdate":"2005-01-01T00:00:00Z","relpermalink":"/publication/chaumont2005books/","section":"publication","summary":"","tags":null,"title":" Books for review If you would like to review a book, and thereby to retain it for your collection, please contact the Book Reviews Editor, whose details can be found by clicking on \u003cU+2018\u003ebooks currently available\u003cU+2019\u003ein the information on the Royal Statistical Society\u003cU+2019\u003es Web site ","type":"publication"},{"authors":["Ziegel E"],"categories":null,"content":"","date":1104537600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1104537600,"objectID":"8051f471dba876e91f283be85f8deba8","permalink":"https://cambroise.github.io/publication/ziegel2005data/","publishdate":"2005-01-01T00:00:00Z","relpermalink":"/publication/ziegel2005data/","section":"publication","summary":"","tags":null,"title":" Data Analysis Tools for DNA Microarrays/Analyzing Microarray Gene Expression Data ","type":"publication"},{"authors":["Avalos M","Grandvalet Y","Ambroise C","others"],"categories":null,"content":"","date":1104537600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1104537600,"objectID":"3747c122562518853aac87505f9ac1b9","permalink":"https://cambroise.github.io/publication/avalos2005discrimination/","publishdate":"2005-01-01T00:00:00Z","relpermalink":"/publication/avalos2005discrimination/","section":"publication","summary":"","tags":null,"title":" Discrimination par mod{e}les additifs parcimonieux ","type":"publication"},{"authors":["Avalos M","Grandvalet Y","Ambroise C"],"categories":null,"content":"","date":1104537600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1104537600,"objectID":"1616a72e21fcdd01820a8a03f88cfa31","permalink":"https://cambroise.github.io/publication/avalos2005model/","publishdate":"2005-01-01T00:00:00Z","relpermalink":"/publication/avalos2005model/","section":"publication","summary":"","tags":null,"title":" Model selection via penalization in the additive Cox model ","type":"publication"},{"authors":["Avalos M","Grandvalet Y","Ambroise C"],"categories":null,"content":"","date":1104537600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1104537600,"objectID":"f76380eb64aaea6756cfa2776b3fba53","permalink":"https://cambroise.github.io/publication/avalos2005penalisation/","publishdate":"2005-01-01T00:00:00Z","relpermalink":"/publication/avalos2005penalisation/","section":"publication","summary":"","tags":null,"title":" P{e}nalisation l1 pour les MAG ","type":"publication"},{"authors":["Charkaoui N","Dubuisson B","Ambroise C","Boatas A"],"categories":null,"content":"","date":1104537600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1104537600,"objectID":"c0e2dcdf8ba156eb4392a119d481dd44","permalink":"https://cambroise.github.io/publication/charkaoui2005pattern/","publishdate":"2005-01-01T00:00:00Z","relpermalink":"/publication/charkaoui2005pattern/","section":"publication","summary":"","tags":null,"title":" Pattern recognition method for off-board automotive vehicle failure isolation ","type":"publication"},{"authors":["Jones L","Ng S","Ambroise C","Monico K","Khan N","McLachlan G"],"categories":null,"content":"","date":1104537600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1104537600,"objectID":"5b9eba4879ca77064845ec35dcd644c7","permalink":"https://cambroise.github.io/publication/jones2005use/","publishdate":"2005-01-01T00:00:00Z","relpermalink":"/publication/jones2005use/","section":"publication","summary":"","tags":null,"title":" Use of Micro Array Data via Model-based Classification in the Study and Prediction of Survival from Lung Cancer ","type":"publication"},{"authors":["Same A","Ambroise C","Govaert G"],"categories":null,"content":"","date":1072915200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1072915200,"objectID":"f9f62eef50f5ed815e48ea5c29f2d3e6","permalink":"https://cambroise.github.io/publication/same2004mixture/","publishdate":"2004-01-01T00:00:00Z","relpermalink":"/publication/same2004mixture/","section":"publication","summary":"","tags":null,"title":" A mixture model approach for on-line clustering ","type":"publication"},{"authors":["Avalos M","Grandvalet Y","Ambroise C"],"categories":null,"content":"","date":1072915200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1072915200,"objectID":"1355786c9af4bc9f390627175c57b796","permalink":"https://cambroise.github.io/publication/avalos2004generalisation/","publishdate":"2004-01-01T00:00:00Z","relpermalink":"/publication/avalos2004generalisation/","section":"publication","summary":"","tags":null,"title":" G{e}n{e}ralisation du lasso aux modeles additifs ","type":"publication"},{"authors":["McLachlan G","Do K","Ambroise C"],"categories":null,"content":"","date":1072915200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1072915200,"objectID":"1c75de4b80f90e58d2ac00a6c1960fb4","permalink":"https://cambroise.github.io/publication/mclachlan2004microarrays/","publishdate":"2004-01-01T00:00:00Z","relpermalink":"/publication/mclachlan2004microarrays/","section":"publication","summary":"","tags":null,"title":" Microarrays in gene expression studies ","type":"publication"},{"authors":["Hamdan H","Govaert G","Ambroise C","Herve C"],"categories":null,"content":"","date":1072915200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1072915200,"objectID":"c99479ad98059a125fc8743e4adbec59","permalink":"https://cambroise.github.io/publication/hamdan2004mixture/","publishdate":"2004-01-01T00:00:00Z","relpermalink":"/publication/hamdan2004mixture/","section":"publication","summary":"","tags":null,"title":" Mixture model approach for acoustic emission control of pressure equipment ","type":"publication"},{"authors":["McLachlan G","Chang S","Mar J","Ambroise C"],"categories":null,"content":"","date":1072915200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1072915200,"objectID":"31f71dfa839cab94839dc278530f5bf0","permalink":"https://cambroise.github.io/publication/mclachlan2004simultaneous/","publishdate":"2004-01-01T00:00:00Z","relpermalink":"/publication/mclachlan2004simultaneous/","section":"publication","summary":"","tags":null,"title":" On the simultaneous use of clinical and microarray expression data in the cluster analysis of tissue samples ","type":"publication"},{"authors":["Avalos M","Grandvalet Y","Ambroise C"],"categories":null,"content":"","date":1072915200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1072915200,"objectID":"a29311ac3631ec531d2a500d2053c7bc","permalink":"https://cambroise.github.io/publication/avalos2004penalized/","publishdate":"2004-01-01T00:00:00Z","relpermalink":"/publication/avalos2004penalized/","section":"publication","summary":"","tags":null,"title":" Penalized additive logistic regression for cardiovascular risk prediction ","type":"publication"},{"authors":["Same A","Ambroise C","Govaert G"],"categories":null,"content":"","date":1041379200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1041379200,"objectID":"812b14c7596eb34fce08db428bcaac98","permalink":"https://cambroise.github.io/publication/same2003mixture/","publishdate":"2003-01-01T00:00:00Z","relpermalink":"/publication/same2003mixture/","section":"publication","summary":"","tags":null,"title":" A mixture model approach for binned data clustering ","type":"publication"},{"authors":["Najjar M","Ambroise C","Cocquerez J"],"categories":null,"content":"","date":1041379200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1041379200,"objectID":"2f6da1e12ef1b789edffde41de0794c4","permalink":"https://cambroise.github.io/publication/najjar2003feature/","publishdate":"2003-01-01T00:00:00Z","relpermalink":"/publication/najjar2003feature/","section":"publication","summary":"","tags":null,"title":" Feature selection for semisupervised learning applied to image retrieval ","type":"publication"},{"authors":["Najjar M","Ambroise C","Cocquerez J"],"categories":null,"content":"","date":1041379200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1041379200,"objectID":"e03fdd9a8fa6a28c51fb437f32de6235","permalink":"https://cambroise.github.io/publication/najjar2003image/","publishdate":"2003-01-01T00:00:00Z","relpermalink":"/publication/najjar2003image/","section":"publication","summary":"","tags":null,"title":" Image retrieval using mixture models and em algorithm ","type":"publication"},{"authors":["Avalos M","Grandvalet Y","Ambroise C"],"categories":null,"content":"","date":1041379200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1041379200,"objectID":"784f00bdf80cb199ad26a97d61cdcc74","permalink":"https://cambroise.github.io/publication/avalos2003regularization/","publishdate":"2003-01-01T00:00:00Z","relpermalink":"/publication/avalos2003regularization/","section":"publication","summary":"","tags":null,"title":" Regularization methods for additive models ","type":"publication"},{"authors":["Najjar M","Cocquerez J","Ambroise C"],"categories":null,"content":"","date":1009843200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1009843200,"objectID":"61ba01b3515efab42ccfb596b2643aec","permalink":"https://cambroise.github.io/publication/najjar2002semi/","publishdate":"2002-01-01T00:00:00Z","relpermalink":"/publication/najjar2002semi/","section":"publication","summary":"","tags":null,"title":" A Semi-supervised Learning Approach to Image Retrieval ","type":"publication"},{"authors":["Ambroise C","Govaert G"],"categories":null,"content":"","date":1009843200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1009843200,"objectID":"2a7330fa4ec5397790595a65d2ebec04","permalink":"https://cambroise.github.io/publication/ambroise2002clustering/","publishdate":"2002-01-01T00:00:00Z","relpermalink":"/publication/ambroise2002clustering/","section":"publication","summary":"","tags":null,"title":" Clustering and Models ","type":"publication"},{"authors":["Ambroise C","McLachlan G"],"categories":null,"content":"","date":1009843200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1009843200,"objectID":"07d9ac850f6a4a2ed0bfe3a49728d281","permalink":"https://cambroise.github.io/publication/ambroise2002selection/","publishdate":"2002-01-01T00:00:00Z","relpermalink":"/publication/ambroise2002selection/","section":"publication","summary":"","tags":null,"title":" Selection bias in gene extraction on the basis of microarray gene-expression data ","type":"publication"},{"authors":["Grandvalet Y","dAlche-Buc F","Ambroise C"],"categories":null,"content":"","date":978307200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":978307200,"objectID":"f9cb84d19ff3f4ea3b9cab9c360e1e77","permalink":"https://cambroise.github.io/publication/grandvalet2001boosting/","publishdate":"2001-01-01T00:00:00Z","relpermalink":"/publication/grandvalet2001boosting/","section":"publication","summary":"","tags":null,"title":" Boosting mixture models for semi-supervised learning ","type":"publication"},{"authors":["Ambroise C","Denoeux T","Govaert G","Smets P"],"categories":null,"content":"","date":978307200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":978307200,"objectID":"24062a6137dd4dccc6a80c5df8e4766b","permalink":"https://cambroise.github.io/publication/ambroise2001learning/","publishdate":"2001-01-01T00:00:00Z","relpermalink":"/publication/ambroise2001learning/","section":"publication","summary":"","tags":null,"title":" Learning from an imprecise teacher: probabilistic and evidential approaches ","type":"publication"},{"authors":["Ambroise C","Grandvalet Y"],"categories":null,"content":"","date":978307200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":978307200,"objectID":"c1d433f83843526de8db7f41a0e709e0","permalink":"https://cambroise.github.io/publication/ambroise2001prediction/","publishdate":"2001-01-01T00:00:00Z","relpermalink":"/publication/ambroise2001prediction/","section":"publication","summary":"","tags":null,"title":" Prediction of ozone peaks by mixture models ","type":"publication"},{"authors":["d'Alche-Buc F","Grandvalet Y","Ambroise C"],"categories":null,"content":"","date":978307200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":978307200,"objectID":"6dc14d6d082d29c2560246695c6519f6","permalink":"https://cambroise.github.io/publication/d2001semi/","publishdate":"2001-01-01T00:00:00Z","relpermalink":"/publication/d2001semi/","section":"publication","summary":"","tags":null,"title":" Semi-supervised marginboost ","type":"publication"},{"authors":["Grandvalet Y","Ambroise C","others"],"categories":null,"content":"","date":978307200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":978307200,"objectID":"57f5b66bc95fd8febbfc917db534f3c8","permalink":"https://cambroise.github.io/publication/grandvalet2001semi/","publishdate":"2001-01-01T00:00:00Z","relpermalink":"/publication/grandvalet2001semi/","section":"publication","summary":"","tags":null,"title":" Semi-supervised marginboost ","type":"publication"},{"authors":["Ambroise C","Govaert G"],"categories":null,"content":"","date":946684800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":946684800,"objectID":"2fc11393c2c16e5a1a4b2b03c7ed180f","permalink":"https://cambroise.github.io/publication/ambroise2000clustering/","publishdate":"2000-01-01T00:00:00Z","relpermalink":"/publication/ambroise2000clustering/","section":"publication","summary":"","tags":null,"title":" Clustering by maximizing a fuzzy classification maximum likelihood criterion ","type":"publication"},{"authors":["Ambroise C","Govaert G"],"categories":null,"content":"","date":946684800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":946684800,"objectID":"57c8bf994714a85e43a0e647f6fe52d6","permalink":"https://cambroise.github.io/publication/ambroise2000algorithm/","publishdate":"2000-01-01T00:00:00Z","relpermalink":"/publication/ambroise2000algorithm/","section":"publication","summary":"","tags":null,"title":" EM algorithm for partially known labels ","type":"publication"},{"authors":["Ambroise C","Seze G","Badran F","Thiria S"],"categories":null,"content":"","date":946684800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":946684800,"objectID":"8e0decc8596c241f5aa6549d5d3ed390","permalink":"https://cambroise.github.io/publication/ambroise2000hierarchical/","publishdate":"2000-01-01T00:00:00Z","relpermalink":"/publication/ambroise2000hierarchical/","section":"publication","summary":"","tags":null,"title":" Hierarchical clustering of self-organizing maps for cloud classification ","type":"publication"},{"authors":["Ambroise C","Govaert G"],"categories":null,"content":"","date":883612800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":883612800,"objectID":"066228185da3de50846b87543f26568c","permalink":"https://cambroise.github.io/publication/ambroise1998iterative/","publishdate":"1998-01-01T00:00:00Z","relpermalink":"/publication/ambroise1998iterative/","section":"publication","summary":"","tags":null,"title":" An iterative algorithm for spatial clustering ","type":"publication"},{"authors":["Ambroise C","Govaert G"],"categories":null,"content":"","date":883612800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":883612800,"objectID":"76915058850ad3fcb5d2ad1619c3bda0","permalink":"https://cambroise.github.io/publication/ambroise1998convergence/","publishdate":"1998-01-01T00:00:00Z","relpermalink":"/publication/ambroise1998convergence/","section":"publication","summary":"","tags":null,"title":" Convergence of an EM-type algorithm for spatial clustering ","type":"publication"},{"authors":["Ambroise C"],"categories":null,"content":"","date":883612800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":883612800,"objectID":"1937caa21cc30f2cb449bd64b9c5c653","permalink":"https://cambroise.github.io/publication/ambroise1998algorithm/","publishdate":"1998-01-01T00:00:00Z","relpermalink":"/publication/ambroise1998algorithm/","section":"publication","summary":"","tags":null,"title":" The EM Algorithm and Extensions, by GM McLachlan and T. Krishnan ","type":"publication"},{"authors":["Ambroise C","Dang M","Govaert G"],"categories":null,"content":"","date":852076800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":852076800,"objectID":"cf7549f12c044a2f0da69716defb3983","permalink":"https://cambroise.github.io/publication/ambroise1997clustering/","publishdate":"1997-01-01T00:00:00Z","relpermalink":"/publication/ambroise1997clustering/","section":"publication","summary":"","tags":null,"title":" Clustering of spatial data by the EM algorithm ","type":"publication"},{"authors":["Ambroise C"],"categories":null,"content":"","date":820454400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":820454400,"objectID":"549492f57a1bf0637db0e03719c72f4c","permalink":"https://cambroise.github.io/publication/ambroise1996analyzing/","publishdate":"1996-01-01T00:00:00Z","relpermalink":"/publication/ambroise1996analyzing/","section":"publication","summary":"","tags":null,"title":" Analyzing dissimilarity matrices via Kohonen maps ","type":"publication"},{"authors":["Ambroise C"],"categories":null,"content":"","date":820454400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":820454400,"objectID":"dcb1659670b01efc0a09a16782e9334a","permalink":"https://cambroise.github.io/publication/ambroise1996approche/","publishdate":"1996-01-01T00:00:00Z","relpermalink":"/publication/ambroise1996approche/","section":"publication","summary":"","tags":null,"title":" Approche probabiliste en classification automatique et contraintes de voisinage ","type":"publication"},{"authors":["Ambroise C","Govaert G"],"categories":null,"content":"","date":820454400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":820454400,"objectID":"940d6b99b7230956ed19a75d4973f48c","permalink":"https://cambroise.github.io/publication/ambroise1996constrained/","publishdate":"1996-01-01T00:00:00Z","relpermalink":"/publication/ambroise1996constrained/","section":"publication","summary":"","tags":null,"title":" Constrained clustering and Kohonen self-organizing maps ","type":"publication"},{"authors":null,"categories":null,"content":"Simulations Gaussiennes import numpy as np import matplotlib.pyplot as plt from scipy.stats import multivariate_normal from matplotlib.colors import ListedColormap mycolormap = ListedColormap(['#FF0000', '#0000FF']) Exchoice = 1 # Choisir l'exemple ############################################## # Data set generation using scipy.stats.multivariate_normal ############################################# def generate_data(n1, n2, mu1, cov1, mu2, cov2): \u0026quot;\u0026quot;\u0026quot; Generates simulated data from two multivariate normal distributions. Parameters: - n1: Size of sample 1 - n2: Size of sample 2 - mu1: Mean vector for group 1 (in 2 dimensions) - cov1: Covariance matrix for group 1 (in 2 dimensions) - mu2: Mean vector for group 2 (in 2 dimensions) - cov2: Covariance matrix for group 2 (in 2 dimensions) Returns: - X: Concatenated data matrix for both groups (n1 + n2, 2) - Y: Associated class vector (0 for group 1, 1 for group 2) \u0026quot;\u0026quot;\u0026quot; # Generate data for each group xG1 = multivariate_normal(mean=mu1, cov=cov1).rvs(n1) xG2 = multivariate_normal(mean=mu2, cov=cov2).rvs(n2) # Concatenate data from both groups X = np.concatenate((xG1, xG2), axis=0) # Create class vector (labels) Y = np.array([0] * n1 + [1] * n2) return X, Y n1, n2 = 50, 150 mu1 = [2, 2] # Moyenne G1 cov1 = [[1, 0], [0, 1]] # Matrice de covariance G1 (indépendance) mu2 = [0, 0] # Moyenne G2 cov2 = [[1, 0], [0, 2]] # Matrice de covariance G2 (indépendance mais variance différente) X, Y = generate_data(n1, n2, mu1, cov1, mu2, cov2) # Visualisation des données générées plt.scatter(X[:, 0], X[:, 1], c=Y, cmap=mycolormap) plt.title('Raw data') plt.grid() plt.xlabel('x1') plt.ylabel('x2') plt.show() Simulation de l\u0026rsquo;exemple 2 (gaussiennes côte-côte) puis affichage de la posterior $P(X = x \\mid Y = 1)$ est une gaussienne bivariée avec une moyenne $\\mu_1 = (\\mu_{11}, \\mu_{12})$ et une matrice de covariance $\\Sigma_1$.\nLa densité a posteriori $P(Y = 1 \\mid X = x)$ peut être calculée à l\u0026rsquo;aide du théorème de Bayes :\n$$ P(Y = 1 \\mid X = x) = \\frac{P(X = x \\mid Y = 1) P(Y = 1)}{P(X = x)} $$\noù :\n$$ P(X = x) = P(X = x \\mid Y = 1) P(Y = 1) + P(X = x \\mid Y = 0) P(Y = 0) $$\nCela permet de calculer la probabilité a posteriori en fonction des probabilités conditionnelles et des probabilités a priori.\nimport numpy as np import matplotlib.pyplot as plt from scipy.stats import multivariate_normal class Bayes_Classifier: def __init__(self, mu1, cov1, mu2, cov2, p1=0.5): \u0026quot;\u0026quot;\u0026quot; Initializes the Bayes Classifier with parameters of two bivariate Gaussian distributions. Parameters: - mu1: Mean vector for class 1 (in 2 dimensions) - cov1: Covariance matrix for class 1 (in 2 dimensions) - mu2: Mean vector for class 0 (in 2 dimensions) - cov2: Covariance matrix for class 0 (in 2 dimensions) - p1: Prior probability of class 1 (default is 0.5) \u0026quot;\u0026quot;\u0026quot; self.mu1 = mu1 self.cov1 = cov1 self.mu2 = mu2 self.cov2 = cov2 self.p1 = p1 self.p2 = 1 - p1 self.rv1 = multivariate_normal(mu1, cov1, allow_singular=True) self.rv2 = multivariate_normal(mu2, cov2, allow_singular=True) def predict_proba(self, X): \u0026quot;\u0026quot;\u0026quot; Predicts the posterior probability of class 1 for the given data points. Parameters: - X: Data points (n_samples, 2) Returns: - p1_x: Posterior probability of class 1 for each data point \u0026quot;\u0026quot;\u0026quot; # Calculate likelihoods P(X | Y=1) and P(X | Y=0) px_1 = self.rv1.pdf(X) px_2 = self.rv2.pdf(X) # Total density P(X=x) px = px_1 * self.p1 + px_2 * self.p2 # Posterior probability P(Y=1 | X=x) using Bayes' theorem p1_x = (px_1 * self.p1) / px return p1_x def display_posterior_density(X, Y, classifier): \u0026quot;\u0026quot;\u0026quot; Displays the posterior density P(Y=1 | X=x) using a contour plot. Parameters: - X: Data points (n_samples, 2) - Y: Class labels (n_samples,) - classifier: Bayes_Classifier object used to predict the posterior probability \u0026quot;\u0026quot;\u0026quot; # Define the grid for evaluating the posterior probability min_x1, min_x2 = np.min(X, axis=0) - 1 max_x1, max_x2 = np.max(X, axis=0) + 1 # Generate a grid of 2D points (mesh) x1, x2 = np.mgrid[min_x1:max_x1:.01, min_x2:max_x2:.01] pos = np.dstack((x1, x2)) # Predict posterior probability of first class for each point in the grid p1_x = classifier.predict_proba(pos.reshape(-1, 2)) if p1_x.ndim \u0026gt; 1 and p1_x.shape[1]\u0026gt;1: p1_x=p1_x[:,0] p1_x=p1_x.reshape(x1.shape) # Display the posterior density in 2D plt.contourf(x1, x2, p1_x, levels=20, cmap='coolwarm') plt.colorbar(label='P(Y=1 | X=x)') plt.contour(x1, x2, p1_x, levels=[0.5], colors='black', linewidths=2) plt.title(\u0026quot;Posterior Density P(Y=1 | X=x)\u0026quot;) plt.xlabel(\u0026quot;x1\u0026quot;) plt.ylabel(\u0026quot;x2\u0026quot;) plt.scatter(X[:, 0], X[:, 1], marker='o', c=Y, cmap='coolwarm', label='Data Points') # Customize the plot plt.title(\u0026quot;Posterior Density P(Y=1 | X=x) with Data Points\u0026quot;) plt.xlabel(\u0026quot;x1\u0026quot;) plt.ylabel(\u0026quot;x2\u0026quot;) plt.legend() # Show the plot plt.show() # Example usage n1, n2 = 200, 200 mu1 = [2, 2] cov1 = [[1, 0], [0, 1]] mu2 = [0, 0] cov2 = [[1, 0], [0, 2]] X, Y = generate_data(n1, n2, mu1, cov1, mu2, cov2) classifier = Bayes_Classifier(mu1, cov1, mu2, cov2) # Display posterior density display_posterior_density(X, Y, classifier) Mon Classifieur de Bayes Naif Classifieur de Bayes Naif\nimport numpy as np class BayesNaifGaussien: def __init__(self): self.mu_0 = None # Moyenne pour la classe 0 self.mu_1 = None # Moyenne pour la classe 1 self.sigma_0 = None # Variance pour la classe 0 self.sigma_1 = None # Variance pour la classe 1 self.p_y0 = None # Probabilité a priori de Y=0 self.p_y1 = None # Probabilité a priori de Y=1 def fit(self, X, Y): \u0026quot;\u0026quot;\u0026quot; Calcule les paramètres du modèle à partir des données X et Y \u0026quot;\u0026quot;\u0026quot; # Séparer les données selon les classes X_0 = X[Y == 0] # Données pour la classe 0 X_1 = X[Y == 1] # Données pour la classe 1 # Calculer les moyennes et variances pour chaque classe self.mu_0 = np.mean(X_0, axis=0) self.mu_1 = np.mean(X_1, axis=0) self.sigma_0 = np.var(X_0, axis=0) self.sigma_1 = np.var(X_1, axis=0) # Probabilités a priori P(Y=0) et P(Y=1) self.p_y0 = len(X_0) / len(X) self.p_y1 = len(X_1) / len(X) def gaussienne(self, x, mu, sigma): \u0026quot;\u0026quot;\u0026quot; Calcul de la densité de probabilité gaussienne pour un point x \u0026quot;\u0026quot;\u0026quot; return (1 / np.sqrt(2 * np.pi * sigma)) * np.exp(-0.5 * ((x - mu) ** 2) / sigma) def predict_proba(self, X): \u0026quot;\u0026quot;\u0026quot; Calcule les probabilités a posteriori P(Y=1 | X) pour chaque individu dans X \u0026quot;\u0026quot;\u0026quot; proba_y1_given_x = [] for x in X: # Calculer P(X | Y=1) et P(X | Y=0) pour chaque variable p_x_given_y1 = np.prod(self.gaussienne(x, self.mu_1, self.sigma_1)) p_x_given_y0 = np.prod(self.gaussienne(x, self.mu_0, self.sigma_0)) # Densité totale P(X=x) p_x = p_x_given_y1 * self.p_y1 + p_x_given_y0 * self.p_y0 # Calculer la probabilité a posteriori P(Y=1 | X=x) (Théorème de Bayes) p_y1_x = (p_x_given_y1 * self.p_y1) / p_x # Ajouter la probabilité pour cet individu proba_y1_given_x.append(p_y1_x) return np.array(proba_y1_given_x) def predict(self, X): \u0026quot;\u0026quot;\u0026quot; Prédit la classe (0 ou 1) pour chaque individu en fonction de P(Y=1 | X) \u0026quot;\u0026quot;\u0026quot; proba_y1 = self.predict_proba(X) return (proba_y1 \u0026gt;= 0.5).astype(int) # Exemple d'utilisation : # Générer des données aléatoires (2 variables) #np.random.seed(42) #X = np.random.randn(100, 2) # 100 individus, 2 variables #Y = np.random.choice([0, 1], size=100) # Classes aléatoires 0 ou 1 # Initialiser et entraîner le modèle model = BayesNaifGaussien() model.fit(X, Y) # Prédire les probabilités a posteriori P(Y=1 | X) proba_y1_given_x = model.predict_proba(X) # Afficher les probabilités et les paramètres du modèle print(\u0026quot;Probabilités P(Y=1 | X):\u0026quot;, proba_y1_given_x) print(\u0026quot;Moyenne classe 0:\u0026quot;, model.mu_0) print(\u0026quot;Moyenne classe 1:\u0026quot;, model.mu_1) print(\u0026quot;Variance classe 0:\u0026quot;, model.sigma_0) print(\u0026quot;Variance classe 1:\u0026quot;, model.sigma_1) # Afficher les performances error=(model.predict(X)!=Y).sum()/len(Y) print(\u0026quot;Erreur empirique\u0026quot;,error) Probabilités P(Y=1 | X): [4.77129432e-02 1.88198612e-01 1.19806441e-02 1.61156057e-03 7.57724865e-01 5.47979333e-03 2.86561102e-02 5.28354680e-03 4.19710179e-03 2.32670361e-03 4.25351100e-05 2.75755452e-02 1.66745301e-03 2.49435759e-01 1.00766811e-03 6.33178333e-02 7.11726400e-02 8.06725044e-03 3.49788468e-02 2.33123217e-01 1.85679153e-01 1.61388217e-02 4.73890955e-04 3.40903507e-03 5.48351905e-02 6.11117153e-01 7.97042638e-04 2.39487337e-02 1.81606673e-01 7.51506787e-01 1.44906911e-01 3.20614819e-03 1.49182986e-02 2.36029577e-02 1.10421848e-03 4.06797993e-02 2.76919459e-01 9.80876216e-03 1.51080835e-03 1.19535527e-02 7.54920408e-02 2.37668468e-03 1.34267871e-01 1.50892025e-04 2.82689122e-04 3.97560524e-01 1.85964298e-01 2.89790882e-02 2.61032354e-01 4.09004267e-01 2.28426189e-04 2.69845210e-01 6.10553550e-03 8.50242142e-01 2.03655676e-01 2.20047876e-02 2.75311642e-02 9.77504492e-02 3.40259695e-01 3.73255885e-01 1.52570034e-03 2.83882103e-01 1.47553228e-03 1.59946985e-02 3.12402875e-03 9.51981172e-03 1.26780125e-01 1.38245003e-03 1.45565399e-01 2.11117520e-01 4.74854726e-04 6.81187545e-02 3.17382874e-01 1.45887411e-01 3.72396824e-03 1.00399080e-04 4.13313282e-03 1.17479805e-03 6.78963271e-01 1.77858266e-02 3.18035514e-01 1.04417578e-01 3.76900434e-02 3.05594629e-01 8.03794773e-05 6.33186245e-01 4.86832863e-03 2.00066529e-02 4.04799919e-03 2.19082430e-01 3.34528481e-01 6.23729351e-03 1.66303929e-02 8.95526616e-03 3.98065567e-02 1.98639470e-01 4.04561745e-02 5.82954816e-02 3.39805011e-03 2.09663974e-02 2.53093383e-02 2.21555326e-02 1.09365256e-02 3.74422495e-02 3.43872900e-04 1.89273542e-02 1.38456942e-03 4.80891278e-02 1.07933445e-01 3.05213147e-01 6.69535889e-02 1.44887853e-03 2.20980438e-03 3.40678910e-02 1.56474474e-02 4.55360912e-04 1.52599230e-04 3.38423666e-02 5.30316981e-02 3.95061749e-03 1.66579442e-03 2.99502034e-01 4.89077220e-01 7.24637743e-02 3.72126273e-03 9.97495673e-01 5.81488703e-02 2.71824520e-03 3.31314879e-03 1.87599392e-01 2.84798275e-02 2.23632018e-01 3.60488048e-01 2.38428084e-01 6.82397142e-02 1.70207819e-04 1.65429076e-02 8.97641020e-02 1.47437064e-02 1.02971798e-02 8.66017352e-04 2.90444112e-01 2.73443451e-03 3.12627059e-02 4.67172669e-02 1.67625382e-02 3.70097236e-02 1.45051358e-03 3.23307861e-01 3.18419139e-03 2.22863238e-02 1.22510786e-03 9.82459480e-02 1.11912747e-02 1.20060399e-01 1.62403349e-03 2.90996052e-01 5.85842940e-02 7.63932976e-03 2.20318786e-02 1.71423565e-02 1.87681205e-02 3.41083199e-02 2.05334553e-03 4.36902743e-01 3.31158281e-03 1.42633539e-04 9.32139342e-04 1.73175620e-02 2.04639977e-03 2.40672704e-05 2.58175562e-03 2.00897353e-01 7.09373320e-02 1.93387721e-02 3.43676984e-02 6.39837963e-03 2.08496946e-02 5.02641687e-02 4.40223776e-02 9.23455857e-01 8.50272970e-01 1.20956546e-01 1.36344422e-01 1.65764476e-01 6.13372600e-02 5.06173937e-03 3.54062021e-02 1.70972598e-03 3.92652867e-01 1.50124067e-01 2.12859692e-02 1.16263615e-02 4.21556783e-02 7.13342266e-02 2.05523669e-02 6.42687173e-02 1.95607635e-01 2.36928616e-01 9.28496158e-02 9.92363237e-01 9.99347763e-01 9.91116855e-01 9.87181820e-01 9.57392190e-01 8.68372052e-01 9.97952198e-01 9.70984948e-01 9.28307824e-01 9.85646977e-01 9.84346244e-01 9.96345508e-01 9.65157702e-01 9.97200756e-01 9.59341537e-01 9.90321766e-01 9.97160385e-01 9.99963510e-01 9.99954006e-01 8.65862808e-01 9.72073336e-01 9.99887907e-01 7.29077611e-01 9.95690771e-01 9.55895128e-01 9.94207279e-01 9.86176885e-01 9.97203715e-01 9.72794523e-01 9.95845219e-01 9.31445242e-01 9.33333563e-01 9.99562104e-01 9.99866670e-01 9.99837689e-01 9.75130335e-01 9.97995441e-01 9.99417007e-01 9.96072060e-01 9.32357076e-01 9.55016044e-01 6.53713472e-01 6.78432054e-01 9.84099988e-01 1.38612115e-01 9.86935547e-01 9.30047978e-01 9.98871069e-01 9.73709329e-01 9.88313439e-01 9.96964639e-01 9.99237123e-01 9.92546248e-01 9.88113710e-01 9.99551783e-01 1.73016802e-01 9.99651727e-01 9.99983129e-01 9.99789445e-01 9.78577405e-01 9.55891996e-01 1.46452492e-01 9.98577642e-01 9.21704460e-01 9.99953932e-01 9.99837455e-01 9.47543861e-01 9.03746805e-01 9.99103334e-01 9.99792672e-01 9.99956825e-01 9.99997796e-01 8.57309795e-01 6.71868292e-01 5.29623647e-01 4.96240922e-01 9.99619180e-01 8.82698551e-01 7.67662542e-01 9.95413843e-01 9.99991943e-01 9.17265098e-01 9.99470345e-01 8.39243490e-01 9.99937582e-01 9.99482108e-01 9.97125578e-01 6.24979920e-01 9.34161919e-01 9.95496765e-01 9.94315135e-01 8.83557684e-01 8.97947696e-01 5.10179180e-01 9.94743249e-01 9.61605183e-01 9.99644538e-01 9.88320148e-01 8.76415088e-01 9.59577794e-01 9.50858263e-01 9.78410036e-01 9.99913326e-01 9.70535075e-01 9.99969452e-01 9.75022751e-01 8.64891438e-01 9.82382548e-01 9.96648735e-01 9.99061819e-01 3.99177158e-01 9.98788237e-01 9.73770146e-01 9.99608663e-01 8.59156079e-01 9.99982757e-01 9.99654456e-01 8.38934579e-01 8.74529610e-01 9.99903513e-01 9.99044151e-01 7.21647474e-01 9.96229908e-01 4.32779327e-02 9.99652362e-01 9.96643859e-01 5.28982658e-01 9.99655356e-01 7.02083645e-01 9.99986917e-01 5.33882446e-01 3.51556956e-01 2.58009589e-01 9.93309344e-01 9.99923436e-01 9.99929086e-01 9.94525560e-01 9.89293529e-01 9.95676851e-01 9.98902457e-01 6.56435543e-01 9.99881910e-01 9.20536849e-01 9.96802987e-01 9.99722059e-01 9.99930401e-01 9.99997011e-01 8.70526167e-01 9.63482657e-01 9.81299644e-01 9.99988951e-01 9.99666706e-01 9.86843060e-01 9.86592838e-01 9.99742465e-01 1.54492741e-01 3.62478287e-01 9.99781860e-01 1.29421022e-02 5.74462107e-01 2.94746510e-01 3.23258171e-01 9.91575605e-01 9.57284443e-01 9.99659660e-01 9.92712669e-01 9.71220091e-01 9.99939050e-01 9.90235282e-01 6.12219544e-01 3.16340570e-01 5.57344204e-01 9.99994937e-01 9.94087895e-01 9.02534982e-01 2.28010496e-01 9.96876362e-01 9.99703341e-01 7.93473528e-01 3.83625876e-01 9.99988105e-01 9.76297207e-01 9.84924536e-01 9.99999680e-01 9.99962153e-01 8.96720802e-01 9.98621979e-01 9.98555326e-01 9.95912839e-01 9.51539421e-01 3.72359423e-01 9.84742990e-01 9.94566778e-01 7.34086111e-01 1.14850155e-01 9.87775139e-01 9.99788052e-01 9.45727676e-01 8.42356808e-01 9.95216255e-01] Moyenne classe 0: [2.04616058 1.91278979] Moyenne classe 1: [-0.07510028 -0.14203197] Variance classe 0: [0.97936836 0.97850395] Variance classe 1: [0.89293017 2.05696981] Erreur empirique 0.0675 Erreur en apprentissage et test pour le classifieur de Bayes naïf from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=1/3, random_state=3) from sklearn.naive_bayes import GaussianNB gnb = GaussianNB(); gnbmod=gnb.fit(X_train, y_train); y_pred_test = gnbmod.predict(X_test) y_pred_train = gnbmod.predict(X_train) E_test=(y_test != y_pred_test).sum()/len(y_test) E_train=(y_train != y_pred_train).sum()/len(y_train) print(\u0026quot;Error on the test set=\u0026quot;,E_test) print(\u0026quot;Error on the train set=\u0026quot;,E_train) Error on the test set= 0.05970149253731343 Error on the train set= 0.07142857142857142 Courbe ROC et AUC pour une validation en 5 plis #Etude MM 2022 y=Y ############################################# # Cross-validation sur la courbe ROC ############################################ import numpy as np import matplotlib.pyplot as plt from sklearn import svm, datasets from sklearn.metrics import auc from sklearn.metrics import RocCurveDisplay from sklearn.model_selection import StratifiedKFold n_samples, n_features = X.shape # Run classifier with cross-validation and plot ROC curves cv = StratifiedKFold(n_splits=6) classifier = GaussianNB(); tprs = [] aucs = [] mean_fpr = np.linspace(0, 1, 100) fig, ax = plt.subplots() for i, (train, test) in enumerate(cv.split(X, y)): y_train=[y[i] for i in train]; y_test=[y[i] for i in test]; classifier.fit(X[train], y_train) viz = RocCurveDisplay.from_estimator( classifier, # Le modèle entraîné X[test], # Les données de test y_test, # Les étiquettes de test name='ROC fold {}'.format(i), # Nom de la courbe pour chaque fold alpha=0.3, # Transparence lw=1, # Épaisseur de la ligne ax=ax # L'axe de la figure pour tracer la courbe ) interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr) interp_tpr[0] = 0.0 tprs.append(interp_tpr) aucs.append(viz.roc_auc) ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', label='Chance', alpha=.8) mean_tpr = np.mean(tprs, axis=0) mean_tpr[-1] = 1.0 mean_auc = auc(mean_fpr, mean_tpr) std_auc = np.std(aucs) ax.plot(mean_fpr, mean_tpr, color='b', label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc), lw=2, alpha=.8) std_tpr = np.std(tprs, axis=0) tprs_upper = np.minimum(mean_tpr + std_tpr, 1) tprs_lower = np.maximum(mean_tpr - std_tpr, 0) ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2, label=r'$\\pm$ 1 std. dev.') ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05], title=\u0026quot;Receiver operating characteristic example\u0026quot;) ax.legend(loc=\u0026quot;lower right\u0026quot;) plt.show() Analyse discriminante linéaire # Example usage from sklearn.model_selection import train_test_split from sklearn.discriminant_analysis import LinearDiscriminantAnalysis from sklearn.metrics import accuracy_score # Generate data n1, n2 = 200, 200 mu1 = [2, 2] cov1 = [[1, 0], [0, 1]] mu2 = [0, 0] cov2 = [[1, 0], [0, 2]] X, Y = generate_data(n1, n2, mu1, cov1, mu2, cov2) # Split the data into train and test sets X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42) # Train LDA model lda = LinearDiscriminantAnalysis() lda.fit(X_train, Y_train) # Predict on test set Y_pred = lda.predict(X_test) # Calculate accuracy accuracy = accuracy_score(Y_test, Y_pred) print(f\u0026quot;LDA Classification Accuracy: {accuracy:.2f}\u0026quot;) # Display posterior density display_posterior_density(X, Y, lda) LDA Classification Accuracy: 0.93 Nous remarquons la frontière linéaire liée à l\u0026rsquo;égalité des matrices de covariance\nAnalyse discriminante quadratique Les matrices de variances-covariances sont laissées libres et la frontière est une hyperbole (proche de la frontière obtenu par le classifieur de Bayes naïf)\n# Example usage with QDA from sklearn.model_selection import train_test_split from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis from sklearn.metrics import accuracy_score # Generate data n1, n2 = 200, 200 mu1 = [2, 2] cov1 = [[1, 0], [0, 1]] mu2 = [0, 0] cov2 = [[1, 0], [0, 2]] X, Y = generate_data(n1, n2, mu1, cov1, mu2, cov2) # Split the data into train and test sets X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42) # Train QDA model qda = QuadraticDiscriminantAnalysis() qda.fit(X_train, Y_train) # Predict on test set Y_pred = qda.predict(X_test) # Calculate accuracy accuracy = accuracy_score(Y_test, Y_pred) print(f\u0026quot;QDA Classification Accuracy: {accuracy:.2f}\u0026quot;) # Display posterior density display_posterior_density(X, Y, qda) QDA Classification Accuracy: 0.93 Les plus proches voisins # Example usage with KNN from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import accuracy_score # Generate data n1, n2 = 200, 200 mu1 = [2, 2] cov1 = [[1, 0], [0, 1]] mu2 = [0, 0] cov2 = [[1, 0], [0, 2]] X, Y = generate_data(n1, n2, mu1, cov1, mu2, cov2) # Split the data into train and test sets X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42) # Train KNN model knn = KNeighborsClassifier(n_neighbors=5) # You can adjust the number of neighbors (k) knn.fit(X_train, Y_train) # Predict on test set Y_pred = knn.predict(X_test) # Calculate accuracy accuracy = accuracy_score(Y_test, Y_pred) print(f\u0026quot;KNN Classification Accuracy: {accuracy:.2f}\u0026quot;) # Display posterior density display_posterior_density(X, Y, knn) KNN Classification Accuracy: 0.85 La régression logistique # Example usage with Logistic Regression from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score # Generate data n1, n2 = 200, 200 mu1 = [2, 2] cov1 = [[1, 0], [0, 1]] mu2 = [0, 0] cov2 = [[1, 0], [0, 2]] X, Y = generate_data(n1, n2, mu1, cov1, mu2, cov2) # Split the data into train and test sets X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42) # Train Logistic Regression model log_reg = LogisticRegression() log_reg.fit(X_train, Y_train) # Predict on test set Y_pred = log_reg.predict(X_test) # Calculate accuracy accuracy = accuracy_score(Y_test, Y_pred) print(f\u0026quot;Logistic Regression Classification Accuracy: {accuracy:.2f}\u0026quot;) # Display posterior density display_posterior_density(X, Y, log_reg) Logistic Regression Classification Accuracy: 0.88 Comparaison des méthodes par AUC et erreur de classification Remarquons les grandes différences entre ensemble d\u0026rsquo;apprentissage et test.\nimport numpy as np import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split, cross_val_score from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis from sklearn.linear_model import LogisticRegression from sklearn.neighbors import KNeighborsClassifier from sklearn.naive_bayes import GaussianNB from sklearn.metrics import roc_auc_score, make_scorer from sklearn.datasets import make_classification # Générer des données simulées n1, n2 = 200, 200 X, Y = make_classification(n_samples=n1 + n2, n_features=2, n_informative=2, n_redundant=0, random_state=42) # Split les données en train/test X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42) # Initialiser les modèles models = { 'Naive Bayes': GaussianNB(), 'LDA': LinearDiscriminantAnalysis(), 'Logistic Regression': LogisticRegression(), 'QDA': QuadraticDiscriminantAnalysis(), 'KNN': KNeighborsClassifier(n_neighbors=5) } # Fonction pour calculer les AUC avec validation croisée (10-fold) def calculate_auc_cv(model, X_train, Y_train, X_test, Y_test): scorer = make_scorer(roc_auc_score) train_scores = cross_val_score(model, X_train, Y_train, cv=10, scoring=scorer) test_scores = cross_val_score(model, X_test, Y_test, cv=10, scoring=scorer) return train_scores, test_scores # Stocker les résultats des AUC results_train = [] results_test = [] model_names = [] # Calculer l'AUC pour chaque modèle for name, model in models.items(): train_auc, test_auc = calculate_auc_cv(model, X_train, Y_train, X_test, Y_test) results_train.append(train_auc) results_test.append(test_auc) model_names.append(name) # Créer les boxplots pour visualiser les AUC en train et en test fig, ax = plt.subplots(figsize=(12, 6)) # Préparer les données pour afficher les boxplots côte à côte positions_train = np.arange(1, len(models) * 2, 2) # Positions pour les boxplots en train positions_test = np.arange(2, len(models) * 2 + 1, 2) # Positions pour les boxplots en test # Boxplot pour l'AUC sur les données de train (en bleu) bp_train = ax.boxplot(results_train, positions=positions_train, widths=0.6, patch_artist=True, boxprops=dict(facecolor=\u0026quot;lightblue\u0026quot;), medianprops=dict(color=\u0026quot;blue\u0026quot;)) # Boxplot pour l'AUC sur les données de test (en orange) bp_test = ax.boxplot(results_test, positions=positions_test, widths=0.6, patch_artist=True, boxprops=dict(facecolor=\u0026quot;orange\u0026quot;), medianprops=dict(color=\u0026quot;red\u0026quot;)) # Ajouter une légende pour les boxplots ax.legend([bp_train[\u0026quot;boxes\u0026quot;][0], bp_test[\u0026quot;boxes\u0026quot;][0]], ['Train', 'Test'], loc='upper right') # Ajuster les labels et le titre ax.set_xticks(np.arange(1.5, len(models) * 2, 2)) ax.set_xticklabels(model_names) ax.set_title(\u0026quot;Comparaison des modèles - AUC en Train et Test (10-fold CV)\u0026quot;) ax.set_ylabel(\u0026quot;AUC Score\u0026quot;) plt.tight_layout() plt.show() import numpy as np import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split, cross_val_score from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis from sklearn.linear_model import LogisticRegression from sklearn.neighbors import KNeighborsClassifier from sklearn.naive_bayes import GaussianNB from sklearn.metrics import make_scorer from sklearn.datasets import make_classification # Générer des données simulées n1, n2 = 200, 200 X, Y = make_classification(n_samples=n1 + n2, n_features=2, n_informative=2, n_redundant=0, random_state=42) # Split les données en train/test X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42) # Initialiser les modèles models = { 'Naive Bayes': GaussianNB(), 'LDA': LinearDiscriminantAnalysis(), 'Logistic Regression': LogisticRegression(), 'QDA': QuadraticDiscriminantAnalysis(), 'KNN': KNeighborsClassifier(n_neighbors=5) } # Fonction pour calculer les erreurs avec validation croisée (10-fold) def calculate_error_cv(model, X_train, Y_train, X_test, Y_test): scorer = make_scorer(lambda y_true, y_pred: 1 - np.mean(y_true == y_pred)) # Calculer l'erreur train_errors = cross_val_score(model, X_train, Y_train, cv=10, scoring=scorer) test_errors = cross_val_score(model, X_test, Y_test, cv=10, scoring=scorer) return train_errors, test_errors # Stocker les résultats des erreurs results_train = [] results_test = [] model_names = [] # Calculer l'erreur pour chaque modèle for name, model in models.items(): train_error, test_error = calculate_error_cv(model, X_train, Y_train, X_test, Y_test) results_train.append(train_error) results_test.append(test_error) model_names.append(name) # Créer les boxplots pour visualiser les erreurs en train et en test fig, ax = plt.subplots(figsize=(12, 6)) # Préparer les données pour afficher les boxplots côte à côte positions_train = np.arange(1, len(models) * 2, 2) # Positions pour les boxplots en train positions_test = np.arange(2, len(models) * 2 + 1, 2) # Positions pour les boxplots en test # Boxplot pour l'erreur sur les données de train (en bleu) bp_train = ax.boxplot(results_train, positions=positions_train, widths=0.6, patch_artist=True, boxprops=dict(facecolor=\u0026quot;lightblue\u0026quot;), medianprops=dict(color=\u0026quot;blue\u0026quot;)) # Boxplot pour l'erreur sur les données de test (en orange) bp_test = ax.boxplot(results_test, positions=positions_test, widths=0.6, patch_artist=True, boxprops=dict(facecolor=\u0026quot;orange\u0026quot;), medianprops=dict(color=\u0026quot;red\u0026quot;)) # Ajouter une légende pour les boxplots ax.legend([bp_train[\u0026quot;boxes\u0026quot;][0], bp_test[\u0026quot;boxes\u0026quot;][0]], ['Train', 'Test'], loc='upper right') # Ajuster les labels et le titre ax.set_xticks(np.arange(1.5, len(models) * 2, 2)) ax.set_xticklabels(model_names) ax.set_title(\u0026quot;Comparaison des modèles - Erreur en Train et Test (10-fold CV)\u0026quot;) ax.set_ylabel(\u0026quot;Erreur de Classification\u0026quot;) plt.tight_layout() plt.show() ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"bd74d1160de68b871033fd2c20cac97c","permalink":"https://cambroise.github.io/courses/introduction-machine-learning/jupyter-gaussian/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/introduction-machine-learning/jupyter-gaussian/","section":"courses","summary":"Simulations Gaussiennes import numpy as np import matplotlib.pyplot as plt from scipy.stats import multivariate_normal from matplotlib.colors import ListedColormap mycolormap = ListedColormap(['#FF0000', '#0000FF']) Exchoice = 1 # Choisir l'exemple ############################################## # Data set generation using scipy.","tags":null,"title":"","type":"courses"},{"authors":null,"categories":null,"content":"Tree, bagging et forêts aléatoires Simulation Soit X un vecteur gaussien de loi $\\mathcal N_p(\\boldsymbol \\mu, \\boldsymbol \\Sigma)$\nimport random as rd from scipy.stats import multivariate_normal import numpy as np import matplotlib.pyplot as plt from matplotlib.colors import ListedColormap import math NG1, NG2 = 50, 150 mu1 = [2, 2] # Moyenne G1 cov1 = [[1, 0], [0, 1]] # Matrice de covariance G1 (indépendance) mu2 = [0, 0] # Moyenne G2 cov2 = [[1, 0], [0, 2]] # Matrice de covariance G2 (indépendance mais variance différente) # Génération des données xG1 = multivariate_normal(mean=mu1, cov=cov1).rvs(NG1) xG2 = multivariate_normal(mean=mu2, cov=cov2).rvs(NG2) X = np.concatenate((xG1, xG2), axis=0) Y = [0] * NG1 + [1] * NG2 # Visualisation des données générées mycolormap = ListedColormap(['#FF0000', '#0000FF']) plt.scatter(X[:, 0], X[:, 1], c=Y, cmap=mycolormap) plt.title('Raw data') plt.grid() plt.xlabel('x1') plt.ylabel('x2') plt.show() Classification avec arbre de décision from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score # Initialiser le classificateur dt_classifier = DecisionTreeClassifier() # Entraîner le modèle dt_classifier.fit(X, Y) # Prédire les probabilités sur le jeu d'entraînement pred_proba_train = dt_classifier.predict_proba(X) # Obtenir les prédictions des classes pred_class_train = np.argmax(pred_proba_train, axis=1) # Calculer l'accuracy sur le jeu d'entraînement train_accuracy = accuracy_score(Y, pred_class_train) # Afficher l'erreur d'entraînement train_error = 1 - train_accuracy print(f\u0026quot;Erreur sur l'ensemble d'entraînement : {train_error:.2f}\u0026quot;) Erreur sur l'ensemble d'entraînement : 0.00 Tracé de la frontière de décision avec contourf # ---- Traçage de la frontière de décision ---- # Créer une grille de points couvrant l'espace des données x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1)) # Prédire la classe pour chaque point de la grille Z = dt_classifier.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) # Tracer la frontière de décision avec contourf plt.contourf(xx, yy, Z, alpha=0.4, cmap=mycolormap) # Tracer les points de données plt.scatter(X[:, 0], X[:, 1], c=Y, edgecolors='k', cmap=mycolormap) plt.title('Decision Boundary of the Decision Tree') plt.grid() plt.xlabel('x1') plt.ylabel('x2') plt.show() help(DecisionTreeClassifier) Help on class DecisionTreeClassifier in module sklearn.tree._classes: class DecisionTreeClassifier(sklearn.base.ClassifierMixin, BaseDecisionTree) | DecisionTreeClassifier(*, criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, class_weight=None, ccp_alpha=0.0, monotonic_cst=None) | | A decision tree classifier. | | Read more in the :ref:`User Guide \u0026lt;tree\u0026gt;`. | | Parameters | ---------- | criterion : {\u0026quot;gini\u0026quot;, \u0026quot;entropy\u0026quot;, \u0026quot;log_loss\u0026quot;}, default=\u0026quot;gini\u0026quot; | The function to measure the quality of a split. Supported criteria are | \u0026quot;gini\u0026quot; for the Gini impurity and \u0026quot;log_loss\u0026quot; and \u0026quot;entropy\u0026quot; both for the | Shannon information gain, see :ref:`tree_mathematical_formulation`. | | splitter : {\u0026quot;best\u0026quot;, \u0026quot;random\u0026quot;}, default=\u0026quot;best\u0026quot; | The strategy used to choose the split at each node. Supported | strategies are \u0026quot;best\u0026quot; to choose the best split and \u0026quot;random\u0026quot; to choose | the best random split. | | max_depth : int, default=None | The maximum depth of the tree. If None, then nodes are expanded until | all leaves are pure or until all leaves contain less than | min_samples_split samples. | | min_samples_split : int or float, default=2 | The minimum number of samples required to split an internal node: | | - If int, then consider `min_samples_split` as the minimum number. | - If float, then `min_samples_split` is a fraction and | `ceil(min_samples_split * n_samples)` are the minimum | number of samples for each split. | | .. versionchanged:: 0.18 | Added float values for fractions. | | min_samples_leaf : int or float, default=1 | The minimum number of samples required to be at a leaf node. | A split point at any depth will only be considered if it leaves at | least ``min_samples_leaf`` training samples in each of the left and | right branches. This may have the effect of smoothing the model, | especially in regression. | | - If int, then consider `min_samples_leaf` as the minimum number. | - If float, then `min_samples_leaf` is a fraction and | `ceil(min_samples_leaf * n_samples)` are the minimum | number of samples for each node. | | .. versionchanged:: 0.18 | Added float values for fractions. | | min_weight_fraction_leaf : float, default=0.0 | The minimum weighted fraction of the sum total of weights (of all | the input samples) required to be at a leaf node. Samples have | equal weight when sample_weight is not provided. | | max_features : int, float or {\u0026quot;sqrt\u0026quot;, \u0026quot;log2\u0026quot;}, default=None | The number of features to consider when looking for the best split: | | - If int, then consider `max_features` features at each split. | - If float, then `max_features` is a fraction and | `max(1, int(max_features * n_features_in_))` features are considered at | each split. | - If \u0026quot;sqrt\u0026quot;, then `max_features=sqrt(n_features)`. | - If \u0026quot;log2\u0026quot;, then `max_features=log2(n_features)`. | - If None, then `max_features=n_features`. | | Note: the search for a split does not stop until at least one | valid partition of the node samples is found, even if it requires to | effectively inspect more than ``max_features`` features. | | random_state : int, RandomState instance or None, default=None | Controls the randomness of the estimator. The features are always | randomly permuted at each split, even if ``splitter`` is set to | ``\u0026quot;best\u0026quot;``. When ``max_features \u0026lt; n_features``, the algorithm will | select ``max_features`` at random at each split before finding the best | split among them. But the best found split may vary across different | runs, even if ``max_features=n_features``. That is the case, if the | improvement of the criterion is identical for several splits and one | split has to be selected at random. To obtain a deterministic behaviour | during fitting, ``random_state`` has to be fixed to an integer. | See :term:`Glossary \u0026lt;random_state\u0026gt;` for details. | | max_leaf_nodes : int, default=None | Grow a tree with ``max_leaf_nodes`` in best-first fashion. | Best nodes are defined as relative reduction in impurity. | If None then unlimited number of leaf nodes. | | min_impurity_decrease : float, default=0.0 | A node will be split if this split induces a decrease of the impurity | greater than or equal to this value. | | The weighted impurity decrease equation is the following:: | | N_t / N * (impurity - N_t_R / N_t * right_impurity | - N_t_L / N_t * left_impurity) | | where ``N`` is the total number of samples, ``N_t`` is the number of | samples at the current node, ``N_t_L`` is the number of samples in the | left child, and ``N_t_R`` is the number of samples in the right child. | | ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum, | if ``sample_weight`` is passed. | | .. versionadded:: 0.19 | | class_weight : dict, list of dict or \u0026quot;balanced\u0026quot;, default=None | Weights associated with classes in the form ``{class_label: weight}``. | If None, all classes are supposed to have weight one. For | multi-output problems, a list of dicts can be provided in the same | order as the columns of y. | | Note that for multioutput (including multilabel) weights should be | defined for each class of every column in its own dict. For example, | for four-class multilabel classification weights should be | [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of | [{1:1}, {2:5}, {3:1}, {4:1}]. | | The \u0026quot;balanced\u0026quot; mode uses the values of y to automatically adjust | weights inversely proportional to class frequencies in the input data | as ``n_samples / (n_classes * np.bincount(y))`` | | For multi-output, the weights of each column of y will be multiplied. | | Note that these weights will be multiplied with sample_weight (passed | through the fit method) if sample_weight is specified. | | ccp_alpha : non-negative float, default=0.0 | Complexity parameter used for Minimal Cost-Complexity Pruning. The | subtree with the largest cost complexity that is smaller than | ``ccp_alpha`` will be chosen. By default, no pruning is performed. See | :ref:`minimal_cost_complexity_pruning` for details. | | .. versionadded:: 0.22 | | monotonic_cst : array-like of int of shape (n_features), default=None | Indicates the monotonicity constraint to enforce on each feature. | - 1: monotonic increase | - 0: no constraint | - -1: monotonic decrease | | If monotonic_cst is None, no constraints are applied. | | Monotonicity constraints are not supported for: | - multiclass classifications (i.e. when `n_classes \u0026gt; 2`), | - multioutput classifications (i.e. when `n_outputs_ \u0026gt; 1`), | - classifications trained on data with missing values. | | The constraints hold over the probability of the positive class. | | Read more in the :ref:`User Guide \u0026lt;monotonic_cst_gbdt\u0026gt;`. | | .. versionadded:: 1.4 | | Attributes | ---------- | classes_ : ndarray of shape (n_classes,) or list of ndarray | The classes labels (single output problem), | or a list of arrays of class labels (multi-output problem). | | feature_importances_ : ndarray of shape (n_features,) | The impurity-based feature importances. | The higher, the more important the feature. | The importance of a feature is computed as the (normalized) | total reduction of the criterion brought by that feature. It is also | known as the Gini importance [4]_. | | Warning: impurity-based feature importances can be misleading for | high cardinality features (many unique values). See | :func:`sklearn.inspection.permutation_importance` as an alternative. | | max_features_ : int | The inferred value of max_features. | | n_classes_ : int or list of int | The number of classes (for single output problems), | or a list containing the number of classes for each | output (for multi-output problems). | | n_features_in_ : int | Number of features seen during :term:`fit`. | | .. versionadded:: 0.24 | | feature_names_in_ : ndarray of shape (`n_features_in_`,) | Names of features seen during :term:`fit`. Defined only when `X` | has feature names that are all strings. | | .. versionadded:: 1.0 | | n_outputs_ : int | The number of outputs when ``fit`` is performed. | | tree_ : Tree instance | The underlying Tree object. Please refer to | ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and | :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py` | for basic usage of these attributes. | | See Also | -------- | DecisionTreeRegressor : A decision tree regressor. | | Notes | ----- | The default values for the parameters controlling the size of the trees | (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and | unpruned trees which can potentially be very large on some data sets. To | reduce memory consumption, the complexity and size of the trees should be | controlled by setting those parameter values. | | The :meth:`predict` method operates using the :func:`numpy.argmax` | function on the outputs of :meth:`predict_proba`. This means that in | case the highest predicted probabilities are tied, the classifier will | predict the tied class with the lowest index in :term:`classes_`. | | References | ---------- | | .. [1] https://en.wikipedia.org/wiki/Decision_tree_learning | | .. [2] L. Breiman, J. Friedman, R. Olshen, and C. Stone, \u0026quot;Classification | and Regression Trees\u0026quot;, Wadsworth, Belmont, CA, 1984. | | .. [3] T. Hastie, R. Tibshirani and J. Friedman. \u0026quot;Elements of Statistical | Learning\u0026quot;, Springer, 2009. | | .. [4] L. Breiman, and A. Cutler, \u0026quot;Random Forests\u0026quot;, | https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm | | Examples | -------- | \u0026gt;\u0026gt;\u0026gt; from sklearn.datasets import load_iris | \u0026gt;\u0026gt;\u0026gt; from sklearn.model_selection import cross_val_score | \u0026gt;\u0026gt;\u0026gt; from sklearn.tree import DecisionTreeClassifier | \u0026gt;\u0026gt;\u0026gt; clf = DecisionTreeClassifier(random_state=0) | \u0026gt;\u0026gt;\u0026gt; iris = load_iris() | \u0026gt;\u0026gt;\u0026gt; cross_val_score(clf, iris.data, iris.target, cv=10) | ... # doctest: +SKIP | ... | array([ 1. , 0.93..., 0.86..., 0.93..., 0.93..., | 0.93..., 0.93..., 1. , 0.93..., 1. ]) | | Method resolution order: | DecisionTreeClassifier | sklearn.base.ClassifierMixin | BaseDecisionTree | sklearn.base.MultiOutputMixin | sklearn.base.BaseEstimator | sklearn.utils._estimator_html_repr._HTMLDocumentationLinkMixin | sklearn.utils._metadata_requests._MetadataRequester | builtins.object | | Methods defined here: | | __init__(self, *, criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, class_weight=None, ccp_alpha=0.0, monotonic_cst=None) | Initialize self. See help(type(self)) for accurate signature. | | fit(self, X, y, sample_weight=None, check_input=True) | Build a decision tree classifier from the training set (X, y). | | Parameters | ---------- | X : {array-like, sparse matrix} of shape (n_samples, n_features) | The training input samples. Internally, it will be converted to | ``dtype=np.float32`` and if a sparse matrix is provided | to a sparse ``csc_matrix``. | | y : array-like of shape (n_samples,) or (n_samples, n_outputs) | The target values (class labels) as integers or strings. | | sample_weight : array-like of shape (n_samples,), default=None | Sample weights. If None, then samples are equally weighted. Splits | that would create child nodes with net zero or negative weight are | ignored while searching for a split in each node. Splits are also | ignored if they would result in any single class carrying a | negative weight in either child node. | | check_input : bool, default=True | Allow to bypass several input checking. | Don't use this parameter unless you know what you're doing. | | Returns | ------- | self : DecisionTreeClassifier | Fitted estimator. | | predict_log_proba(self, X) | Predict class log-probabilities of the input samples X. | | Parameters | ---------- | X : {array-like, sparse matrix} of shape (n_samples, n_features) | The input samples. Internally, it will be converted to | ``dtype=np.float32`` and if a sparse matrix is provided | to a sparse ``csr_matrix``. | | Returns | ------- | proba : ndarray of shape (n_samples, n_classes) or list of n_outputs such arrays if n_outputs \u0026gt; 1 | The class log-probabilities of the input samples. The order of the | classes corresponds to that in the attribute :term:`classes_`. | | predict_proba(self, X, check_input=True) | Predict class probabilities of the input samples X. | | The predicted class probability is the fraction of samples of the same | class in a leaf. | | Parameters | ---------- | X : {array-like, sparse matrix} of shape (n_samples, n_features) | The input samples. Internally, it will be converted to | ``dtype=np.float32`` and if a sparse matrix is provided | to a sparse ``csr_matrix``. | | check_input : bool, default=True | Allow to bypass several input checking. | Don't use this parameter unless you know what you're doing. | | Returns | ------- | proba : ndarray of shape (n_samples, n_classes) or list of n_outputs such arrays if n_outputs \u0026gt; 1 | The class probabilities of the input samples. The order of the | classes corresponds to that in the attribute :term:`classes_`. | | set_fit_request(self: sklearn.tree._classes.DecisionTreeClassifier, *, check_input: Union[bool, NoneType, str] = '$UNCHANGED$', sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -\u0026gt; sklearn.tree._classes.DecisionTreeClassifier | Request metadata passed to the ``fit`` method. | | Note that this method is only relevant if | ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`). | Please see :ref:`User Guide \u0026lt;metadata_routing\u0026gt;` on how the routing | mechanism works. | | The options for each parameter are: | | - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided. | | - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``. | | - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it. | | - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name. | | The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the | existing request. This allows you to change the request for some | parameters and not others. | | .. versionadded:: 1.3 | | .. note:: | This method is only relevant if this estimator is used as a | sub-estimator of a meta-estimator, e.g. used inside a | :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect. | | Parameters | ---------- | check_input : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED | Metadata routing for ``check_input`` parameter in ``fit``. | | sample_weight : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED | Metadata routing for ``sample_weight`` parameter in ``fit``. | | Returns | ------- | self : object | The updated object. | | set_predict_proba_request(self: sklearn.tree._classes.DecisionTreeClassifier, *, check_input: Union[bool, NoneType, str] = '$UNCHANGED$') -\u0026gt; sklearn.tree._classes.DecisionTreeClassifier | Request metadata passed to the ``predict_proba`` method. | | Note that this method is only relevant if | ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`). | Please see :ref:`User Guide \u0026lt;metadata_routing\u0026gt;` on how the routing | mechanism works. | | The options for each parameter are: | | - ``True``: metadata is requested, and passed to ``predict_proba`` if provided. The request is ignored if metadata is not provided. | | - ``False``: metadata is not requested and the meta-estimator will not pass it to ``predict_proba``. | | - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it. | | - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name. | | The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the | existing request. This allows you to change the request for some | parameters and not others. | | .. versionadded:: 1.3 | | .. note:: | This method is only relevant if this estimator is used as a | sub-estimator of a meta-estimator, e.g. used inside a | :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect. | | Parameters | ---------- | check_input : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED | Metadata routing for ``check_input`` parameter in ``predict_proba``. | | Returns | ------- | self : object | The updated object. | | set_predict_request(self: sklearn.tree._classes.DecisionTreeClassifier, *, check_input: Union[bool, NoneType, str] = '$UNCHANGED$') -\u0026gt; sklearn.tree._classes.DecisionTreeClassifier | Request metadata passed to the ``predict`` method. | | Note that this method is only relevant if | ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`). | Please see :ref:`User Guide \u0026lt;metadata_routing\u0026gt;` on how the routing | mechanism works. | | The options for each parameter are: | | - ``True``: metadata is requested, and passed to ``predict`` if provided. The request is ignored if metadata is not provided. | | - ``False``: metadata is not requested and the meta-estimator will not pass it to ``predict``. | | - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it. | | - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name. | | The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the | existing request. This allows you to change the request for some | parameters and not others. | | .. versionadded:: 1.3 | | .. note:: | This method is only relevant if this estimator is used as a | sub-estimator of a meta-estimator, e.g. used inside a | :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect. | | Parameters | ---------- | check_input : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED | Metadata routing for ``check_input`` parameter in ``predict``. | | Returns | ------- | self : object | The updated object. | | set_score_request(self: sklearn.tree._classes.DecisionTreeClassifier, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -\u0026gt; sklearn.tree._classes.DecisionTreeClassifier | Request metadata passed to the ``score`` method. | | Note that this method is only relevant if | ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`). | Please see :ref:`User Guide \u0026lt;metadata_routing\u0026gt;` on how the routing | mechanism works. | | The options for each parameter are: | | - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided. | | - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``. | | - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it. | | - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name. | | The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the | existing request. This allows you to change the request for some | parameters and not others. | | .. versionadded:: 1.3 | | .. note:: | This method is only relevant if this estimator is used as a | sub-estimator of a meta-estimator, e.g. used inside a | :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect. | | Parameters | ---------- | sample_weight : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED | Metadata routing for ``sample_weight`` parameter in ``score``. | | Returns | ------- | self : object | The updated object. | | ---------------------------------------------------------------------- | Data and other attributes defined here: | | __abstractmethods__ = frozenset() | | __annotations__ = {'_parameter_constraints': \u0026lt;class 'dict'\u0026gt;} | | ---------------------------------------------------------------------- | Methods inherited from sklearn.base.ClassifierMixin: | | score(self, X, y, sample_weight=None) | Return the mean accuracy on the given test data and labels. | | In multi-label classification, this is the subset accuracy | which is a harsh metric since you require for each sample that | each label set be correctly predicted. | | Parameters | ---------- | X : array-like of shape (n_samples, n_features) | Test samples. | | y : array-like of shape (n_samples,) or (n_samples, n_outputs) | True labels for `X`. | | sample_weight : array-like of shape (n_samples,), default=None | Sample weights. | | Returns | ------- | score : float | Mean accuracy of ``self.predict(X)`` w.r.t. `y`. | | ---------------------------------------------------------------------- | Data descriptors inherited from sklearn.base.ClassifierMixin: | | __dict__ | dictionary for instance variables | | __weakref__ | list of weak references to the object | | ---------------------------------------------------------------------- | Methods inherited from BaseDecisionTree: | | apply(self, X, check_input=True) | Return the index of the leaf that each sample is predicted as. | | .. versionadded:: 0.17 | | Parameters | ---------- | X : {array-like, sparse matrix} of shape (n_samples, n_features) | The input samples. Internally, it will be converted to | ``dtype=np.float32`` and if a sparse matrix is provided | to a sparse ``csr_matrix``. | | check_input : bool, default=True | Allow to bypass several input checking. | Don't use this parameter unless you know what you're doing. | | Returns | ------- | X_leaves : array-like of shape (n_samples,) | For each datapoint x in X, return the index of the leaf x | ends up in. Leaves are numbered within | ``[0; self.tree_.node_count)``, possibly with gaps in the | numbering. | | cost_complexity_pruning_path(self, X, y, sample_weight=None) | Compute the pruning path during Minimal Cost-Complexity Pruning. | | See :ref:`minimal_cost_complexity_pruning` for details on the pruning | process. | | Parameters | ---------- | X : {array-like, sparse matrix} of shape (n_samples, n_features) | The training input samples. Internally, it will be converted to | ``dtype=np.float32`` and if a sparse matrix is provided | to a sparse ``csc_matrix``. | | y : array-like of shape (n_samples,) or (n_samples, n_outputs) | The target values (class labels) as integers or strings. | | sample_weight : array-like of shape (n_samples,), default=None | Sample weights. If None, then samples are equally weighted. Splits | that would create child nodes with net zero or negative weight are | ignored while searching for a split in each node. Splits are also | ignored if they would result in any single class carrying a | negative weight in either child node. | | Returns | ------- | ccp_path : :class:`~sklearn.utils.Bunch` | Dictionary-like object, with the following attributes. | | ccp_alphas : ndarray | Effective alphas of subtree during pruning. | | impurities : ndarray | Sum of the impurities of the subtree leaves for the | corresponding alpha value in ``ccp_alphas``. | | decision_path(self, X, check_input=True) | Return the decision path in the tree. | | .. versionadded:: 0.18 | | Parameters | ---------- | X : {array-like, sparse matrix} of shape (n_samples, n_features) | The input samples. Internally, it will be converted to | ``dtype=np.float32`` and if a sparse matrix is provided | to a sparse ``csr_matrix``. | | check_input : bool, default=True | Allow to bypass several input checking. | Don't use this parameter unless you know what you're doing. | | Returns | ------- | indicator : sparse matrix of shape (n_samples, n_nodes) | Return a node indicator CSR matrix where non zero elements | indicates that the samples goes through the nodes. | | get_depth(self) | Return the depth of the decision tree. | | The depth of a tree is the maximum distance between the root | and any leaf. | | Returns | ------- | self.tree_.max_depth : int | The maximum depth of the tree. | | get_n_leaves(self) | Return the number of leaves of the decision tree. | | Returns | ------- | self.tree_.n_leaves : int | Number of leaves. | | predict(self, X, check_input=True) | Predict class or regression value for X. | | For a classification model, the predicted class for each sample in X is | returned. For a regression model, the predicted value based on X is | returned. | | Parameters | ---------- | X : {array-like, sparse matrix} of shape (n_samples, n_features) | The input samples. Internally, it will be converted to | ``dtype=np.float32`` and if a sparse matrix is provided | to a sparse ``csr_matrix``. | | check_input : bool, default=True | Allow to bypass several input checking. | Don't use this parameter unless you know what you're doing. | | Returns | ------- | y : array-like of shape (n_samples,) or (n_samples, n_outputs) | The predicted classes, or the predict values. | | ---------------------------------------------------------------------- | Readonly properties inherited from BaseDecisionTree: | | feature_importances_ | Return the feature importances. | | The importance of a feature is computed as the (normalized) total | reduction of the criterion brought by that feature. | It is also known as the Gini importance. | | Warning: impurity-based feature importances can be misleading for | high cardinality features (many unique values). See | :func:`sklearn.inspection.permutation_importance` as an alternative. | | Returns | ------- | feature_importances_ : ndarray of shape (n_features,) | Normalized total reduction of criteria by feature | (Gini importance). | | ---------------------------------------------------------------------- | Methods inherited from sklearn.base.BaseEstimator: | | __getstate__(self) | Helper for pickle. | | __repr__(self, N_CHAR_MAX=700) | Return repr(self). | | __setstate__(self, state) | | __sklearn_clone__(self) | | get_params(self, deep=True) | Get parameters for this estimator. | | Parameters | ---------- | deep : bool, default=True | If True, will return the parameters for this estimator and | contained subobjects that are estimators. | | Returns | ------- | params : dict | Parameter names mapped to their values. | | set_params(self, **params) | Set the parameters of this estimator. | | The method works on simple estimators as well as on nested objects | (such as :class:`~sklearn.pipeline.Pipeline`). The latter have | parameters of the form ``\u0026lt;component\u0026gt;__\u0026lt;parameter\u0026gt;`` so that it's | possible to update each component of a nested object. | | Parameters | ---------- | **params : dict | Estimator parameters. | | Returns | ------- | self : estimator instance | Estimator instance. | | ---------------------------------------------------------------------- | Methods inherited from sklearn.utils._metadata_requests._MetadataRequester: | | get_metadata_routing(self) | Get metadata routing of this object. | | Please check :ref:`User Guide \u0026lt;metadata_routing\u0026gt;` on how the routing | mechanism works. | | Returns | ------- | routing : MetadataRequest | A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating | routing information. | | ---------------------------------------------------------------------- | Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester: | | __init_subclass__(**kwargs) from abc.ABCMeta | Set the ``set_{method}_request`` methods. | | This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It | looks for the information available in the set default values which are | set using ``__metadata_request__*`` class attributes, or inferred | from method signatures. | | The ``__metadata_request__*`` class attributes are used when a method | does not explicitly accept a metadata through its arguments or if the | developer would like to specify a request value for those metadata | which are different from the default ``None``. | | References | ---------- | .. [1] https://www.python.org/dev/peps/pep-0487 Les paramètres de l\u0026rsquo;arbre de décision sont fixés mais peuvent être changés. Pour obtenir un arbre plus simple, il est par exemple possible d\u0026rsquo;augmenter le nombre de noeuds minimum par feuille.\ndt_classifier = DecisionTreeClassifier() # Afficher les paramètres du modèle params = dt_classifier.get_params() print(params) {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': None, 'splitter': 'best'} dt_classifier.set_params(min_samples_split= 30) DecisionTreeClassifier(min_samples_split=30)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u0026nbsp;\u0026nbsp;DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFittedDecisionTreeClassifier(min_samples_split=30) # Entraîner le modèle dt_classifier.fit(X, Y) Z = dt_classifier.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) # Tracer la frontière de décision avec contourf plt.contourf(xx, yy, Z, alpha=0.4, cmap=mycolormap) # Tracer les points de données plt.scatter(X[:, 0], X[:, 1], c=Y, edgecolors='k', cmap=mycolormap) plt.title('Decision Boundary of the Decision Tree') plt.grid() plt.xlabel('x1') plt.ylabel('x2') plt.show() La méthode CART avec l\u0026rsquo;élagage pour obtenir les valeurs de coût-complexité import numpy as np import matplotlib.pyplot as plt from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score # Générer des données exemple (X, Y) - Tes données # Assure-toi d'avoir X et Y définis avant ceci X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=1) # Initialiser le classificateur sans élagage pour obtenir le chemin d'élagage dt_classifier = DecisionTreeClassifier(random_state=1) path = dt_classifier.cost_complexity_pruning_path(X_train, Y_train) ccp_alphas = path.ccp_alphas # Liste des valeurs de ccp_alpha impurities = path.impurities # Coût total de l'arbre pour chaque alpha # Stocker les erreurs pour chaque ccp_alpha train_errors = [] test_errors = [] # Entraîner un arbre pour chaque valeur de ccp_alpha et calculer les erreurs for ccp_alpha in ccp_alphas: dt_classifier_pruned = DecisionTreeClassifier(random_state=1, ccp_alpha=ccp_alpha) dt_classifier_pruned.fit(X_train, Y_train) # Prédiction sur l'ensemble d'entraînement et de test train_pred = dt_classifier_pruned.predict(X_train) test_pred = dt_classifier_pruned.predict(X_test) # Calculer l'erreur (1 - précision) train_error = 1 - accuracy_score(Y_train, train_pred) test_error = 1 - accuracy_score(Y_test, test_pred) # Stocker les erreurs train_errors.append(train_error) test_errors.append(test_error) # Tracer les erreurs d'apprentissage et de test en fonction de ccp_alpha plt.figure(figsize=(10, 6)) plt.plot(ccp_alphas, train_errors, marker='o', label=\u0026quot;Erreur d'apprentissage\u0026quot;, drawstyle=\u0026quot;steps-post\u0026quot;) plt.plot(ccp_alphas, test_errors, marker='o', label=\u0026quot;Erreur de test\u0026quot;, drawstyle=\u0026quot;steps-post\u0026quot;) plt.xlabel(\u0026quot;Valeur de ccp_alpha\u0026quot;) plt.ylabel(\u0026quot;Erreur\u0026quot;) plt.title(\u0026quot;Erreur d'apprentissage et de test en fonction de ccp_alpha\u0026quot;) plt.legend() plt.grid(True) plt.show() Bagging from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import BaggingClassifier from sklearn.metrics import accuracy_score # Initialiser le modèle d'arbre de décision treemod = DecisionTreeClassifier() # Initialiser le modèle Bagging avec l'arbre de décision comme estimateur de base bagmod = BaggingClassifier(estimator=treemod, n_estimators=100, random_state=0) # Entraîner les modèles (arbre et bagging) sur les données treemodfit = treemod.fit(X, Y) # Facultatif ici si tu utilises le modèle bagging bagmodfit = bagmod.fit(X, Y) # Prédire les classes sur l'ensemble d'entraînement avec Bagging pY_train = bagmodfit.predict(X) # Utiliser predict() pour obtenir les classes # Calculer l'erreur d'entraînement train_error = 1 - accuracy_score(Y, pY_train) print(\u0026quot;L'erreur en apprentissage du Bagging est \u0026quot;, train_error) Z = bagmodfit.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) # Tracer la frontière de décision avec contourf plt.contourf(xx, yy, Z, alpha=0.4, cmap=mycolormap) # Tracer les points de données plt.scatter(X[:, 0], X[:, 1], c=Y, edgecolors='k', cmap=mycolormap) plt.title('Decision Boundary of the Bagged Trees') plt.grid() plt.xlabel('x1') plt.ylabel('x2') plt.show() L'erreur en apprentissage du Bagging est 0.0 Forêt aléatoire from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score import numpy as np import matplotlib.pyplot as plt from matplotlib.colors import ListedColormap # Remplace le BaggingClassifier par le RandomForestClassifier forestmod = RandomForestClassifier(n_estimators=100, random_state=0) # Entraîner le modèle de forêt aléatoire sur les données forestmodfit = forestmod.fit(X, Y) # Prédire les classes sur l'ensemble d'entraînement avec RandomForest pY_train = forestmodfit.predict(X) # Calculer l'erreur d'entraînement train_error = 1 - accuracy_score(Y, pY_train) print(\u0026quot;L'erreur en apprentissage de la forêt aléatoire est \u0026quot;, train_error) # Prédire les classes sur la grille de points avec RandomForest Z = forestmodfit.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) # Tracer la frontière de décision avec contourf mycolormap = ListedColormap(['#FF0000', '#0000FF']) plt.contourf(xx, yy, Z, alpha=0.4, cmap=mycolormap) # Tracer les points de données plt.scatter(X[:, 0], X[:, 1], c=Y, edgecolors='k', cmap=mycolormap) plt.title('Decision Boundary of the Random Forest') plt.grid() plt.xlabel('x1') plt.ylabel('x2') plt.show() L'erreur en apprentissage de la forêt aléatoire est 0.0 ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0a745734e1930eb43677fc85f6f5c3ce","permalink":"https://cambroise.github.io/courses/introduction-machine-learning/jupyter-tree/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/introduction-machine-learning/jupyter-tree/","section":"courses","summary":"Tree, bagging et forêts aléatoires Simulation Soit X un vecteur gaussien de loi $\\mathcal N_p(\\boldsymbol \\mu, \\boldsymbol \\Sigma)$\nimport random as rd from scipy.stats import multivariate_normal import numpy as np import matplotlib.","tags":null,"title":"","type":"courses"}]